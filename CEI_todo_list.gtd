/*
### 000 정신 교육
    # 대학원은 내인생 마지막 도전이다. 힘내자.후회없이 가자. 앞으로 더 도전없다. 유지다.
      마지막 내인생의 혁신이다. 마지막 리스크 테이킹이다.
    # 왜 걷지 않고 달리냐고 묻는 다면. 달릴 수 있는 힘이 있는데 매 순간 최선을 다해 달리지 않을 이유가 없다.
    # 위 추즈투 고투더문 - 여러분은 어려운 일을 해냈다. 나는 실용주의자라서 어려운 일은 피해왔지만
      어려운 일이기 때문에 의미가 있는 일도 있다.
    # 고시 안보고 Fail-Safe 한 솔루션으로 회사를 선택했고 All or Nothing 이 아닌 연속적인 성공을 추구하기로 했다.
      대학원 2년만큼 명시적으로 보이는 기회가 자주 오지 않는다. 무조건 2년간 실패없이 노력만큼 보상이 주어지는
      고시라고 생각하고 2년간 수도자 생활하자.
    # 내가 나를 어떻게 보는가가 나를 만든다. 1등하자.
    # 과거로 돌아갈 기회가 있다면 .. 그게 지금이다
    # 2년간 잘 준비해서 최종발표 때 대박치자


### 000 일정 목표 : 매일 수정본 팀원 공유 - 발표 준비용
    ## 목 : PPT 초안 완성

    ## 금,토,일
        # PPT 추가 장표 작성
        # PPT - #배경(1,2,3), #구현(4,6)  완성 (1차 종목분 까지)

    ## 월
        # 오전 : #구현(5) 완성 (1차 종목분 까지)
        # 오후 : 출근 (1시까지)

    ## 화
        # PPT - #활용(8) 8. 예측모델생성 완성
        # PPT - #활용(7,8,9) 완성 (1차 종목분 까지)

        # 조원들 최종버전 메일로 보내기 >>> 향후 일정 및 Zoom 회의실 번호도 미리 전달
          (권형우 선임 발표 리허설 준비 요청)
        # 교수님 보내기 - 참조 - 교수님, 조원들 포함

    ## 수
        # 2차 종목 분석 - 시장정보 포함	
        # PPT - #활용 - 2차 종목 분석 그래프 추가
		
        # PPT - 발표 노트에 포인트 위주로 코멘트 작성
        # PPT - 조원들 피드백 반영 (있을 경우)

    ## 금 (12/11)
        # 행정실에 메일로 제출하기 : PPT(폰트포함), PDF

    ## 이후
        # 권형우 선임 발표 준비 - 내용 설명 및 지원
        # 스크립트 - 발표용 스크립트 노트에 달아보기 (출퇴근 시 지하철에서 계속 읽으면서 발표 연습해보고 스크립트 달기)

*/





--------------------
(추가 종목 분석)


$$$
### 200 시장정보 받아서 올려놓기 + 종목 더찾고 PPT, 발표스크립트 작성
              + 다음 수집할 종목 찾기 (추가 종목 수집 되는 동안)


$$$ 대기
### 600 2차 수집 대상 스크래핑
키움증권     039490 https://finance.naver.com/item/board.nhn?code=039490&page=228
종근당바이오 063160 https://finance.naver.com/item/board.nhn?code=063160&page=720
미래에셋대우 006800 https://finance.naver.com/item/board.nhn?code=006800&page=1043
종근당       185750 https://finance.naver.com/item/board.nhn?code=185750&page=1217
네이버       035420 https://finance.naver.com/item/board.nhn?code=035420&page=2156
엔투텍       227950 https://finance.naver.com/item/board.nhn?code=227950&page=2283
SK케미칼     285130 https://finance.naver.com/item/board.nhn?code=285130&page=4452
LG화학       051910 https://finance.naver.com/item/board.nhn?code=051910&page=5361
수젠텍       253840 https://finance.naver.com/item/board.nhn?code=253840&page=6498
카카오       035720 https://finance.naver.com/item/board.nhn?code=035720&page=9099
씨젠         096530 https://finance.naver.com/item/board.nhn?code=096530&page=27886
셀트리온     068270 https://finance.naver.com/item/board.nhn?code=068270&page=34825




--------------------
(기타)







=====================================================================================================================
(완료)



### 201208

    교수님, 미래에셋 5조 발표 최종 자료 전달 및 향 후 일정 말씀드립니다


    # (1) 발표 자료 제출 (2020.12.11.(금)  까지)
        - 첨부된 파일로 제가 12/11일에 행정실 메일로 제출 하겠습니다
          * 수정 필요사항 있는 경우 말씀해주세요


    # (2) 5조 발표 리허설 (2020.12.15.(화) 18:30)
        - Zoom : https://korea-ac-kr.zoom.us/j/6684744692
          회의 ID: 668 474 4692

        - 발표 예정자 : 권형우 학우


    # (3) 최종발표회
        - 사전 test
          2020.12.16.(수) 17:00(발표자 이외 학생),
                          17:15(1,2,3조 발표자),
                          17:30(4,5,6조 발표자)

        - 최종발표
          2020.12.21.(월) 15:00
          https://meet.google.com/fuz-zfto-ddu 로 발표회 당일 14:30까지 개인별(1인)으로 접속
          * pc 사용자는 반드시 chrome으로 접속, 모바일 기기는 google meet 어플로 접속
          * 단, 발표자는 반드시 pc를 사용하여 접속






### 201203 (D-2W) 졸업 프로젝트 (종목추가,주가예측모델)

    # ppt 코로나 언택트 시대
    컨택트 소수가 아닌 >>> 스마트 머니가 소문을 움직였다면
    언택트 다수가 소문을 주도하는 시대가 된다.
    >>> 파도에 주동자가 있는가? 파도는 흐름일뿐 어느 물분자가 주도하지 않는다.
    >>> 세력도 이와 같다. 큰형님 주동자가 있는 경우도 있지만 파도와 같이 흐름으로 나타나는 것일뿐인 경우도 있다

    # ppt 소문에 사서 뉴스에 판다
    매수는 소문인덱스 갑자기 올라갔을때
    매도는 뉴스인덱스 (뉴스개수. 평균 일주일치 갯수가 하루에 나온날 또는 한달치 갯수가 일주일새 나온날) 많이 나온 당일
    또는 매수 시점의 목표가

    # ppt 또는 소문인덱스가 갑자기 꺾였을때도 매도 시점

    # ppt 주가예측 결과
    검증일치율
    테스트 일치율 다. 넣고
    카지노 승률 51프로
    작은차이가 반복된 시행에서 큰 차이

    ## ppt 비고
    좋아요는 의미있는 것 거르는 노이즈 제거용으로만.. 감성분석은 BERT 로만 했다고 하는게 더 좋아보임

    ## ppt 진흙속의 진주
    알라딘

    ## ppt 텍스트 시연 : bert 성능
    3상 통과 확실! >>>> 이것만 긍정인것 보여 줌
    3상 통과 할 수 있을까?
    3상 통과 택도 없다

    ## ppt 일론머스크
    페이팔? 테슬라? 스페이스엑스? GPT?
    GPT?!!

    ## ppt
    심리지수 넣기전 주가예측 성능과
    심리지수 인덱스.넣은 후 주가예측 성능 개선 정도 보여쥬기 (인덱스의 가치)
    -상승/ 하락예측(네이버 수집시 상승하락도 수집.
    - 6개 다 인풋으로 넣기(like 총 개수, dislike 총개수도)
    - 넣기전은 전일주가
    - 전일주가+VIX

    ## ppt
    이렇게 심리가 중요한데
    게시판은 심리 그 자체
    그런데 심리에 올라탈 것인가? 심리를 역이용해서 오버슈팅된 만큼을 빼고 가치를 볼것인가?
    >>> 과거 추이를 기반으로 딥러닝이 결정하게 하라
    >>> 인간은 데이터만 모아주고 판단은 AI가 딥러닝으로 한다
    (딥러닝 특징 : 블랙박스)

    # ppt 활용안 : 구독 서비스 마이데이터... 자산관리 서비스 판매.. (해외 국내 사례 수수료)
    구독서비스로 인덱스 데이터 판매
    (개인매매)



### 090 아래 999 등으로 이것저것 ppt 아이디어 써 놓은 것 중 쓸 만한 것 찾기
### 999 주가 예측 모델 개발
    lstm 한 종목내 패턴의 역사적인 재발생을 보기 위한 모델. 130일 내에 패턴의 재발생이.없었다
    한 종목내에서의 감정인덱스와 패턴을 빌견하기네 너무 짧은 기간

    감정인덱스- 생각보다 장기적인 추이.
    종목 발굴 매수매도 시점 시그널 용.
    (장기적으로)
    (단기적인 매일매일의 데이트레이딩용은 아님)

    그리고 한 종목내 페턴 발견보다
    감정인덱스의 변화는 1년 이상의 장기적인 패턴에 가깝다. 10년 정도 데이터가 쌓이먄 한 종목에 대해서도 가능하겠지만
    (LSTM은 중요한 정보는 계속 보존되는 모델)

    여러 종목의 패턴을 귀납적으로 분석해
    비슷한 패턴을 찾아내는게 10년 기다리는 것보다 실전적으로 돈벌려면 도움이 될것이다

    요즘 개인들 매매 패턴 - 리딩하는 그룹이 있고 따라산다. (유투브, 유료 리딩 정보 등)
    물론 바람직하지 않지만 선두 그룹의 패턴을 찾으면 먼저 시작하면 필승이다

    lstm은 단일종목 과거추이 보고 - 역사적으로 비슷한 패턴이 나오면 추측 (아주 오래전의 패턴도 기억)

    여러종목의 패턴을 학습시키는 것이 필요
    우리가 알고 싶은건 그래프를 보고 심리 인덱스가 뛰고 나면 거래량이 동반하면서 상승하고 곧 이는 주가상승이더라

    --- 그냥 lstm으로 여러종목 데이터 통으로 학습 해보기


    사람의매매-감정에 따른 비효율이 존재하고 프로그램매매는 이런 감정의 비효율 만큼을 먹고
    이득을 본다 (적정가격-오버슈팅, 언더슈팅? 그래프로 표시)

    기본적으로 사람도 인덱스 기반 매매를 하면 이런 비효율을 없앨 수 있다


    #샘플 (일단 주가 예측 말고 실수값 예측으로 해보기)
    >>> 거래 데이터는 종가 시가 포함해서 다시 만들기
    https://dsstudy.tistory.com/10

    LSTM RNN


    ## (6) 최종 주가예측모델(vix지표 포함) lstm으로 만들어 볼 수도?
    : 데이터랩팀 : 뉴스로 주가예측
    - 뉴스와 5일뒤 주가, 20일뒤 주가를 Y로 넣고 CNN 으로 모델을 학습해서 적용하고 있음



### 201208
- 전기차 : 새로운 기술이 아님
- 하둡 : 새로운 기술이 아님
- 자연어처리 : 새로운 기술이 아님
그런데 성능이 어느 기준을 넘기면서 활용용도가 폭발함


### 080 발표자료 및 발표 스크립트
    - BERT 가 긍정적인 댓글, 부정적인 댓글 잘 분류해주는 것 샘플로 보여줌 (편집의 힘!)
    - BERT로 만들어낸 소문지수가 주가에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)
    - 통계적인 글의 양이 거래량에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)
    - 주가예측에 유용한 지표로 심리지수 인덱스가 사용된 것 보여줌

    # 아이디어
        - 글의 갯수, 길이 등등도 피쳐로 딥러닝
        - 네이버무비 + 수작업 분류한 것 합쳐서 BERT 전이학습

    # 매수시점 신풍제약 그래프에 표시하기



### 100 #활용 쪽에 8,9번 작성
    7. 주가 상관관계 분석
    8. 예측모델생성
    9. 결과 정리 (그래프) >>> 활용방안. 마이데이터



### 040 lstm 분석 결과 장표로 추가
    : 멘트 잘 쓰기
        - 그래프 분석 및 상관관계 분석
        - 일일 주가 예측
        - 결론 : 일일 주가 예측 보다 종목발굴에 맞는 모델이다

    : 왜 시계열 분석(LSTM) 실패하는가?
      장기적 상승/하락 패턴은 몇년에 한번 발생하는 이벤트 .
      1년치 데이터로 패턴 학습이 불가.
      10년치를 모은다고 해도 > 고작 3650 개의 데이터

    : 어떻게 해결? 전 종목에 대한 유사패턴 학습으로 극복해야 함
      코스피 + 코스닥 2000 종목 * 365 =



### 060 상관관계 그래프 날짜 겹쳐보기 (범례 나눠서)


### 201208 32Page
    #32 페이지 그림 한장에 예측, 개선후 예측 다 찍어보기 - 범례3개 (주가, 추가전 예측, 추가후 예측)

    # 32페이지 총 3개 - 잘 안된 사유. 그림1, 그림2
    (아니면 셀트리온 같은 다른걸로 기간 길게 해서 학습해보기)

    - 인풋에 뭐 아웃풋에 뭐 들어간다 설명 (제곱 포함)
    - 개선 성능 미미.. 또는 모델의 정확도가 낮아서 개선의미가 없음..
    - 이유 - LSTM 은 중요한 정보는 잊지 않는다. 하지만 패턴 분석 1년은 너무 짧다... 100년도 길지 않다. 그리고 수직적 과거 100년 전의 패턴과 현재의 패턴은 다를 수 밖에없다
    - 여러종목을 통합해서 패턴분석 해야한다.
    - 즉, 원래 모델인 단일 종목에 대한 수직적 패턴 분석은 1~2년의 데이터로는 데이터 부족으로 패턴 학습을 못함. 원래 모델 자체의 문제로 개선효과 측정이 어려움.

    - 이쯤 되면 지표개발에서 너무 벗어나서..결론은 그래프로. 주가 예측이 본 프로젝트의 목표가 아님.
    - 인덱스 개발이 목표였고 그래프 상으로 의미가 있었으며 주가예측에 활용시. 단일종목에 대한 수직적 분석보다 전 종목에 대한 수평적 분석이 이루어져야 한다.(네이버 게시판 활성화는 2019년 부터)

    #
    32페이지 단일종목 시계열 쥬가 예측
    (짧게.. 잘 안된거는 짧게)
    - 인퓻에 뭐 아웃풋에 뭐
    - 인풋에 뭐뭐 추가(제곱 포함)(그림으로 표시)
    개선 성능 미미.. 또는 모델의 정확도가 낮아서 개선의미가 없음..

    이유 - LSTM 은 중요한 정보는잊지 않는다 하지만 패턴 분석 1년은 너무 짧다... 100년도 길지 않다. 그리고 수직적 과거 100뇬전의 패턴과 현재의 패턴은 다를 수 밖에없다
    --- 여러종목을 통합해서 패턴분석 해야ㅠ한다.

    즉, 원래 모델인 단일 종목에 대한 슈직적 패턴 분섣은 1~2년의 데이터로는 데이터 부족으로 패턴 학습을 못함. 원래 모델 자체의 문제로 개선효과 측정이 어려움.

    이쯤 되면 지표개발에서 너무 벗어나서..결론은 그래프로. 주가 예측이 본 프로젝트의 목표가 아님.

    인덱스 개발이 목표였고 그래프 상으로 의미가 있었으며 주가예측에 활용시. 담일종목에 대한 수직적 분석보다 전 종목에ㅜ대한 수평적 분석이 이루어 져야ㅠ한다.(네이버 게시판 활성화는 2019년 부터)


    #
    이유는 우리가 앞선 그래프 분석시에 발견한 가설인
    심리 인덱스의 급격한 개선 이벤트 발생
    시간차를 두고 거래량 급상승
     거래량 상승 이후 주가 급상승
    의 선 순환은 학습기간동안 단 한번 발생한 이벤트로


    소문에 사서 뉴스에 파는 모델을 제대로 학습하려면
    한 종목을 시계열로 학습할게 아니라
    가능하면 많은 종목의 주가흐름과 감정인덱스 흐름을
    귀납적으로 학습해야 하기때문


    따라서 본 프로젝트에서는 그래프로 감정인덱스의 중요성을 발견하는 선에서 목표달성하고 마무리
    마이데이터 시대를 맞아 인덱스 오픈API 판매, 실시간 제공 ... 서비스, 트레이딩 부서 활용. 고객에게 시그널 (종목포착) 서비스


### 201208 #활용
    ## 2차 3차 그래프 분석 해보고 괜찮은 것 활용 부분에 추가하기
    (LSTM 도 기간 길게 해서 다시 학습해보기. 급상승만 있는것 말고 등락이 있는 걸로)




### 201208 주가예측  상승/하락 예측
    매수/매도 포지션  롱/숏 >>> 아웃풋...

    하지만 급상승기를 학습데이터로 써서
    테스트 시에도 주가예측이 정확하지 못했으므로 크게 의미있다고 보기 어려움


### 999 마무리
    # 일론머스크 GPT -3
    : BERT 라는 거인의 어깨에 올라선 것 만으로 의미있는 결과물을 만들어 낼 수 있었다.
    GPT-3 라는 거인의 어깨에 올라설 수 있게 되면 어떤 결과물을 만들수 있을지 기대되지 않으십니까
    언젠가는 GPT-3도 풀릴겁니다

    # EOD 맺음말 - 케네디
    : 케네디가 1962년에 이 연설을 하고 우주공학에 엄청난 투자를 합니다
    그리고 1969년에 달나라에 사람을 보냅니다

    : 지금이 인공지능 분야로 치면 달나라에 사람을 보내기 직전인듯 합니다
    인공지능의 발전이 폭발 직전입니다.
    저희가 2년동안 쉽지 않았지만 배운걸로
    언젠가는 회사도 달나라 보내고
    한가지 더 희망사항이 있다면 제 계좌도 달나라 갔으면 좋겠습니다



### 201208 발표 노트
        첫번째 아이디어는
        인터넷 커뮤니티 감정분석을 통한
        개인 심리지수의 개발입니다

        올해, 코로나는 많은 변화를 가져왔습니다
        주식시장도 크게 변했는데요
        과거에는 개인이 외인, 기관을 단순히 추종했다면
        코로나 이후에는
        동학개미, 로빈후드로 불리며
        오히려 시장을 개인이 주도하는 역전현상이 일어났습니다
        기술주를 중심으로 스스로 분석하고
        인터넷을 통해 의견을 공유하기 시작했습니다
        그에 따라서 개인의 동향을 예측하고 대응하는 것이
        중요해졌는데요
        과거의 분석은 가치나 기관 외인의 수급분석 위주로
        진행되었기 때문에
        개인의 동향에 대한 지표는 후행적인 지표인
        개인신용잔고 정도 밖에  잘 활용되지 못하는 실정입니다


        그리고 기술적인 부분을 생각해보면
        최근에 AI 활용측면에서
        가장 진보가 빠른 영역이 언어 모델일 것입니다
        심지어 GPT-3 같은 경우 너무 강력해서
        공개조차 보류될 정도인데요
        하지만 기술은 언젠가는 대중화되고 오픈될 것입니다
        학부생들이 대학과제로 GPT-3를 사용한 개발을 하게될거고
        기업에서도 일상적으로 활용하게 될 날이
        그렇게 오래 걸리지는 않을 것 같습니다


        그래서 저희 조에서 어떤 것을 할 것이냐면
        먼저, 뉴스 하나를 보여드리겠습니다
        한국은행에서 뉴스를 분석한 심리지수를
        만든다는 기사인데요
        미국 샌프란시스코 연방준비은행이 개발한
        뉴스심리지수를 본뜬 것이라고 합니다
        미국의 뉴스심리지수와 소비자심리지수를
        비교한 표인데요
        상당한 상관관계를 가지는 것을 볼 수 있습니다


        여기 개인들이 자유롭게 주식에 관한 얘기를 주고받는
        많은 커뮤니티들이 있습니다
        바다처럼 광대하고 대부분은 영양가가 없는
        글 들이라 무시됩니다 99%는 쓸모가 없습니다
        하지만 자동화된 스크래핑 기술과 감정분석으로
        그 속에 녹아있는 개인들의 심리를 파악할 수 있다면
        바닷물에서 소금을 만들어 내듯이
        가치를 만들어 낼 수 있지 않을까요
        그래서 이 프로젝트를 통해서
        인터넷 커뮤니티를 감정분석하여
        개인들의 매일매일의 투자심리를
        나타내는 지표를 만드려고 합니다
        그렇게 만들어진 지표를 다음날의 주가와
        상관관계 분석까지 해보려고 하고요
        선행관계가 나타나면 대박이고
        후행관계가 발견되더라도
        갭 차이가 벌어지면 매수/매도 신호로 활용할 수 있습니다
        상관관계 분석에 1차 실패 하더라도
        다음날의 개인 수급 및 개인주도주의 주가예측에
        유용한 지표로 고객 제공이 가능할 것입니다

        이 첫번째 과제는 기본적으로 스크래핑된 커뮤니티 데이터로
        감정분석, 상관관계를 수행하기 때문에
        데이터 입수와 기술적 구현상에서
        장애물이 상당히 적은 편이며
        그만큼 프로젝트의 최종 구현성공 가능성도
        높은 편입니다

        10년 전만 해도 주식 종목 코드를 많이 외우는게
        브로커의 굉장한 덕목이자
        재능으로 취급 받았다



        자연어처리 과목 프로젝트에서 89%의 정확도를
        보인 Bert 모델을 활용함
        (Bert 멀티랭귀지 모델은 어마어마한 양의 텍스트 학습을 기반으로 자연어 전반에 대한 이해 + 한글학습은 네이버 영화평으로 1차 전이학습했음
        (한글 전용 bert 모델을 쓰면 더 나을 수도 있지만 프로젝트 시간상 익숙한 걸로... 제너럴한 한글 Bert 성능표 참조)



        실제 DB 에 쌓은 모습
        단 네이버 영화평으로 한글은 익혔지만
        “머선12go?!”  “가즈아!”  “전강후약” “상한가10방”
        등 주식에 특화된 한글은 아직 잘 이해못함
        추가 데이터를 손으로 만들어서 전이학습 필요
        그리고 인터넷의 글은 노이즈가 굉장히 많음
        추천수 1이상인것 만 따로 모아서 인덱스 만들어볼 예정



        “3상 통과” , “FDA 승인 허가” 같이 종목별로 긍정 부정이 다르다

        만약에 몇개 종목만 집중한다면 종목별로  언어모델 학습도 다르게 하는것이 좋음. 본 프로젝트는 시간상 범용적으로 함.
        (ex) “김정일 트럼프 만남” “미사일 발사” 등은 방산주와 남북경협주에 전혀 반대되는긍정/부정의 의미를 가짐




        저장된 모델을 불러와서
        Classification을 수행한 모습
        > 최종결과는 다시 DB 에 저장

        >>> 본 프로젝트에서 나온 인덱스로 개인들의 심리지수 파악가능
        >>> 그것도 종목별로. 학습데이터를 만들어서 계속 학습 시키면
        어디에도 없는 개인들의 주식심리 분석에 특화된 언어모델을 만들 수 있음


        커뮤니티 글로 일별 심리지수 인덱스 생성시 날짜 끊는 기준

        : 아침 08:30 이전까지 전일자에 포함 시킴
        (동시호가 08:30~09:00)

        실제 주식 매매시 실시간 인프라가 잘 되어 있다는 가정하에
        08:30 데이터 까지 전일의 데이터로 보고
        당일의 주가 예측에 활용 할 수 있음


        사실 한국은행이 뉴스 심리지수 만든것처럼 개인커뮤니티심리지수를 만드는것 까지가 본 프로젝트의 1차 목표이고
        이렇게 생성된 인덱스를 개인고객이나 사내 트레이딩부서에
        제공하는 것까지만 1차 목표지만
        지속적 개선을 위해 품질 판단 기준이 필요하고

        이 개인심리지표가 얼마나 중요한지 와닫지 않으니까 상관관계 분석이나 주가 예측 모델까지 해보겠음. 단, 의미있는 수준으로 하려면 주식커뮤니티 글에 특화된 학습데이터를 지속적으로 만들면서 성능을 개선해야 함. 본 프로젝트에서는 주가예측모델은 프로토타입 수준에서 진행

        하지만 급상승기를 학습데이터로 써서
        테스트 시에도 주가예측이 정확하지 못했으므로 크게 의미있다고 보기 어려움
        이유는 우리가 앞선 그래프 분석시에 발견한 가설인
        1. 심리 인덱스의 급격한 개선 이벤트 발생
        2. 시간차를 두고 거래량 급상승
        3. 거래량 상승 이후 주가 급상승
        의 선 순환은 학습기간동안 단 한번 발생한 이벤트로

        소문에 사서 뉴스에 파는 모델을 제대로 학습하려면
        한 종목을 시계열로 학습할게 아니라
        가능하면 많은 종목의 주가흐름과 감정인덱스 흐름을
        귀납적으로 학습해야 하기때문

        따라서 본 프로젝트에서는 그래프로 감정인덱스의 중요성을 발견하는 선에서 목표달성하고 마무리
        마이데이터 시대를 맞아 인덱스 판매, 실시간 제공



### 20201208
그리고 기술적인 부분을 생각해보면
최근에 AI 활용측면에서
가장 진보가 빠른 영역이 언어 모델일 것입니다
심지어 GPT-3 같은 경우 너무 강력해서
공개조차 보류될 정도인데요
하지만 기술은 언젠가는 대중화되고 오픈될 것입니다
학부생들이 대학과제로 GPT-3를 사용한 개발을 하게될거고
기업에서도 일상적으로 활용하게 될 날이
그렇게 오래 걸리지는 않을 것 같습니다


### 201208
31 Page 돌발퀴즈
셜
설명시 인덱스 급상승 날짜 주가 3만원 정도
채팅창에 신풍제약 고점가격 제일먼저-스마트펜



### 900 데이터 만들어서 모델 실제 추가 전이학습 >>> 관련 장표만 만들기
    : 수작업 라벨링
    : 한 1000건 정도 했다고 하기

    # ppt Bert 설명 : 전이학습 : 거인의 어깨위에서
    BERT 거인위에
    > 네이버로 한글학습, 인터넷 구어 학습
    > 수작업 라벨링 데이터로 주식 전용 언어 학습




### 020 19 Page 추가 예정 (인터넷언어 학습 + 주식 언어 학습을 어떻게 시켰는지. BERT와 전이학습 설명. 거인의 어깨위에 ...)
        : 5번(BERT) 만 정리하면
          1,2,3,4,5,6 까지의 #배경, #구현 부분은 완성됨

            - 그래프 스케일 조정해서 하나로 합쳐진것
            - BERT 로 실제 분류되는 것 샘플 : 떡상이다! 호재가 만발하네요. 지금이 바닥이네요
            - BERT 성능비교표 (한버트)
            >>> 여기까지 하고 조원들 피드백



### 030 Bert 비교
    https://beomi.github.io/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/
    # 보편적인 언어 모델 > 한글 인터넷 언어에 특화된 모델 > 도메인에 특화된 모델
    https://www.semanticscholar.org/paper/KR-BERT%3A-A-Small-Scale-Korean-Specific-Language-Lee-Jang/c0ba595b2bef54f2552ec4716bb187901f52f4a3
    # NSMC
    NSMC KoBERT
    # SK텔레콤은 '언어신동 AI’에 어떻게 한국어를 가르쳤을까
    https://www.ajunews.com/view/20201011091342159
    # 이 SK텔레콤의 AI 기술들은 일부 NLP 영역에서 기존 한국어 AI 기술보다 뛰어난 성능을 보여 준다. SK텔레콤 ALT Labs 측은 "네이버 영화 리뷰 데이터(NSMC)를 활용한 '감정분석 분류' 작업에서 KoBERT는 90.1%, KoGPT2는 89.9%의 성능(정확도)을 보인다"며 "KoBERT 사용 이전에 이 작업은 일반적으로 83~85%의 정확도를 보인다고 알려져 있었다"고 설명했다
    # 5조 89%로 1등 한것도 붙여넣기
    (1) Multilingual pretrained model
    (2) SKT  : Kobert
    (3) ETRI : KorBert
    (4) HanBERT


### 019 전이학습 설명 페이지
        # 전이학습 설명 : 왼쪽 항목설명(거인그림), 오른쪽 전이학습에 사용된 데이터 설명
        [] : 종목별 모델 분리 하여 특화된 긍정/부정 학습 >>>> 본 프로젝트의 범위에서는 제외
        [] : 주식 전용 용어 학습  (수작업 데이터 분류)
        [] : 한글 인터넷 언어 전이학습 (네이버 영화평 데이터)
        [] : 버트 멀티랭귀지 모델 - 인류공통의 언어라는 것에 대한 신경망 (거인그림)

        https://ebbnflow.tistory.com/151



### 018 2020 1학기 BERT 중간발표 PPT 읽어보기
    # BERT 설명 : 장성섭이... 추천합니다.. 이 페이지 쓸만한데 > 주식으로 바꾸면 더좋을 듯. (코랩에서 화면 만들기)
    # 거인의 어깨에 올라서서 >> 전이학습 설명할 때 쓸 만할 듯.


### 017 분류한것 샘플
SELECT DATE, TITLE, CONTENT, label FROM T019170_label
WHERE DATE LIKE '2020.06.20%'



### 015 삼성전자, 신풍제약 한 줄로 그래프 세우기


### 016 LSTM 분석 일단 붙여넣은 장표 만들어 놓기




### 700 3차 수집 대상 종목 발굴 및 스크래핑 (주가보다 개인 심리가 먼저 움직인 종목)

### 201204 23,24 page 조회 이미지 수정

### 010 18 Page 추가 예정 (노이즈 데이터 처리방식 : 좋아요가 더 많은 글)



### 001 PPT 배경 사진 교체
    : 셔터스톡
    PICK10FREE
    첫달 이내에 취소가능



### 100 PPT
    # 3 활용
        - 그래프 분석 및 상관관계 분석
        - 일일 주가 예측
        - 결론 : 일일 주가 예측 보다 종목발굴에 맞는 모델이다


### 100 PPT
    : 일단 내가 오리지날로 처음부터 끝까지 만들고
      장수석님 장표 중 쓸만한 부분 따와서 넣자
      (처음부터 끝까지 같은 호흡으로 만들어야 통일성이 있음)

    커뮤니티 심리지수로 종목발굴
    - 소문에 사서 뉴스에 판다 -

    : PPT 작성은 2>3>1 순서로 작성 (1은 다른 사람이 작성해도 됨)

    # 1 배경
    # 2 구현
    # 3 활용
        - 그래프 분석 및 상관관계 분석
        - 일일 주가 예측
        - 결론 : 일일 주가 예측 보다 종목발굴에 맞는 모델이다


### 201203 (D-2W) 졸업 프로젝트 (종목추가,주가예측모델)
    # Bert 설명 : 전이학습 : 거인의 어깨위에서
    BERT 거인위에
    > 네이버로 한글학습, 인터넷 구어 학습
    > 수작업 라벨링 데이터로 주식 전용 언어 학습


    # ★발표는 그냥 끝까지 믿고 맡긴다.
    대신 자료 가다듬고 발표 리허설 하면서 피드백을 통해서
    내가 원하는 사항을 반영
    >>> 팀으로 일하는 능력 및 팀장 역할 연습
    + 빅데이터 지원군 양성 (선발대)


    # 코로나 언택트 시대
    컨택트 소수가 아닌 >>> 스마트 머니가 소문을 움직였다면
    언택트 자수가 소문을 주도하는 시대가 된다.
    >>> 파도에 주동자가 있는가? 파도는 흐름일뿐 어느 물분자가 주도하지 않는다.
    >>> 세력도 이와 같다. 큰형님 주동자가 있는 경우도 있지만 파도와 같이 흐름으로 나타나는 것일뿐인 경우도 있다



    # 구독 서비스 마이데이터... 자산관리 서비스 판매.. (해외 국내 사례 수수료)
    구독서비스로 인덱스 데이터 판매
    (개인매매)

    # 추가종목 - 씨젠, 수젠텍,
    엔투텍, 종근당바이오, 종근당, 녹십자, 키움증권, 미래에셋대우

    [매주 화 18:30 - 21:30] 나 수업
    [매주 화 19:00~22:00] 지연 줌 수업


    ## (6) 최종 주가예측모델(vix지표 포함) lstm으로 만들어 볼 수도?
    : 데이터랩팀 : 뉴스로 주가예측
    - 뉴스와 5일뒤 주가, 20일뒤 주가를 Y로 넣고 CNN 으로 모델을 학습해서 적용하고 있음



    # 소문에 사서 뉴스에 판다
    매수는 소문인덱스 갑자기 올라갔을때
    매도는 뉴스인덱스 (뉴스개수. 평균 일주일치 갯수가 하루에 나온날 또는 한달치 갯수가 일주일새 나온날) 많이 나온 당일
    또는 매수 시점의 목표가

    # 또는 소문인덱스가 갑자기 꺾였을때도 매도 시점


    # 주가예측 결과
    검증일치율
    테스트 일치율 다. 넣고
    카지노 승률 51프로
    작은차이가 반복된 시행에서 큰 차이

    ## 비고
    좋아요는 의미있는 것 거르는 노이즈 제거용으로만.. 감성분석은 BERT 로만 했다고 하는게 더 좋아보임

    ## ppt 진흙속의 진주
    알라딘

    ## ppt 텍스트 시연 : bert 성능
    3상 통과 확실! >>>> 이것만 긍정인것 보여 줌
    3상 통과 할 수 있을까?
    3상 통과 택도 없다


    ## ppt 일론머스크
    페이팔? 테슬라? 스페이스엑스? GPT?
    GPT?!!

    ## ppt
    심리지수 넣기전 주가예측 성능과
    심리지수 인덱스.넣은 후 주가예측 성능 개선 정도 보여쥬기 (인덱스의 가치)
    -상승/ 하락예측(네이버 수집시 상승하락도 수집.
    - 6개 다 인풋으로 넣기(like 총 개수, dislike 총개수도)
    - 넣기전은 전일주가
    - 전일주가+VIX

    ## ppt
    이렇게 심리가 중요한데
    게시판은 심리 그 자체
    그런데 심리에 올라탈 것인가? 심리를 역이용해서 오버슈팅된 만큼을 빼고 가치를 볼것인가?
    >>> 과거 추이를 기반으로 딥러닝이 결정하게 하라
    >>> 인간은 데이터만 모아주고 판단은 AI가 딥러닝으로 한다
    (딥러닝 특징 : 블랙박스)


    ## 프로토타입
    (1) MariaDB : 데이터 수집한 것 하나로 합치기
    (2) MariaDB : 라벨 컬럼 하나 추가해서 합친 테이블 만들기
    (3) Colab 으로 모델 불러오기
         + DB 불러오기
         + 모델링 해서 라벨 달기
         + 다시 DB에 저장하기
    (4) 주가, 거래량 데이터 어디서 구하기 : 거래소? 회사내?


    ## 도서관 외 미팅 공간
    : 오픈된 공간이 더 좋음. 칸막이 있으면 별로.
    - 도서관 : 08:00~22:00 (하나스퀘어, 4층 노트북)
    # 과도 5층 도서관 앞 휴게룸
    # 과도 4층 휴게실
    # 과도1층 인피니트라운지
    # 지하1층 델타라운지
    # 하스 중앙 피아노실
    # 과도1층 문쪽 공간(와이파이 잘 안됨)


    ## 일정
    12 [D-4W] 11/17 2,3 이상근, 김현우
    13 [D-3W] ★11/24 4,5 강재우, `
    14 [D-2W] 12/1 1,6 정순영, 임희석
    15 [D-1W] 12/8 2,3 이상근, 김현우
    16 [D-W]★ ★ 12/15 학교최종발표 : 1,2,3,4,5,6 정순영, 이상근, 김현우, 강재우, 백승준, 임희석
    ## 조편성
    강재우 - 유근영
    백승준 - 박준하
    정순영 - 이인희
    임희석 - 노시희, 오세현
    이상근 - 백전호, 성다야
    김현우 - 김유석

    ## 201104 회식
    프로젝트 팀원과 지도교수님의 식사는
    기존 이용하시던 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부) 외 더씨, 무르무르드구스토 에서도 가능합니다^^
    (금액은 1회 30만원정도로 생각하고 있으나, 특별히 제한은 없는 것으로 하겠습니다)

    다만 선결제가 필요한 식당이 있어,
    식사하시기 전에 일정과 식사장소를 행정실로 전달 부탁드리겠습니다~

    학생분들께 제공해드렸던 식대지원(1인 12,000원까지)은 기존에 안내드린 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부)에서만 이용가능하십니다!

### 900 데이터 만들어서 모델 추가 전이학습
    : 수작업 라벨링
    : Like 높은 것 : KODEX 레버리지로 학습시키기
    : 구글문서로 올려서 협업하기


### 800 발표자료
- BERT 가 긍정적인 댓글, 부정적인 댓글 잘 분류해주는 것 샘플로 보여줌 (편집의 힘!)
- BERT로 만들어낸 소문지수가 주가에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)
- 통계적인 글의 양이 거래량에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)
- 주가예측에 유용한 지표로 심리지수 인덱스가 사용된 것 보여줌


### 910 그냥 상승/하락 잘 안되면 코스닥 대비 상승/하락 구해서 .. 코스닥 대비 상승률 예측



### 201128 시장정보에 정보 단순화 해서 없앴던것 더 넣어서 주가예측 다시 해보기
## 익일 종가 정보 예측
## 상승 하락 1 0 으로 예측



### 201122
lstm polynomial
add polynomial features



### 201110 LSTM 주가 예측 모델
//# 휴일자 처리 ???
//# 전일의 데이터를 기초로 익일의 주가를 예측.
//# 날짜별로 된 샘플 찾기 - INPUT 만 바꿔주면 됨
https://dsstudy.tistory.com/10#
file:///C:/Users/user/Desktop/201122%20%EC%A3%BC%EA%B0%80%20%EC%98%88%EC%B8%A1%20%EB%AA%A8%EB%8D%B8%20%EC%83%98%ED%94%8C/%EA%B0%84%EB%8B%A8%ED%95%9C%20%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9C%BC%EB%A1%9C%20%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90%20%EC%A3%BC%EA%B0%80%20%EC%98%88%EC%B8%A1%ED%95%98%EA%B8%B0.mhtml

SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T005930_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T006280_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T019170_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T058820_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T122630_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T252670_index ORDER BY NEW_BAS_DT;

SELECT DATE_FORMAT(NEW_BAS_DT,'%Y-%m-%d') AS NEW_BAS_DT

SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T005930_index
WHERE NEW_BAS_DT NOT IN
('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
ORDER BY NEW_BAS_DT;


SELECT PRICE, UPDOWN, AMOUNT FROM T005930_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T006280_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T019170_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T058820_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T122630_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T252670_market ORDER BY NEW_BAS_DT;




### 합치기

DROP TABLE T005930_all;
CREATE TABLE T005930_all AS
WITH V1 AS (
SELECT NEW_BAS_DT, 익일종가, 종가, 대비, 거래량, 시가, 고가, 저가 FROM T005930_market_all
WHERE NEW_BAS_DT NOT IN ('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
AND NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT
),
V2 AS (
SELECT * FROM T005930_index
WHERE NEW_BAS_DT NOT IN ('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
AND NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT
)
SELECT V1.NEW_BAS_DT,   익일종가,   종가, 대비, 거래량,    시가, 고가, 저가, CNT,    EMO_I,  EMO_RT, R_CNT,  R_EMO_I,    R_EMO_RT FROM V1, V2
WHERE V1.NEW_BAS_DT = V2.NEW_BAS_DT;
SELECT COUNT(*) FROM T005930_all;


DROP TABLE T019170_all;
CREATE TABLE T019170_all AS
WITH V1 AS (
SELECT NEW_BAS_DT, 익일종가, 종가, 대비, 거래량, 시가, 고가, 저가 FROM T019170_market_all
WHERE NEW_BAS_DT NOT IN ('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
AND NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT
),
V2 AS (
SELECT * FROM T019170_index
WHERE NEW_BAS_DT NOT IN ('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
AND NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT
)
SELECT V1.NEW_BAS_DT,   익일종가,   종가, 대비, 거래량,    시가, 고가, 저가, CNT,    EMO_I,  EMO_RT, R_CNT,  R_EMO_I,    R_EMO_RT FROM V1, V2
WHERE V1.NEW_BAS_DT = V2.NEW_BAS_DT;
SELECT COUNT(*) FROM T019170_all;



SELECT * FROM T005930_market_all
WHERE NEW_BAS_DT NOT IN ('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
AND NEW_BAS_DT BETWEEN '20200101' AND '20201009'




### 201128 시장정보에 정보 단순화 해서 없앴던것 더 넣어서 주가예측 다시 해보기
https://m.blog.naver.com/wideeyed/221160038616

//T005930_market_all 삼성전자
//T019170_market_all 신풍제약




CREATE TABLE `T005930_market_all` (
    `NEW_BAS_DT` VARCHAR(12),
    `익일종가` BIGINT(21) ,
    `익일대비` BIGINT(21) ,
    `익일UPDOWN` BIGINT(21) ,
    `종가` BIGINT(21) ,
    `대비` BIGINT(21) ,
    `거래량` BIGINT(21) ,
    `거래대금` BIGINT(21),
    `시가` BIGINT(21) ,
    `고가` BIGINT(21) ,
    `저가` BIGINT(21) ,
    `시가총액` BIGINT(21) ,
    `상장주식수` BIGINT(21)
)

## 시장정보 추가
    : 삼성전자, 신풍제약만

## 익일 종가 정보 붙이기

## 상승 하락 1 0 으로 붙이기



### 201115 part5_일자별 주가, 일자별 거래량 가져오기 - DB에 넣기

//T005930_market_all 삼성전자
//T019170_market_all 신풍제약


CREATE TABLE `T005930_market_all` (
    `NEW_BAS_DT` VARCHAR(12),
    `익일종가` BIGINT(21) ,
    `익일대비` BIGINT(21) ,
    `익일UPDOWN` BIGINT(21) ,
    `종가` BIGINT(21) ,
    `대비` BIGINT(21) ,
    `거래량` BIGINT(21) ,
    `거래대금` BIGINT(21),
    `시가` BIGINT(21) ,
    `고가` BIGINT(21) ,
    `저가` BIGINT(21) ,
    `시가총액` BIGINT(21) ,
    `상장주식수` BIGINT(21)
)
COLLATE='utf8_general_ci'
ENGINE=InnoDB
;
년월일,종가,대비,거래량(주),거래대금(원),시가,고가,저가,시가총액(백만),상장주식수(주)

### 201115 조원들 엑셀 공유
>>> word 파일, 엑셀로 공유
>>> CMG 제약에서 원하는 그림이 나오나?


>>> 좋은 종목 추천 해달라.. 주가가 오르기 전에 게시판에 해당종목 긍정적인 의견, 열성팬들이 나타나기 시작한 종목,  주가가 오르기전에 좋은소식, 찌라시가 먼저 돌앗던것


### 201115 part6_상관관계 분석.ipynb
# 상관관계 분석
: 글의 갯수, 감정분석 비율
: 주가, 거래량, 투자자별 매매동향 ( 거래소? )

# 상관관계 - 그래프 그려보기 : 상관관계도 분석 : 지난 학기 과제4번? 참조
# 상관관계 분석은 당일것 하고 비교도 해보고 전일것 하고도 비교해보기

# 엑셀 상관관계 분석 메뉴얼
# https://m.blog.naver.com/windkiy/221293099492
# https://m.blog.naver.com/PostView.nhn?blogId=windkiy&logNo=221300940369&proxyReferer=https:%2F%2Fwww.google.com%2F

# 그냥 상승/하락 잘 안되면 코스닥 대비 상승/하락 구해서 .. 코스닥 대비 상승률 예측
# 휴일자 처리 ???



SELECT * FROM T005930 LIMIT 10;
SELECT * FROM T006280 LIMIT 10;
SELECT * FROM T019170 LIMIT 10;
SELECT * FROM T058820 LIMIT 10;
SELECT * FROM T122630 LIMIT 10;
SELECT * FROM T252670 LIMIT 10;


SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T005930_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T006280_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T019170_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T058820_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T122630_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T252670_index ORDER BY NEW_BAS_DT;

SELECT DATE_FORMAT(NEW_BAS_DT,'%Y-%m-%d') AS NEW_BAS_DT

SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T006280_index
WHERE NEW_BAS_DT NOT IN
('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
ORDER BY NEW_BAS_DT;


SELECT PRICE, UPDOWN, AMOUNT FROM T005930_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T006280_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T019170_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T058820_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T122630_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T252670_market ORDER BY NEW_BAS_DT;





### 201115 part5_일자별 주가, 일자별 거래량 가져오기 - 게시판 글 휴일자 빼고 뽑기
감정인덱스_주가(영업일)_20201115_v1.xlsx

SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T005930_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T006280_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T019170_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T058820_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T122630_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T252670_index ORDER BY NEW_BAS_DT;

SELECT DATE_FORMAT(NEW_BAS_DT,'%Y-%m-%d') AS NEW_BAS_DT

SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T005930_index
WHERE NEW_BAS_DT NOT IN
('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')
ORDER BY NEW_BAS_DT;


SELECT PRICE, UPDOWN, AMOUNT FROM T005930_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T006280_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T019170_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T058820_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T122630_market ORDER BY NEW_BAS_DT;
SELECT PRICE, UPDOWN, AMOUNT FROM T252670_market ORDER BY NEW_BAS_DT;



### 201115 part5_일자별 주가, 일자별 거래량 가져오기 - 엑셀에 추가
# 게시판 글 휴일자 빼고 다시 뽑기
# 주가, 거래량 그래프 추가, 등락 추가
# 6개 종목 다시 그리기
# 주석은 표지에 달기 : 다시 만들기 쉽게


### 201115 part5_일자별 주가, 일자별 거래량 가져오기 - 휴일자 구하기
# 빼기
SELECT COUNT(*) FROM T005930_market UNION ALL /* 삼성전자             */
SELECT COUNT(*) FROM T006280_market UNION ALL /* 녹십자               */
SELECT COUNT(*) FROM T019170_market UNION ALL /* 신풍제약             */
SELECT COUNT(*) FROM T058820_market UNION ALL /* CMG제약              */
SELECT COUNT(*) FROM T122630_market UNION ALL /* KODEX레버리지        */
SELECT COUNT(*) FROM T252670_market ;         /* KODEX200선물인버스2X */

SELECT COUNT(*) FROM T005930_index UNION ALL
SELECT COUNT(*) FROM T006280_index UNION ALL
SELECT COUNT(*) FROM T019170_index UNION ALL
SELECT COUNT(*) FROM T058820_index UNION ALL
SELECT COUNT(*) FROM T122630_index UNION ALL
SELECT COUNT(*) FROM T252670_index ;

SELECT NEW_BAS_DT FROM T005930_index
where NEW_BAS_DT NOT IN (
SELECT NEW_BAS_DT FROM T005930_market
)

('20200101','20200104','20200105','20200111','20200112','20200118','20200119','20200124','20200125','20200126','20200127','20200201','20200202','20200208','20200209','20200215','20200216','20200222','20200223','20200229','20200301','20200307','20200308','20200314','20200315','20200321','20200322','20200328','20200329','20200404','20200405','20200411','20200412','20200415','20200418','20200419','20200425','20200426','20200430','20200501','20200502','20200503','20200505','20200509','20200510','20200516','20200517','20200523','20200524','20200530','20200531','20200606','20200607','20200613','20200614','20200620','20200621','20200627','20200628','20200704','20200705','20200711','20200712','20200718','20200719','20200725','20200726','20200801','20200802','20200808','20200809','20200815','20200816','20200817','20200822','20200823','20200829','20200830','20200905','20200906','20200912','20200913','20200919','20200920','20200926','20200927','20200930','20201001','20201002','20201003','20201004','20201009')



### 201115 part5_일자별 주가, 일자별 거래량 가져오기 - DB에 넣기

//T005930_market 삼성전자
//T006280_market 녹십자
//T019170_market 신풍제약
//T058820_market CMG제약
//T122630_market KODEX레버리지
//T252670_market KODEX200선물인버스2X


CREATE TABLE `T006280_market` (
    `NEW_BAS_DT` VARCHAR(12),
    `PRICE` BIGINT(21) NOT NULL,
    `UPDOWN` BIGINT(21) NOT NULL,
    `AMOUNT` BIGINT(21) NOT NULL
)
COLLATE='utf8_general_ci'
ENGINE=InnoDB
;



    /* 삼성전자 */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
    SELECT COUNT(*) FROM T005930; /*415828 175MB*/

    /* 녹십자 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
    SELECT COUNT(*) FROM T006280; /*59933 28MB*/

    /* 신풍제약 */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
    SELECT COUNT(*) FROM T019170; /*469022 186MB*/

    /* CMG제약 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
    SELECT COUNT(*) FROM T058820; /*81848 30MB*/

    /* KODEX레버리지 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
    SELECT COUNT(*) FROM T122630; /*171048 59MB*/

    /* KODEX200선물인버스2X */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
    SELECT COUNT(*) FROM T252670; /*339407 135MB*/


### 201115 part5_일자별 주가, 일자별 거래량 가져오기
거래소 데이터 일별시세

### 201115 part5_일자별 주가, 일자별 거래량 가져오기
/*
: 주가
: 거래량
: 상승/하락 구분
: 상승/하락 수치
# 네이버 ???
# 거래소 ???
# 회사데이터 ???
*/
# 네이버 일별시세 스크래핑 하기

            <iframe name='day' src='/item/sise_day.nhn?code=058820' width="100%"  height=360 marginheight=0 bottommargin=0 topmargin=0 SCROLLING=no frameborder=0 title="일별 시세"></iframe>


https://finance.naver.com/item/sise.nhn?code=058820
https://finance.naver.com/item/sise_day.nhn?code=058820
https://finance.naver.com/item/sise_day.nhn?code=058820&page=1


### 201115 part4_일별인덱스생성_감성_갯수 - 060
# 엑셀로 인덱스들 그래프 그려보기


/*
*/
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T005930_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T006280_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T019170_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T058820_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T122630_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T252670_index ORDER BY NEW_BAS_DT;

INSERT INTO T006280_index
SELECT '20200104',0,0,0,0,0,0 ;

### 201115 part4_일별인덱스생성_감성_갯수 - 050 정리해서 6개 종목다 별도 테이블로 만들기
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 일별 글 건수 인덱스 만들기 - DB작업   R_CNT
# 일별 감정 지수 인덱스 만들기 - DB작업 R_EMO_I

# 일별 글 건수 인덱스  : CNT
# 일별 감정 지수 인덱스  : EMO_I
# EMO_RT : 감정지수 비율 인덱스

# 일별 글 건수 인덱스   R_CNT  (추천 수 1 이상이고 추천이 더 많은 것 중)
# 일별 감정 지수 인덱스 R_EMO_I (추천 수 1 이상이고 추천이 더 많은 것 중)
# R_EMO_RT : 감정지수 비율 인덱스  (추천 수 1 이상이고 추천이 더 많은 것 중)


CREATE TABLE T006280_index AS
WITH V1 AS
( SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, CASE WHEN A.`LIKE` >= 1 AND `LIKE` > `DISLIKE` THEN 1 ELSE 0 END AS LIKE_I
, A.*
FROM T006280_label A
)
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
, CASE WHEN COUNT(*) = 0 THEN 0 ELSE NVL(SUM(label) / COUNT(*),0) END AS EMO_RT
, SUM(LIKE_I) AS  R_CNT
, SUM(LIKE_I*label) AS  R_EMO_I
, CASE WHEN SUM(LIKE_I) = 0 THEN 0 ELSE NVL(SUM(LIKE_I*label) / SUM(LIKE_I) ,0) END AS R_EMO_RT
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT


WITH V1 AS
( SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, CASE WHEN A.`LIKE` >= 1 AND `LIKE` > `DISLIKE` THEN 1 ELSE 0 END AS LIKE_I
, A.*
FROM T006280_label A
)
SELECT NEW_BAS_DT
, COUNT(*) AS "글 건수"
, SUM(label) AS  "긍정적인 글 건수"
, CASE WHEN COUNT(*) = 0 THEN 0 ELSE NVL(SUM(label) / COUNT(*),0) END AS "긍정 비율 인덱스"
, SUM(LIKE_I) AS  "F_글 건수"
, SUM(LIKE_I*label) AS  "F_긍정적인 글 건수"
, CASE WHEN SUM(LIKE_I) = 0 THEN 0 ELSE NVL(SUM(LIKE_I*label) / SUM(LIKE_I) ,0) END AS "F_긍정 비율 인덱스"
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT



### 201115 part4_일별인덱스생성_감성_갯수 - 060
# 엑셀로 인덱스들 그래프 그려보기

/*
*/
SELECT COUNT(*) FROM T005930_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T006280_index UNION ALL /* 281 */
SELECT COUNT(*) FROM T019170_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T058820_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T122630_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T252670_index ;         /* 283 */

    /* 삼성전자 */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
    SELECT COUNT(*) FROM T005930; /*415828 175MB*/

    /* 녹십자 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
    SELECT COUNT(*) FROM T006280; /*59933 28MB*/

    /* CMG제약 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
    SELECT COUNT(*) FROM T058820; /*81848 30MB*/

    /* KODEX 레버리지 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
    SELECT COUNT(*) FROM T122630; /*171048 59MB*/

    /* KODEX 200선물인버스2X */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
    SELECT COUNT(*) FROM T252670; /*339407 135MB*/

    /* 신풍제약 */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
    SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201115 작업한 부분까지 설명 PPT 만들어서 공유
C:\Users\user\Desktop\# 졸프 PPT\# PPT
미래에셋_프로젝트설계및구현(5조)_중간진행공유_20201111.pptx

### 201010 part4_일별인덱스생성_감성_갯수 - 050 정리해서 6개 종목다 별도 테이블로 만들기  - 캡쳐 해서 PPT로 만들고 공유
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 일별 글 건수 인덱스 만들기 - DB작업   R_CNT
# 일별 감정 지수 인덱스 만들기 - DB작업 R_EMO_I

# 일별 글 건수 인덱스  : CNT
# 일별 감정 지수 인덱스  : EMO_I
# EMO_RT : 감정지수 비율 인덱스

# 일별 글 건수 인덱스   R_CNT  (추천 수 1 이상이고 추천이 더 많은 것 중)
# 일별 감정 지수 인덱스 R_EMO_I (추천 수 1 이상이고 추천이 더 많은 것 중)
# R_EMO_RT : 감정지수 비율 인덱스  (추천 수 1 이상이고 추천이 더 많은 것 중)


### 666 추천수 반영한 데이터셋 사용 - 상관관계 분석


### 201010 part4_일별인덱스생성_감성_갯수 - 040 : 추천 수 1 이상이고 추천이 더 많은 것
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 일별 글 건수 인덱스 만들기 - DB작업   R_CNT
# 일별 감정 지수 인덱스 만들기 - DB작업 R_EMO_I

WITH V1 AS
( SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, CASE WHEN A.`LIKE` >= 1 AND `LIKE` > `DISLIKE` THEN 1 ELSE 0 END AS LIKE_I
, A.*
FROM T058820_label A
)
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
, NVL(SUM(label) / COUNT(*),0) AS EMO_RT
, SUM(LIKE_I) AS  R_CNT
, SUM(LIKE_I*label) AS  R_EMO_I
, NVL(SUM(LIKE_I*label) / SUM(LIKE_I) ,0) AS EMO_RT
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT


SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, CASE WHEN A.`LIKE` >= 1 AND `LIKE` > `DISLIKE` THEN 1 ELSE 0 END AS LIKE_I
, A.*
FROM T058820_label A LIMIT 30;


WITH V1 AS
( SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.*
FROM T058820_label A
)
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT




### 201010 part4_일별인덱스생성_감성_갯수 - 030
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)
WITH V1 AS
( SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.*
FROM T058820_label A
)
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT

### 201010 part4_일별인덱스생성_감성_갯수 - 020
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)

    SELECT
    SUBSTR(DATE,1,10) AS BAS_DT
    , DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR AS BAS_DATE
    , DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
    , A.*
    FROM T058820_label A LIMIT 300;

WITH V1 AS
( SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.*
FROM T058820_label A
)
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT


    6개 종목의 인덱스 대상기간을 맞추어
    대상 기간 : 6개 종목의 2020.1.1 ~ 2020.10.09 까지 인덱스 만듬
    (몇몇 종목은 더 길게 수집되었으나 향후 필요시에 활용)

    /* 녹십자 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
    SELECT COUNT(*) FROM T006280; /*59933 28MB*/

    /* CMG제약 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
    SELECT COUNT(*) FROM T058820; /*81848 30MB*/

    /* KODEX 레버리지 */ /* 전기간 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
    SELECT COUNT(*) FROM T122630; /*171048 59MB*/

    /* KODEX 200선물인버스2X */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
    SELECT COUNT(*) FROM T252670; /*339407 135MB*/

    /* 삼성전자 */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
    SELECT COUNT(*) FROM T005930; /*415828 175MB*/

    /* 신풍제약 */ /* 2020 수집완료 */
    SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
    SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201010 서론
주식에 맞는 성격
넷째 손가락이 길면 - 남성호르몬 트레이딩 잘한다
>>> 다 의미없음 : 인덱스들을 조합한 지표기준으로 투자 >>> 개인의 감정이 들어갈 일이 없다.


### 201010 part3_모델불러와서_Classification수행.ipynb - 060
    # 전체 종목에 대해서 label 만들기

    - DB저장 시간 측정중 T005930_label_B
    - 신풍제약 019170 DB 저장중



### 201010 part3_모델불러와서_Classification수행.ipynb - 060
    # 전체 종목에 대해서 label 만들기
    ///* 녹십자                */ SELECT COUNT(*) FROM T006280; /*  59933  28MB */
    ///* CMG제약               */ SELECT COUNT(*) FROM T058820; /*  81848  30MB */
    ///* KODEX 레버리지        */ SELECT COUNT(*) FROM T122630; /* 171048  59MB */
    /* KODEX 200선물인버스2X */ SELECT COUNT(*) FROM T252670; /* 339407 135MB */
    /* 삼성전자              */ SELECT COUNT(*) FROM T005930; /* 415828 175MB */
    /* 신풍제약              */ SELECT COUNT(*) FROM T019170; /* 469022 186MB */

    >>> 다 되면 아카이빙 compress 처리하기

6개 종목의 인덱스 대상기간을 맞추어
대상 기간 : 6개 종목의 2020.1.1 ~ 2020.10.09 까지 인덱스 만듬
(몇몇 종목은 더 길게 수집되었으나 향후 필요시에 활용)

https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=2138

/* 녹십자 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/

/* CMG제약 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
SELECT COUNT(*) FROM T058820; /*81848 30MB*/

/* KODEX 레버리지 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
SELECT COUNT(*) FROM T122630; /*171048 59MB*/

/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
SELECT COUNT(*) FROM T252670; /*339407 135MB*/

/* 삼성전자 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930; /*415828 175MB*/

/* 신풍제약 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201010 part4_일별인덱스생성_감성_갯수 - 020
# 일자 기준 잡기
# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)
# 전일의 데이터를 기초로 익일의 주가를 예측.
# 상관관계 분석은 당일것 하고 비교도 해보고 전일것 하고도 비교해보기

# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)
SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE,'%Y.%m.%d %H:%i') AS BAS_DATE
, A.*
FROM T058820_label A LIMIT 300;

select  *
from    `ordermaster`
where   `Pick_date` = curdate() and
        `Pick_time` between (now() - interval 2 hour) and now() and
        `Status` = 2

SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR AS BAS_DATE
, A.*
FROM T058820_label A LIMIT 300;

SELECT
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR AS BAS_DATE
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.*
FROM T058820_label A LIMIT 300;


### 201010 part3_모델불러와서_Classification수행.ipynb - 050
# 종목번호 변수처리


### 201010 part3_모델불러와서_Classification수행.ipynb - 040
# DB에 저장
SELECT * FROM T006280_label LIMIT 300;


### 201010 part3_모델불러와서_Classification수행.ipynb - 020
# submit set 만드는 부분 참조


### 201010 part3_모델불러와서_Classification수행.ipynb - 030
# (3) DB : 분석용 테이블 생성하기 : 태그컬럼 하나 추가 (날짜달린 원본데이터는 놔두고 여기다가 부어서 분석하기)
태그컬럼 : 작업전 2 > 작업 후 긍정1, 부정0




### 201010 part3_모델불러와서_Classification수행.ipynb - 010
# (2) 모델 불러오기 ( 불러온 걸로 시연 해보기 )
* 앞부분에 모델 갱신시점 확인도 하기
!ls -l --block-size=K /content/gdrive/'My Drive'/bert_model_save
!ls -l --block-size=M /content/gdrive/'My Drive'/bert_model_save/pytorch_model.bin


### 201010 part3_모델불러와서_Classification수행.ipynb - 001
## part3_모델불러와서_Classification수행

# (1) dataframe 불러오기
//* mariaDB 에 저장된 것 가져와서 감정분석
//: 저장된 데이터 dataframe 으로 불러와서 감정분석해서 건별로 태그 달기
//: dataframe에 컬럼1개 추가
//df['label'] = 0
//: 태그 단 것 DB에 저장하기 (컬럼 1개 추가)

/* 녹십자 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/




###
[매주 화 18:30 - 21:30] 나 수업
[매주 화 19:00~22:00] 지연 줌 수업


### 비고
좋아요는 의미있는 것 거르는 노이즈 제거용으로만.. 감성분석은 BERT 로만 했다고 하는게 더 좋아보임


### 프로토타입
(1) MariaDB : 데이터 수집한 것 하나로 합치기
(2) MariaDB : 라벨 컬럼 하나 추가해서 합친 테이블 만들기
(3) Colab 으로 모델 불러오기
     + DB 불러오기
     + 모델링 해서 라벨 달기
     + 다시 DB에 저장하기
(4) 주가, 거래량 데이터 어디서 구하기 : 거래소? 회사내?


### 도서관 외 미팅 공간
: 오픈된 공간이 더 좋음. 칸막이 있으면 별로.
- 도서관 : 08:00~22:00 (하나스퀘어, 4층 노트북)
# 과도 5층 도서관 앞 휴게룸
# 과도 4층 휴게실
# 과도1층 인피니트라운지
# 지하1층 델타라운지
# 하스 중앙 피아노실
# 과도1층 문쪽 공간(와이파이 잘 안됨)

### 일정
12 [D-4W] 11/17 2,3 이상근, 김현우
13 [D-3W] ★11/24 4,5 강재우, 백승준
14 [D-2W] 12/1 1,6 정순영, 임희석
15 [D-1W] 12/8 2,3 이상근, 김현우
16 [D-W]★ ★ 12/15 학교최종발표 : 1,2,3,4,5,6 정순영, 이상근, 김현우, 강재우, 백승준, 임희석

### 조편성
강재우 - 유근영
백승준 - 박준하
정순영 - 이인희
임희석 - 노시희, 오세현
이상근 - 백전호, 성다야
김현우 - 김유석

### 201104 회식
프로젝트 팀원과 지도교수님의 식사는
기존 이용하시던 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부) 외 더씨, 무르무르드구스토 에서도 가능합니다^^
(금액은 1회 30만원정도로 생각하고 있으나, 특별히 제한은 없는 것으로 하겠습니다)

다만 선결제가 필요한 식당이 있어,
식사하시기 전에 일정과 식사장소를 행정실로 전달 부탁드리겠습니다~

학생분들께 제공해드렸던 식대지원(1인 12,000원까지)은 기존에 안내드린 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부)에서만 이용가능하십니다!


### (D-5W) 졸업 프로젝트
nbooooo@korea.ac.kr (mailto:nbooooo@korea.ac.kr) (mailto:nbooooo@korea.ac.kr (mailto:nbooooo@korea.ac.kr))
http://korea-ac-kr.zoom.us/ (http://korea-ac-kr.zoom.us/) (http://korea-ac-kr.zoom.us/ (http://korea-ac-kr.zoom.us/))

[매주 화 18:30 - 21:30] 나 수업
[매주 화 19:00~22:00] 지연 줌 수업


### 좋아요는 의미있는 것 거르는 노이즈 제거용으로만.. 감성분석은 BERT 로만 했다고 하는게 더 좋아보임

### 프로토타입
(1) MariaDB : 데이터 수집한 것 하나로 합치기
(2) MariaDB : 라벨 컬럼 하나 추가해서 합친 테이블 만들기
(3) Colab 으로 모델 불러오기
     + DB 불러오기
     + 모델링 해서 라벨 달기
     + 다시 DB에 저장하기
(4) 주가, 거래량 데이터 어디서 구하기 : 거래소? 회사내?



### 공부장소
: 오픈된 공간이 더 좋음. 칸막이 있으면 별로.
- 도서관 : 08:00~22:00 (하나스퀘어, 4층 노트북)
- 과도 5층 방
- 과도 1층 오픈 라운지
- 델타스퀘어 : 과도 지하1층
- 과도 4층 정수기 방
- 하스 피아노방
- 과도 1층 엘리베이터옆 후문쪽 자리 : 와이파이 안 됨

### 빅스 미국공포지수

### 201104 회식
프로젝트 팀원과 지도교수님의 식사는
기존 이용하시던 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부) 외 더씨, 무르무르드구스토 에서도 가능합니다^^
(금액은 1회 30만원정도로 생각하고 있으나, 특별히 제한은 없는 것으로 하겠습니다)

다만 선결제가 필요한 식당이 있어,
식사하시기 전에 일정과 식사장소를 행정실로 전달 부탁드리겠습니다~

학생분들께 제공해드렸던 식대지원(1인 12,000원까지)은 기존에 안내드린 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부)에서만 이용가능하십니다!

더씨 : 네이버
출처 : 네이버 플레이스
 - http://naver.me/FSNFaVtG

무르무르드구스토 : 네이버
출처 : 네이버 플레이스
 - http://naver.me/FkbR0em0

다만 선결제가 필요한 식당이 있어,
식사하시기 전에 일정과 식사장소를 행정실로 전달 부탁드리겠습니다~

학생분들께 제공해드렸던 식대지원(1인 12,000원까지)은 기존에 안내드린 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부)에서만 이용가능하십니다!


### 조편성
강재우 - 유근영
백승준 - 박준하

정순영 - 이인희
임희석 - 노시희, 오세현

이상근 - 백전호, 성다야
김현우 - 김유석

### 1차 : 아이디어 발표하고 정하기

### 2차 : 정한 아이디어 구체화 시키고 데이터, 모델등 구체화
  2,3,4,5,최종 별 과업 마일스톤 만들고 역할분담
  일정작성

### 3차 : 프로토타입 개발 시연

# 주차 : 9/1~ 12/31

제2기 디지털융합금융학과
프로젝트설계및구현 1개 과목
강의시간은 18:30 - 21:30

실시간 온라인 강의로 진행

날짜 조 지도교수

9/1,9/22,10/13,11/3,11/24,12/15

1 [D-15W]  ★9/1 4,5 강재우, 백승준
2 [D-14W]  9/8 1,6 정순영, 임희석
3 [D-13W]  9/15 2,3 이상근, 김현우
4 [D-12W]  ★9/22 4,5 강재우, 백승준
5 [D-11W]  9/29 1,6 정순영, 임희석
6 [D-10W]  10/6 2,3 이상근, 김현우
7 [D-9W]  ★10/13 4,5 강재우, 백승준
8 [D-8W]  10/20 1,6 정순영, 임희석
9 [D-7W]  10/27 2,3 이상근, 김현우
-------------------------------------------------------
10 [D-6W]  ★11/3 4,5 강재우, 백승준
11 [D-5W]  11/10 1,6 정순영, 임희석
12 [D-4W]  11/17 2,3 이상근, 김현우
13 [D-3W]  ★11/24 4,5 강재우, 백승준
14 [D-2W]  12/1 1,6 정순영, 임희석
15 [D-1W]  12/8 2,3 이상근, 김현우
16 [D-W]★ ★  12/15 학교최종발표 : 1,2,3,4,5,6 정순영, 이상근, 김현우, 강재우, 백승준, 임희석


2. 진행방식
  1) 수업일 전날까지 지도교수에게 발표자료 전달
  2) 수업일에는 모든 원생들이 실시간으로 수업에 참여
  3) 발표 조는 수업시간에 발표를 진행한 후 지도교수의 피드백을 받음
  4) 마지막(12/15) 발표는 각 조당 20분씩 발표하며 지도교수의 피드백 없이 진행

위 사항을 참고하시어 다음학기 강의 및 졸업 프로젝트를 진행해주시기 바랍니다.
감사합니다.


### 도서관 외 미팅 공간
# 과도 5층 도서관 앞 휴게룸
# 과도 4층 휴게실
# 과도1층 인피니트라운지
# 지하1층 델타라운지
# 하스 중앙 피아노실
# 과도1층 문쪽 공간(와이파이 잘 안됨)


###
(애들이랑 눈오는날 놀러왔을 때는 내려주고 집에주차 > 따릉이로 이동 > 다시 픽업 식으로 해도 될 듯?)
고려대 주차 : 2시간 까지 5500원 (30분 1천원)
2시간 지나면 10분당 천원해서 시간당 6천원임
# 과도1층 문쪽 공간(와이파이 잘 안됨)
중앙광장 지하 군데군데 소파&책상 있고 (KU프라이드 라운지라는 작은 라운지 있는데 여는지 모르겠네요) 하나스퀘어에는 피아노방이라는 라운지 있습니다.


### 201110 압축
    (1) ROW_FORMAT=COMPRESSED
    (2) 엔진 = ARCHIVE

    ALTER TABLE 'T005930_B' ROW_FORMAT=COMPRESSED;
    ??? ALTER TABLE T006280_B PAGE_COMPRESSED=1;
    HeidiSQL 에서 기본으로 지원함


### 201110 MariaDB 압축 옵션 (PASS : 필요없음)
SHOW VARIABLES WHERE Variable_name LIKE "have_%" OR Variable_name LIKE "%_compression_%"

innodb_compression_algorithm zlib


### 20201110 테이블 백업 해놓기
CREATE TABLE T006280_B AS SELECT * FROM T006280;
CREATE TABLE T058820_B AS SELECT * FROM T058820;
CREATE TABLE T122630_B AS SELECT * FROM T122630;
CREATE TABLE T252670_B AS SELECT * FROM T252670;
CREATE TABLE T005930_B AS SELECT * FROM T005930;
CREATE TABLE T019170_B AS SELECT * FROM T019170;

ALTER TABLE T006280_B PAGE_COMPRESSED=1;
OPTIMIZE TABLE T006280_B;

/* 녹십자 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=2138
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/

/* CMG제약 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
SELECT COUNT(*) FROM T058820; /*81848 30MB*/

/* KODEX 레버리지 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
SELECT COUNT(*) FROM T122630; /*171048 59MB*/

/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
SELECT COUNT(*) FROM T252670; /*339407 135MB*/

/* 삼성전자 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930; /*415828 175MB*/

/* 신풍제약 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201110 데이터 수집 현황

/* 녹십자 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=2138
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/
SELECT * FROM T006280 LIMIT 10;

/* CMG제약 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=058820&page=1
https://finance.naver.com/item/board.nhn?code=058820&page=4086
SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
SELECT COUNT(*) FROM T058820; /*81848 30MB*/
SELECT * FROM T058820 LIMIT 10;

/* KODEX 레버리지 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=122630&page=1
https://finance.naver.com/item/board.nhn?code=122630&page=4506
SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
SELECT COUNT(*) FROM T122630; /*171048 59MB*/
SELECT * FROM T122630 LIMIT 10;

/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
https://finance.naver.com/item/board.nhn?code=252670&page=1
https://finance.naver.com/item/board.nhn?code=252670&page=19871
SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
SELECT COUNT(*) FROM T252670; /*339407 135MB*/
SELECT * FROM T252670 LIMIT 10;

/* 삼성전자 */ /* 2020 수집완료 */
https://finance.naver.com/item/board.nhn?code=005930&page=1
https://finance.naver.com/item/board.nhn?code=005930&page=21800
SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930; /*415828 175MB*/
SELECT * FROM T005930 LIMIT 10;

/* 신풍제약 */ /* 2020 수집완료 */
https://finance.naver.com/item/board.nhn?code=019170&page=1
https://finance.naver.com/item/board.nhn?code=019170&page=25000
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/
SELECT * FROM T019170 LIMIT 10;


### 201110  KODEX 200선물인버스2X

/*SELECT MIN(DATE), MAX(DATE) FROM T252670;*/ /*2020.04.29 00:00 ~ 2020.10.10 12:23*/
/* 2020.04.28 10:10 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201009;*/  /*2020.09.18 09:14 2020.10.10 12:23*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201010;*/  /*2020.07.21 08:08 2020.09.23 17:36*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201011;*/  /*2020.04.28 10:10 2020.08.11 12:33*/
SELECT MIN(DATE), MAX(DATE) FROM T252670_20201107_B;   /*2019.12.17 10:09 2020.04.29 11:07*/

# 로그2

DELETE FROM T252670 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.09.19 00:00 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201010 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.21 08:08 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.22 00:02 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201011 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.28 10:10 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.04.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.29 00:00 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201107_B WHERE DATE < '2020.04.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.28 10:10 ~ 2020.10.10 12:23*/


### 201110

/* 신풍제약 */
https://finance.naver.com/item/board.nhn?code=019170&page=1
https://finance.naver.com/item/board.nhn?code=019170&page=25000
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/
SELECT * FROM T019170 LIMIT 10;

/*SELECT MIN(DATE), MAX(DATE) FROM T019170*/ /*2020.07.28 00:00 ~ 2020.10.10 16:18*/
/* 2020.07.27 19:54 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201009;*/  /*2020.09.28 13:37 2020.10.10 16:18*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201010;*/  /*2020.09.09 09:27 2020.10.02 13:29*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201011;*/  /*2020.07.27 19:54 2020.09.09 10:37*/
SELECT MIN(DATE), MAX(DATE) FROM T019170_20201107_B;   /*2018.10.01 21:23 2020.10.20 22:32*/

# 로그
DELETE FROM T019170 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.29 00:01 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201010 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:27 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:30 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201011 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.07.27 19:54 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.07.28 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170 /*2020.07.28 00:00 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201107_B WHERE DATE < '2020.07.28 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.07.27 19:54 ~ 2020.10.10 16:18*/


### 201110

/* 삼성전자 */
https://finance.naver.com/item/board.nhn?code=005930&page=10800
https://finance.naver.com/item/board.nhn?code=005930&page=15800 $$$ (1)
https://finance.naver.com/item/board.nhn?code=005930&page=21800 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T005930;*/ /*2020.04.05 00:00 ~ 2020.10.10 16:07*/
/* 2020.04.04 09:29 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201009;*/  /*2020.09.14 14:16 2020.10.10 16:07*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201010;*/  /*2020.07.06 16:57 2020.09.18 03:08*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201011;*/  /*2020.04.04 09:29 2020.09.18 03:11*/
SELECT MIN(DATE), MAX(DATE) FROM T005930_20201107_B;   /*2019.12.09 15:08 2020.07.28 03:34*/


# 로그 1

SELECT STR_TO_DATE('2020090100','%Y%m%d%H');
SELECT COUNT(*) FROM T005930_20201009 WHERE DATE < '2020.09.15 00:00';

DELETE FROM T005930 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.09.15 00:06 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201010 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.06 16:57 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.07 00:03 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201011 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.04 09:29 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.04.05 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.05 00:00 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201107_B WHERE DATE < '2020.04.05 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2019.12.09 15:08 2020.10.10 16:07*/

### 201010 데이터 2차 수집 - 1000~4000 page
토요일 밤에 집에서 돌려놓기
일요일 저녁에 집에서 확인하기


### 201010 모델 제일 결과 좋았던 걸로 학습시켜서 저장하기
# part2_bert감정분석_모델학습및저장
일요일에 회사에서 돌려놓기
일요일에 집에서 저녁에 확인하기


### 201110 깃허브
## 깃허브
https://github.com/head4ths/CEI

# 깃허브 폴더
/data                    데이터 파일 넣는 곳
/model                   모델 넣는 곳
/etc_reference_html      참고용 html 넣는 곳
/etc_reference_source    참고용 소스 넣는 곳


### 201109 데이터 수집 현황


/* 녹십자 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=3000
/*SELECT MIN(DATE), MAX(DATE) FROM T006280_20201107;*/  /*2017.06.08 14:16 2020.11.07 19:51*/

/* KODEX 레버리지 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=122630&page=1
/*SELECT MIN(DATE), MAX(DATE) FROM T122630;*/ /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201009;*/  /*2020.05.24 04:13 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201010;*/  /*2018.12.02 19:11 2020.06.24 17:13*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201011;*/  /*2017.06.07 07:55 2018.12.03 20:39*/

/* CMG제약 */
https://finance.naver.com/item/board.nhn?code=058820&page=1
https://finance.naver.com/item/board.nhn?code=058820&page=4082  다다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T058820;*/ /* */



/* KODEX 200선물인버스2X */
https://finance.naver.com/item/board.nhn?code=252670&page=10900
https://finance.naver.com/item/board.nhn?code=252670&page=15900 $$$ (1)
https://finance.naver.com/item/board.nhn?code=252670&page=20900 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T252670;*/ /*2020.04.29 00:00 ~ 2020.10.10 12:23*/
/* 2020.04.28 10:10 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201009;*/  /*2020.09.18 09:14 2020.10.10 12:23*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201010;*/  /*2020.07.21 08:08 2020.09.23 17:36*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201011;*/  /*2020.04.28 10:10 2020.08.11 12:33*/

/* 삼성전자 */
https://finance.naver.com/item/board.nhn?code=005930&page=10800
https://finance.naver.com/item/board.nhn?code=005930&page=15800 $$$ (1)
https://finance.naver.com/item/board.nhn?code=005930&page=21800 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T005930;*/ /*2020.04.05 00:00 ~ 2020.10.10 16:07*/
/* 2020.04.04 09:29 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201009;*/  /*2020.09.14 14:16 2020.10.10 16:07*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201010;*/  /*2020.07.06 16:57 2020.09.18 03:08*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201011;*/  /*2020.04.04 09:29 2020.09.18 03:11*/

/* 신풍제약 */
https://finance.naver.com/item/board.nhn?code=019170&page=11300  다음번
https://finance.naver.com/item/board.nhn?code=019170&page=25000  다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T019170*/ /*2020.07.28 00:00 ~ 2020.10.10 16:18*/
/* 2020.07.27 19:54 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201009;*/  /*2020.09.28 13:37 2020.10.10 16:18*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201010;*/  /*2020.09.09 09:27 2020.10.02 13:29*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201011;*/  /*2020.07.27 19:54 2020.09.09 10:37*/




### 201107  4차 데이터 수집
/*
(1) 적재기간 확인하고 추가로 적재

(2) 테이블간 겹치는 부분 확인해서 중복없이 한 테이블로 합치기
CREATE TABLE T005930 AS SELECT * FROM T005930_20201009;
CREATE TABLE T252670 AS SELECT * FROM T252670_20201009;
CREATE TABLE T019170 AS SELECT * FROM T019170_20201009;
CREATE TABLE T122630 AS SELECT * FROM T122630_20201009;


*/



/* KODEX 200선물인버스2X */
https://finance.naver.com/item/board.nhn?code=252670&page=10900
https://finance.naver.com/item/board.nhn?code=252670&page=15900 $$$ (1)
https://finance.naver.com/item/board.nhn?code=252670&page=20900 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T252670;*/ /*2020.04.29 00:00 ~ 2020.10.10 12:23*/
/* 2020.04.28 10:10 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201009;*/  /*2020.09.18 09:14 2020.10.10 12:23*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201010;*/  /*2020.07.21 08:08 2020.09.23 17:36*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201011;*/  /*2020.04.28 10:10 2020.08.11 12:33*/

/* 삼성전자 */
https://finance.naver.com/item/board.nhn?code=005930&page=10800
https://finance.naver.com/item/board.nhn?code=005930&page=15800 $$$ (1)
https://finance.naver.com/item/board.nhn?code=005930&page=21800 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T005930;*/ /*2020.04.05 00:00 ~ 2020.10.10 16:07*/
/* 2020.04.04 09:29 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201009;*/  /*2020.09.14 14:16 2020.10.10 16:07*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201010;*/  /*2020.07.06 16:57 2020.09.18 03:08*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201011;*/  /*2020.04.04 09:29 2020.09.18 03:11*/

/* 신풍제약 */
https://finance.naver.com/item/board.nhn?code=019170&page=11300  다음번
https://finance.naver.com/item/board.nhn?code=019170&page=25000  다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T019170*/ /*2020.07.28 00:00 ~ 2020.10.10 16:18*/
/* 2020.07.27 19:54 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201009;*/  /*2020.09.28 13:37 2020.10.10 16:18*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201010;*/  /*2020.09.09 09:27 2020.10.02 13:29*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201011;*/  /*2020.07.27 19:54 2020.09.09 10:37*/



/* CMG제약 */
https://finance.naver.com/item/board.nhn?code=058820&page=1
https://finance.naver.com/item/board.nhn?code=058820&page=4082  다다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T058820;*/ /* */



/* 녹십자 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=3000
/*SELECT MIN(DATE), MAX(DATE) FROM T006280_20201107;*/  /*2017.06.08 14:16 2020.11.07 19:51*/

/* KODEX 레버리지 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=122630&page=1
/*SELECT MIN(DATE), MAX(DATE) FROM T122630;*/ /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201009;*/  /*2020.05.24 04:13 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201010;*/  /*2018.12.02 19:11 2020.06.24 17:13*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201011;*/  /*2017.06.07 07:55 2018.12.03 20:39*/


# 로그 1

SELECT STR_TO_DATE('2020090100','%Y%m%d%H');
SELECT COUNT(*) FROM T005930_20201009 WHERE DATE < '2020.09.15 00:00';

DELETE FROM T005930 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.09.15 00:06 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201010 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.06 16:57 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.07 00:03 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201011 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.04 09:29 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.04.05 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.05 00:00 ~ 2020.10.10 16:07*/


# 로그2

DELETE FROM T252670 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.09.19 00:00 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201010 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.21 08:08 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.22 00:02 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201011 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.28 10:10 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.04.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.29 00:00 ~ 2020.10.10 12:23*/


# 로그3

DELETE FROM T019170 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.29 00:01 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201010 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:27 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:30 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201011 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.07.27 19:54 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.07.28 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170 /*2020.07.28 00:00 ~ 2020.10.10 16:18*/


# 로그4


DELETE FROM T122630 WHERE DATE < '2020.05.25 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2020.05.25 00:12 ~ 2020.10.10 07:40*/

INSERT INTO T122630 SELECT * FROM T122630_20201010 WHERE DATE < '2020.05.25 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2018.12.02 19:11 ~ 2020.10.10 07:40*/

DELETE FROM T122630 WHERE DATE < '2018.12.02 20:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2018.12.02 21:20 ~ 2020.10.10 07:40*/

INSERT INTO T122630 SELECT * FROM T122630_20201011 WHERE DATE < '2018.12.02 20:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2017.06.07 07:55 ~ 2020.10.10 07:40*/


### 201010 데이터 수집 현황
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

##
계속 뒤로 밀리니까
(1차) 1~1000 page 스크랩했으면 (5H)
(2차) 1000~4000 page 스크랩 하면 됨. (15H 예상) (테이블 이름 20201010으로 바꾸기)
    >>> 계속 밀리니까 겹치는 부분이 있음 (1000~1100 페이지 정도까지의 앞부분)
    >>> 합칠때는 시간순으로 보고 중복된 것 지우고 합치기

##
: 1000번 돌림 > 20000개 > 15000초 (4H)  18MB
: 총 40MB
SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

SELECT COUNT(*) FROM T122630_20201009; /* 20000건 완료 */ /* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT COUNT(*) FROM T252670_20201009; /* 20000건 완료 */ /* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT COUNT(*) FROM T019170_20201009; /* 20000건 완료 */ /* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930_20201009; /* 20000건 완료 */ /* 2020.09.28 13:37 ~ 2020.10.10 16:18 */

/* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT * FROM T122630_20201009 ORDER BY DATE asc;
SELECT * FROM T122630_20201009 ORDER BY DATE desc;

/* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT * FROM T252670_20201009 ORDER BY DATE asc;
SELECT * FROM T252670_20201009 ORDER BY DATE desc;

/* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT * FROM T005930_20201009 ORDER BY DATE asc;
SELECT * FROM T005930_20201009 ORDER BY DATE desc;

/* 2020.09.28 13:37 ~ 2020.10.10 16:18 */
SELECT * FROM T019170_20201009 ORDER BY DATE asc;
SELECT * FROM T019170_20201009 ORDER BY DATE desc;


### 201010
    ## 발표 컨셉
    소문에 사서 뉴스에 판다.
    뉴스를 분석하면 이미 늦는다
    소문을 분석해라!!!

    소문에 해당하는
    소문지수

    이오 플로우

    >>>>> 여러 종목 돌리다 보면... 분명히 소문지수가 주가보다 먼저 튀는 종목이 있을 것임
    >>>>> 발표 용으로는 제격

    # 발표 컨셉
    알라딘 - 진흙속의 진주
    뻘(진흙)글 속에서 통계적으로 숨겨진 진짜배기 소문을 찾다

    # 발표 컨셉
    BERT 정확도 학습시 test set에 대하여 90몇 프로 나온걸로 정확도 높아 정확한 척
    현혹 가능.


### 200910

    결정한 부분
    - 개발 환경 : 구글 Colab 및 Github (https://github.com/head4ths/CEI/)) 사용
    - 데이터 저장 : 개인 NAS 서버에 Maria DB (스크래핑 데이터, 개인매수동향, 주가데이터 보관) (http://121.128.223.185:3307)  cei / Mrs너구리1!  : 개인 PC 라 IP 바뀔수도 ??? 프로젝트 기간이 짧아서 별 상관없을 듯 )
    - 모델 중간 저장 : ??? 구글 드라이브 ???

    Todo 크게 네 부분
    ( 1,2,3,4 가 순차적이라 프로토타입 만들어보는 단계에서는 전원 참여 식으로 운영 )
    ( 프로토타입으로 검증되면 개선은 각자 담당 영역 맡아서 진행 )
    - (1) 스크래핑 및 전처리 후 DB화 저장
    - (2) BERT 감정분석 ( 네이버 무비 영화 감성 분석 - 전이학습 모델 사용 ) 후 태그 달기
    - (3) 개인매수동향 ( 코스콤 등에서 다운로드 ) 이나 주가데이터와 상관관계 분석 - 사회공학적인 인사이트 필요
    - (4) 발표


### 201003 040 일자별 인덱스 만들기


### 201009 050 일자별 주가, 일자별 거래량 가져오기


### 201003 060 상관관계 분석
    : 글의 갯수, 감정분석 비율
    : 주가, 거래량, 투자자별 매매동향 ( 거래소? )

### 201003 020 Bert 소스 정리
## part3_모델불러와서_Classification수행
# 불러오기 ( 불러온 걸로 시연 해보기 )

# 앞부분에 모델 갱신시점 확인도 하기
!ls -l --block-size=K /content/gdrive/'My Drive'/bert_model_save
!ls -l --block-size=M /content/gdrive/'My Drive'/bert_model_save/pytorch_model.bin



### 201010  참고
Github의 경우 개당 파일은 100MB 제한이고 있지만 전체 용량 제한은 없다. Bitbucket은 개당 파일의 제한은 없지만 전체용량이 2GB이상이 안되는 제한사항이 존재 한다. Github의 경우 50Mb 는 Warning을 표시하며, 100Mb 부터는 Error를 나타낸다.


### 201010 데이터 수집 현황
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

: 1000번 돌림 > 20000개 > 15000초 (4H)  18MB
: 총 40MB
SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

SELECT COUNT(*) FROM T122630_20201009; /* 20000건 완료 */ /* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT COUNT(*) FROM T252670_20201009; /* 20000건 완료 */ /* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT COUNT(*) FROM T019170_20201009; /* 20000건 완료 */ /* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930_20201009; /* 20000건 완료 */ /* 2020.09.28 13:37 ~ 2020.10.10 16:18 */

/* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT * FROM T122630_20201009 ORDER BY DATE asc;
SELECT * FROM T122630_20201009 ORDER BY DATE desc;

/* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT * FROM T252670_20201009 ORDER BY DATE asc;
SELECT * FROM T252670_20201009 ORDER BY DATE desc;

/* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT * FROM T005930_20201009 ORDER BY DATE asc;
SELECT * FROM T005930_20201009 ORDER BY DATE desc;

/* 2020.09.28 13:37 ~ 2020.10.10 16:18 */
SELECT * FROM T019170_20201009 ORDER BY DATE asc;
SELECT * FROM T019170_20201009 ORDER BY DATE desc;


### 201003 020 Bert 소스 정리
## part2_bert감정분석_모델학습및저장
    : 검증 완료


### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 >>> 다시 불러오기 해보기
: 700MB 정도 되는 듯 ???
: 구글 드라이브 쓰는게 구글 코랩에서 저장하고 불러오는데 매우 빠름 !!! 좋은 선택 !!!
: 뭐가 저장되는지 다운 받아지면 한번 열어보기


## 참조2 (Good!)
https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP

## 핵심 키워드
# from_pretrained
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# save_pretrained
bcm.save_pretrained('the output directory path')

## 원래 소스
model = BertForSequenceClassification.from_pretrained(gv_model_nm, num_labels=2)
tokenizer = BertTokenizer.from_pretrained(gv_model_nm, do_lower_case=gv_do_lower_case)

## 불러오기 ( 불러온 걸로 시연 해보기 )



### 201010 bert 소스 수정
## 소스
(Colab) part2_bert감정분석

## 학습시 과정에서 시간 출력하게 수정하기
#615K_20200615_v19


### 201010
OSError: [Errno 95] Operation not supported: '/content/drive/Mask_RCNN' on Google Colab-자주끊기는-런타임-방지하기
https://stackoverflow.com/questions/60132033/oserror-errno-95-operation-not-supported-content-drive-mask-rcnn-on-googl




### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조4
# BertForSequenceClassification
# How to save a model as a BertModel #2094
https://github.com/huggingface/transformers/issues/2094

save_pretrained

from transformers import BertForSequenceClassification, BertConfig

config = BertConfig.from_pretrained("bert-base-cased", num_labels=3)
model = BertForSequenceClassification.from_pretrained("bert-base-cased", config=config)
model.load_state_dict(torch.load("SAVED_SST_MODEL_DIR/pytorch_model.bin"))


# How to load BertforSequenceClassification models weights into BertforTokenClassification model? https://stackoverflow.com/questions/60897514/how-to-load-bertforsequenceclassification-models-weights-into-bertfortokenclassi


new_model = BertForTokenClassification(config=config)
new_model.bert.load_state_dict(model.bert.state_dict())

!mkdir -p saved_model
model.save('saved_model/my_model')

# BertForTokenClassification model.save

model = BertForSequenceClassification.from_pretrained(gv_model_nm, num_labels=2)

## 참조
https://github.com/huggingface/transformers/issues/2517
# load config
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# load a sequence model
bsm = BertForTokenClassification.from_pretrained('bert-base-uncased', config=conf)
# get bert core model
bcm = bsm.bert
# save the core model
bcm.save_pretrained('the output directory path')
# you also need to save your tokenizer in the same directory

## 참조2 - 검색
https://www.google.com/search?sxsrf=ALeKk013i9Ir4dm_XebJUjc_kCRRRuGjew%3A1602323594165&ei=ioSBX8m8CaTKmAWB46SQBw&q=BertForTokenClassification+model+save_pretrained+colab&oq=BertForTokenClassification+model+save_pretrained+colab&gs_lcp=CgZwc3ktYWIQAzoECCMQJzoHCCMQrgIQJzoHCCEQChCgAVCXLFi4PmDhQmgBcAB4AIAB1QGIAZELkgEFMC43LjGYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwjJqZTF4KnsAhUkJaYKHYExCXIQ4dUDCA0&uact=5

## 참조2 (Good!)
https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP

## 참조3
https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb#scrollTo

## 핵심 키워드
# from_pretrained
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# save_pretrained
bcm.save_pretrained('the output directory path')




### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조2
# 참조2
http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221564411127&categoryNo=49&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

from os import path
from google.colab import drive

model_dir_name = 'bert_model_save'
drive.mount('/content/gdrive')
model_base_dir = path.join('./gdrive/My Drive/', model_dir_name)
if not path.exists(model_base_dir):
  print('Check your google drive directory. See you file explorer')


with open(path.join(model_base_dir, "myfile.txt"), "w") as f:
    f.write("Google Colab is good!!!")



### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조1
## 참조 1,2,3은 개념만 대강 보고 참조4에서 bert 모델용으로 나온 것 쓰기

## 모델 중간 저장 - 텐서 플로우
# 참조1
https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko#scrollTo=mQF_dlgIVOvq

tf.keras.callbacks.ModelCheckpoint
checkpoint_path = "training_1/cp.ckpt"

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest_checkpoint

# 이전에 저장한 가중치를 로드합니다
model.load_weights(latest)


수동으로 가중치 저장하기
앞에서 가중치를 모델에 로드하는 방법을 보았습니다. 수동으로 가중치를 저장하는 것도 쉽습니다. Model.save_weights 메서드를 사용합니다. tf.keras는, 특히 save_weights는 .ckpt 확장자를 가진 텐서플로 체크포인트 포맷을 사용합니다(.h5 확장자의 HDF5으로 저장하는 것은 Save and serialize models 가이드에서 다룹니다):


전체 모델 저장하기
model.save 메서드를 호출하여 모델의 구조, 가중치, 훈련 설정을 하나의 파일/폴더에 저장합니다. 모델을 저장하기 때문에 원본 파이썬 코드*가 없어도 사용할 수 있습니다. 옵티마이저 상태가 복원되므로 정확히 중지한 시점에서 다시 훈련을 시작할 수 있습니다.

두 개의 포맷(SavedModel과 HDF5)으로 모델을 저장할 수 있습니다. 텐서플로의 SavedModel 포맷은 TF2.x에서기본 파일 포맷입니다. 하지만 HDF5 포맷으로 저장할 수도 있습니다. 두 파일 포맷으로 전체 모델을 저장하는 방법은 아래에서 자세히 설명합니다.

전체 모델을 저장하는 기능은 매우 유용합니다. TensorFlow.js로 모델을 로드한 다음 웹 브라우저에서 모델을 훈련하고 실행할 수 있습니다(Saved Model, HDF5). 또는 모바일 장치에 맞도록 변환한 다음 TensorFlow Lite를 사용하여 실행할 수 있습니다(Saved Model, HDF5).

* 사용자 정의 객체(예를 들면 상속으로 만든 클래스나 층)는 저장하고 로드하는데 특별한 주의가 필요합니다. 아래 사용자 정의 객체 저장하기 섹션을 참고하세요.


# SavedModel로 전체 모델을 저장합니다
!mkdir -p saved_model
model.save('saved_model/my_model')

new_model = tf.keras.models.load_model('saved_model/my_model')

# 모델 구조를 확인합니다
new_model.summary()

model.load_weights(latest)
model.save
tf.keras.models.load_model


### 201010 모델 저장용 local 설치
pip install --upgrade pip
pip install -q pyyaml h5py
pip install pyyaml h5py
ERROR: Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'c:\\programdata\\anaconda3\\lib\\site-packages\\pip\\_internal\\build_env.py'
Consider using the `--user` option or check the permissions.
해당 폴더에 적절한 권한이 없어서 발생하는 문제로, 관리자 권한으로 CMD를 실행하면 해결됩니다.
C:\\Users\\user\\AppData\\Local\\Temp\\pip-uninstall-8wl6ryze\\pip.exe
C:\Users\user\AppData\Local\Temp\pip-uninstall-8wl6ryze\



### 201010 GPU
안녕하세요. 서버에 대해 1도 모르는 대학원생이 작은 서버구축 관련하여 질문 드립니다. 최근 같이 일하게 된 Psychology/Biomedical Engineering 교수님이 (작은) 연구실용 서버 구축하는 것을 도와드리게 되었습니다.
첫 번째 질문입니다. 딥러닝 연구 목적으로 생각했을 때 Tesla P100 한 대 vs. TITAN RTX 두 대 vs. RTX 2080 Ti 네 대 비교한다면 어떤 게 좋을까요? 24/7 계속해서 GPU 돌릴게 아니고 간단한 벤치마크로 봐도 RTX 2080 Ti가 나아보이는데요... 경험 있으신 분들 의견을 구합니다. 저는 Titan Xp와 1080ti 밖에 사용을 안 해봤습니다 ㅜㅜ
두 번째 질문입니다. 현재 이 연구실은 PowerEdge T440 Tower Server를 가지고 있습니다 ㅜㅜ 지금까지 GPU로 연산을 돌리지 않았아서 T440을 가지고 있는 것 같아요. Intel Xeon Gold 5118 (2.3G, 12C/24T, 10.4GT/s, 16M
Cache) x 2개, 32GB RDIMM 2666MT/s Dual Rank x 6개, 4TB 7.2K RPM SATA 6Gbps 512n 3.5in Hot-plug Hard Drive x 8개가 장착되어 있습니다. 중요한건 꽂을 수 있는 GPU가 Quadro NVS 310나 P4000 밖에 없습니다. OMG... 이런 경우에 어떤 해결책이 있을까요? Tower를 T640 (Nvidia P100 장착가능)이나 R740 (Nvidia P100 or V100 장착가능)로 바꾸는 것은 현명한 방법일까요? 혹은 RTX 2080 Ti 설치가능한 Dell Server용 Tower가 있나요?
세 번째 질문입니다. 데이터는 주로 2D 이미지 일 것 같은데요, HDD로 충분할까요 아니면 적어도 SATA SSD를 구입해야 할까요? (이 질문에 대한 답변은 충분히 되었습니다.)
어떤 코멘트라도 좋으니 생각 공유해주시면 감사하겠습니다.

https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/




### 201010 데이터 수집은
어차피 GPU 안 쓰니까
집 데스크탑에서 돌리는게 나음

주피터노트북으로도 여러건 동시에 돌릴 수 있고 selenium 도 좀더 빠른것 같고
특히 DB에 건건이 CONNECT 하고 INSERT 하는게 같은 망이라서 그런지 비교도 안되게 빠름

구글 코랩은 계속 세션 끊김.

BERT는 GPU 써야 되고 자원써서 발열 심하니까 코랩 어쩔 수 없이 쓰지만
데이터 수집은 로컬이 우월.



### 555 모델 중간 저장 - 텐서 플로우
- 참조1
https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko#scrollTo=mQF_dlgIVOvq

- 참조2
http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221564411127&categoryNo=49&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

- 참조3
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko





### 201010 데스크탑 로컬
    # KODEX레버리지
    SELECT COUNT(*) FROM T122630_20201009
    2353 / 20000


### 201010 데스크탑 로컬
C:\Users\head4\myjupyter\cei



### 201009 소스 정리하고 종목별로 나누기



### 201009 오늘 밤에 자기 전에 데스크탑으로 돌려 놓고 데이터 대량으로 모으기
: 꼭 한번에 다 받을 필요없음. 끊어서 받아와서 합쳐도 됨
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB




### 201010 데스크탑 로컬
C:\Users\head4\myjupyter\cei

### 201010 데스크탑

https://stackoverflow.com/questions/30337394/pandas-to-sql-fails-on-duplicate-primary-key

 SELECT COUNT(*) FROM T122630_20201009

df1
SELECT * FROM T122630_20201009
ORDER BY DATE ASC;



### 201010 Colab-자주끊기는-런타임-방지하기 데스크탑
Colab - 세션유지

방법은

구글 코랩에서 F12로 개발자 도구창을 열고

Console 선택 후

아래의 코드를 입력한 뒤 엔터를 누르면됩니다.

function ClickConnect() {var buttons = document.querySelectorAll("colab-dialog.yes-no-dialog paper-button#cancel"); buttons.forEach(function(btn) { btn.click(); }); console.log("1분마다 자동 재연결"); document.querySelector("colab-toolbar-button#connect").click(); } setInterval(ClickConnect,1000*60);

출처: https://somjang.tistory.com/entry/Google-Colab-자주끊기는-런타임-방지하기 [솜씨좋은장씨]


1. 구글 colab에서 90 time out 세션 유지 javascript 코드

function ClickConnect() {
    var buttons = document.querySelectorAll("colab-dialog.yes-no-dialog paper-button#cancel");
    buttons.forEach(function(btn) {
        btn.click();
    });
    console.log("1분마다 자동 재연결");
    document.querySelector("colab-toolbar-button#connect").click();
}
setInterval(ClickConnect,1000*60);

2. 구글 colab에서 buffered data was truncated after reaching the output size limit 에러 방지를 위한 현재 출력창 자동 지우기

function CleanCurrentOutput(){
    var btn = document.querySelector(".output-icon.clear_outputs_enabled.output-icon-selected[title$='현재 실행 중...'] iron-icon[command=clear-focused-or-selected-outputs]");
    if(btn) { console.log("30분마다 출력 지우기");
     btn.click();
    }
}
setInterval(CleanCurrentOutput,1000*60*30);



### 201009 로컬로 데스크탑에서 1번 소스 파이썬으로 돌려보고 시간재기 (로컬 수행)
: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB



### 201009 로컬로 노트북에서 1번 소스 파이썬으로 돌려보고 시간재기 (로컬 수행)
: 주피터 노트북

: 속도는 거의 똑같음.

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB
https://stackoverflow.com/questions/29858752/error-message-chromedriver-executable-needs-to-be-available-in-the-path
driver = webdriver.Chrome('/path/to/chromedriver')
driver = webdriver.Chrome("C:/Users/head4/myjupyter/cei/chromedriver.exe")

# How To Make Selenium WebDriver Scripts Faster
https://seleniumjava.com/2015/12/12/how-to-make-selenium-webdriver-scripts-faster/



### 201009 010 mariaDB 에 저장하기 # 여러건 수집해보기 (시간, 용량 측정)

# 사이즈 : MySQL 테이블 및 데이타베이스 크기 알아내기
https://www.lesstif.com/dbms/mysql-17105786.html

SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

0.38 MB : 420개
SELECT COUNT(*) FROM 신풍제약_20201009;

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB


### 20201009
SELECT * FROM 신풍제약_20201009
WHERE DATE LIKE '2020.10.09 19%'

SELECT * FROM TABLES;
SELECT * FROM TABLES WHERE TABLE_SCHEMA = 'cei';

### 201009 010 mariaDB 에 저장하기 # 저장된 것 다시 불러와서 dataframe 만들기
https://greendreamtrre.tistory.com/196

indata = pd.read_sql_query("select * from kopo_product_volume", engine)
indata.head()



### 201009 010 mariaDB 에 저장하기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율




### 201009 010 mariaDB 에 저장하기 # v7 mariaDB 테이블 만들기
### 201009 010 mariaDB 에 저장하기 # v8 mariaDB 랑 연동해서 담아보기
### 201009 010 mariaDB 에 저장하기 # v9 중복된것 있으면 PK기준 덮어쓰기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율


### 201009 010 mariaDB 에 저장하기 # 소스 찾아보기
    : Google colab mariadb conn

    # 파이썬에서 마리아 DB 연결하기
    https://m.blog.naver.com/smonone/221491863406

    # sqlalchemy와 pandas로 MariaDB에 데이터 insert하기
    https://yeomkyeorae.github.io/pandas/sqlalchemy/

SQLALCHEMY_DATABASE_URI = 'mysqI+mysqIconnector://yeom：yeom@IocaIhost:3306/price'
engine = sqlalchemy.create_engine(SQLALCHEMY_DATABASE_URI, echo=FaIse)

from sqlalchemy import create_engine engine = create_engine('mssql+pymssql://username:passwd@host/database', echo=True)

출처: https://excelsior-cjh.tistory.com/77 [EXCELSIOR]
GRANT ALL PRIVILEGES ON alchemy.* to 'cei'@'%';

SQLALCHEMY_DATABASE_URI =


engine = sqlalchemy.create_engine('mysqI+mysqIconnector://cei:Mrssjrnfl1!@121.128.223.185:3307/cei', echo=True)

https://stackoverflow.com/questions/51783313/how-do-i-get-sqlalchemy-create-engine-with-mysqlconnector-to-connect-using-mysql

SELECT * FROM df1_20201009;
selloutData.to_sql(name=resultname, con=engine, index = False, if_exists='replace')
use database_name;

use cei;

MariaDB [(none)]> use cei;
Database changed
MariaDB [cei]> desc df1_20201009;
+---------+------+------+-----+---------+-------+
| Field   | Type | Null | Key | Default | Extra |
+---------+------+------+-----+---------+-------+
| DATE    | text | YES  |     | NULL    |       |
| ITEM    | text | YES  |     | NULL    |       |
| TITLE   | text | YES  |     | NULL    |       |
| CONTENT | text | YES  |     | NULL    |       |
| READ    | text | YES  |     | NULL    |       |
| LIKE    | text | YES  |     | NULL    |       |
| DISLIKE | text | YES  |     | NULL    |       |
| HREF    | text | YES  |     | NULL    |       |
+---------+------+------+-----+---------+-------+
8 rows in set (0.009 sec)


MariaDB [cei]> select * from df1_20201009;




### 201003 010 mariaDB 에 저장하기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율
    # v7 mariaDB 테이블 만들기
    # v8 mariaDB 랑 연동해서 담아보기
    # v9 중복된것 있으면 PK기준 덮어쓰기


### 200920 github 에 csv write 하는 방법
: 알아보기 >>> 일단 보류 mariaDB 가 되면 그게 더 나음

https://gist.github.com/ifightcrime/f9cae5a568656897041e

# csv save on github upload
https://stackoverflow.com/questions/46866077/how-to-upload-csv-files-to-github-repo-and-use-them-as-data-for-my-r-scripts

# Dataverse csv upload
https://projects.iq.harvard.edu/odap/frequently-asked-questions





### 200920 데이터 파일 중간 저장 깃허브 data 폴더에 해보기

### 200920 스크랩 해서 깃허브 data 폴더에 넣는 것 (폴더 따로 해서) 해보기

### 200920 크롤링 참조
https://jeongwookie.github.io/2019/03/18/190318-naver-finance-data-crawling-using-python/


### 201003 002 스크랩 해서 dataframe 만들어 보기  v3
    /*
    : 필요한 부분만 list 로 뽑기
        시각   : 2020.10.03 11:26
        제목   : 부광에서 왔습니다
        내용   : fill content at the next step
        링크   : /item/board_read.nhn?code=019170&amp;nid=145679220&amp;st=&amp;sw=&amp;page=1
        조회수 : 90
        공감   : 4
        비공감 : 0
    */
    //### 참조 국민청원
    https://colab.research.google.com/github/huisung/president_go_kr_petitions_scraping/blob/master/get_petition.ipynb#scrollTo=T7v_K1bAgNtJ

    //: 함수형으로 만들기
    //: dataframe 형식으로 담기
    //: dataframe 제목 달기
    //: 클릭해서 들어가는 페이지 내용도 가져와보기 (해당 URL 주고 loop 돌려 찾기?)
    //: v4 컨텐츠 가져오기도 함수화 시키기
    //: v5 selenium 등 부분 안쓰면 지우기
    //: v6 여러페이지 loop 로 받아오기


### 201003 002 스크랩 해서 dataframe 만들어 보기  v2
    //: 게시판 글 부분만 뽑아내기 : 앞부분 자르기
    //: 게시판 글 부분만 뽑아내기 : 뒷부분 자르기
    //  :  href="/item/board_read.nhn 아닌것 골라내기

### 201003 002 스크랩 해서 dataframe 만들어 보기  v1
    # 신풍제약  019170  : 제2의 신풍을 찾아보자
        https://finance.naver.com/item/board.nhn?code=019170&page=1
        https://finance.naver.com/item/board.nhn?code=019170&page=2
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675817&st=&sw=&page=8
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675611&st=&sw=&page=9
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675609&st=&sw=&page=9
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675261&st=&sw=&page=1

        https://m.blog.naver.com/PostView.nhn?blogId=timtaeil&logNo=221420471952&categoryNo=30&proxyReferer=https:%2F%2Fwww.google.com%2F

        ============================================================
        <tr align="center" onmouseout="mouseOut(this)" onmouseover="mouseOver(this)">
        <td><span class="tah p10 gray03">2020.10.03 11:40</span></td>
        <td class="title">
        <a href="/item/board_read.nhn?code=019170&amp;nid=145679533&amp;st=&amp;sw=&amp;page=1" onclick="return singleSubmitCheck();" title="셀트랑 신풍 둘다 승인되지 않을까?">셀트랑 신풍 둘다 승인되지 않을까?
                        <span class="tah p9" style="color:#639933">[<b>4</b>]</span>
        <img height="10" src="https://ssl.pstatic.net/imgstock/images5/new.gif" width="10"/></a></td>
        <td class="p11">
                        seoh****
                    </td>
        <td><span class="tah p10 gray03">99</span></td>
        <td><strong class="tah p10 red01">9</strong></td>
        <td><strong class="tah p10 blue01">1</strong></td>
        </tr>

        http://hleecaster.com/python-web-crawling-with-beautifulsoup/
        http://hleecaster.com/narajangteo-crawling/

        https://stackoverflow.com/questions/40760441/exclude-unwanted-tag-on-beautifulsoup-python

        https://jungwoon.github.io/python/crawling/2018/04/12/Crawling-2/




    # KODEX 200선물인버스2X  252670 : 특정종목이 아닌 전체 흐름



### 201003 001 어디 스크랩 할지 정하기
    DC? >>> 너무 뻘글 - 노이즈가 많음
    팍스넷?
    네이버 인버스?

    네이버 - 신풍제약
    개인매수 상위종목 - 인기검색종목

    # NAVER 인기검색종목 (글이 많은 것)
        신풍제약    019170
        KODEX 200선물인버스2X    252670


### 200920 NAS MariaDB 접속
    ## http://himchan1185.synology.me:5000/

    ## MariaDB 유저
    cei / Mrs너구리1!

    ## Command
    # 집PC, 외부 (외부망)
    mysql -u cei -p -h 121.128.223.185 --port 3307 //OK
    # 노트북 (내부망)
    mysql -u cei -p -h 172.30.1.35 --port 3307    //OK : 노트북



### 200920 NAS 포트포워딩 설정 (외부IP로 특정 포트 접속시 특정 내부IP 장비로 포워딩)


선택  소스IP 주소 소스포트    외부포트    내부 IP 주소    내부 포트   프로토콜    설명  플래그
        -   5000-5000   172.30.1.35 5000-5000   TCP
        -   5001-5001   172.30.1.35 5001-5001   TCP
        -   21-21   172.30.1.35 21-21   TCP
        -   3307-3307   172.30.1.35 3307-3307   TCP
        -   5005-5005   172.30.1.35 5005-5005   TCP
        -   5006-5006   172.30.1.35 5006-5006   TCP
        -   80-80   172.30.1.35 80-80   TCP
        -   60003-60003 172.30.1.35 60003-60003 TCP

        ## 외부접속
        121.128.223.185:5000
        121.128.223.185:3307

        172.30.1.35:5000
        172.30.1.35:3307


        ## NAS 장비
        http://172.30.1.35:5000/

        172.30.1.35:5000
        # 외부망 접속
        https://swimdrg.tistory.com/8

        ## 와이파이
        172.30.1.254


        ## 노트북
        시스템 정보
        장비명 홈허브
        모델명/제조사 TI04-708H / allRadio
        버전  1.2.7
        날짜/시간   September 20 21:58:41 2020
        시스템업타임  2 Hour(s) 12 Minute(s) 55 Second(s) Elapsed
        메모리사용량  60.35%
        CPU사용량  1분: 3.50% / 5분: 3.00% / 15분: 2.72%
        대표 MAC 주소   00:07:89:C2:8C:96
        인터넷 연결정보
        인터페이스   WAN
        IP 할당 방식    DHCP
        IP주소    121.128.223.185
        서브넷마스크  255.255.255.0
        게이트웨이   121.128.223.254
        기본 DNS  168.126.63.1
        보조 DNS  168.126.63.2
        LAN 연결 정보
        IP 할당 정책    kt모드
        DHCP 서버 활성
        IP 주소   172.30.1.254
        서브넷마스크  255.255.255.0
        코넷 DHCP IP 범위   172.30.1.1 - 172.30.1.60
        프리미엄 DHCP IP 범위 172.30.1.128 - 172.30.1.148
        DHCP 임대시간 (sec) 3600

        ## 200920 Mysql 접속
        https://hannom.tistory.com/113
        IPv4 주소 . . . . . . . . . : 172.30.1.54 (노트북)
        IPv4 게이트웨이 . . . . . . . . . : 172.30.1.254 (노트북)

        IPv4 주소 . . . . . . . . . : 121.128.227.139 (집PC)
        http://homehub.olleh.com:8899/nat/portfwd

           연결별 DNS 접미사. . . . : kornet
           링크-로컬 IPv6 주소 . . . . : fe80::21f6:b6a6:1043:db30%6
           IPv4 주소 . . . . . . . . . : 121.128.227.139
           서브넷 마스크 . . . . . . . : 255.255.255.0
           기본 게이트웨이 . . . . . . : 121.128.227.254

        C:\WINDOWS\System32>mysql -u cei -p -h 172.30.1.54 --port 3307
        Enter password: ***********
        ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)
        시놀로지 mariaDB ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)

        ## http://172.30.1.35/phpMyAdmin/


        ## https://devks.tistory.com/2


        ## 노트북에서 연결 성공함
        172.30.1.35 //OK
        172.30.1.54
        3307

        mysql -u cei -p -h 121.128.227.185 --port 3307

        mysql -u cei -p -h 172.30.1.35 --port 3307 //OK : 노트북

        ## 외부에서 해보기


        ## 200920 mariaDB pandas 연동해보기

        ## 집 PC 에서 해보기
        # KT 와이파이 구성
        https://quasarzone.com/bbs/qb_tip/views/21528
        192.168.0.1

        # KT PC 공유기 DDNS


        # tcping
        C:\>tcping -d -s 121.128.227.139 3307

        2020:09:20 21:41:55 Probing 172.30.1.35:3307/tcp - No response - time=2007.774ms
        2020:09:20 21:41:57 Probing 172.30.1.35:3307/tcp - No response - time=2007.982ms
        2020:09:20 21:41:59 Probing 172.30.1.35:3307/tcp - No response - time=2014.208ms
        2020:09:20 21:42:01 Probing 172.30.1.35:3307/tcp - No response - time=2002.515ms

        Ping statistics for 172.30.1.35:3307
             4 probes sent.
             0 successful, 4 failed.  (100.00% fail)
        Was unable to connect, cannot provide trip statistics.

        C:\>

### 200920 MariaDB 10
## 참고
# Synology Nas에서 MariaDB 서버 처음부터 끝까지 설치하기
https://redapply.tistory.com/entry/Synology-%EC%97%90%EC%84%9C-MariaDB-%EC%84%9C%EB%B2%84-%EC%84%A4%EC%B9%98-%ED%95%9C%EB%B2%88%EC%97%90-%EB%81%9D%EA%B9%8C%EC%A7%80-%ED%95%98%EA%B8%B0

# KT 공유기 포트 포워딩
https://m.blog.naver.com/PostView.nhn?blogId=zetezz&logNo=221224911338&proxyReferer=https%3A%2F%2Fwww.google.com%2F

# 시놀로지 마리아DB 외부접속
https://lotus77.tistory.com/40

# CentOS 7 에 MariaDB 10 설치하기
https://blog.miyam.net/86

## MariaDB 10
root / Mrs너구리1!
3307

# KT 포트포워딩
https://m.blog.naver.com/PostView.nhn?blogId=zetezz&logNo=221224911338&proxyReferer=https%3A%2F%2Fwww.google.com%2F
IPv4 주소 . . . . . . . . . : 172.30.1.54 (노트북)
IPv4 주소 . . . . . . . . . : 121.128.227.139 (집PC)
http://homehub.olleh.com:8899/nat/portfwd

## phpMyAdmin
https://www.clien.net/service/board/cm_nas/12411854
 공인ip/phpMyAdmin/
포트포워딩 해야 접속 가능할 듯
https://himchan1185.synology.me:60004/phpMyAdmin/
https://himchan1185.synology.me:60003/phpMyAdmin/
121.128.227.139
포트포워딩 해야 접속 가능할 듯
http://172.30.1.35/phpMyAdmin/
노트북으로는 접속 성공함

http://172.30.1.35/phpMyAdmin/db_structure.php?server=1&db=cei&table=test


## phpMyAdmin
유저생성
cei / Mrs너구리1!

mysql -u cei -p -h 172.30.1.54 --port 3307
mysql -u cei -p -h 172.30.1.54 --port 3307

C:\WINDOWS\System32>mysql -u cei -p -h 172.30.1.54 --port 3307
Enter password: ***********
ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)


### 200920 스크랩 소스 깃허브에 넣기


### 깃허브 만들기
커뮤니티 감정 인덱스
PJ_COMM_EMO_INDEX
CEI
PJ_COMM_EMO_INDEX (Project Community Emotional Index)


https://github.com/head4ths/CEI

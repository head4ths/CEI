




$$$
### 201115 part5_일자별 주가, 일자별 거래량 가져오기
/*
: 주가
: 거래량 
: 상승/하락 구분 
: 상승/하락 수치 
# 네이버 ???
# 거래소 ???
# 회사데이터 ???
*/
# 네이버 일별시세 스크래핑 하기 



$$$
### 201115 part5_일자별 주가, 일자별 거래량 가져오기 - 엑셀에 추가 




--------------------


$$$
### 201010 part6_상관관계 분석.ipynb
# 상관관계 분석
: 글의 갯수, 감정분석 비율
: 주가, 거래량, 투자자별 매매동향 ( 거래소? )

# 상관관계 - 그래프 그려보기 : 상관관계도 분석 : 지난 학기 과제4번? 참조
# 상관관계 분석은 당일것 하고 비교도 해보고 전일것 하고도 비교해보기



--------------------


$$$
### 201110 LSTM 주가 예측 모델
# 날짜별로 된 샘플 찾기 - INPUT 만 바꿔주면 됨
# 전일의 데이터를 기초로 익일의 주가를 예측.



--------------------


### 666 데이터 만들어서 모델 추가 전이학습
    : 수작업 라벨링
    : Like 높은 것 : KODEX 레버리지로 학습시키기
    : 구글문서로 올려서 협업하기




--------------------


### 777 다른 종목 추가 스크래핑 고려 (주가보다 개인 심리가 먼저 움직인 종목)
    : 이오플로우 ?? 

### 888 발표자료
- BERT 가 긍정적인 댓글, 부정적인 댓글 잘 분류해주는 것 샘플로 보여줌 (편집의 힘!)
- BERT로 만들어낸 소문지수가 주가에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)
- 통계적인 글의 양이 거래량에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)
- 주가예측에 유용한 지표로 심리지수 인덱스가 사용된 것 보여줌 


### 999 아이디어
- 글의 갯수, 길이 등등도 피쳐로 딥러닝

- 네이버무비 + 수작업 분류한 것 합쳐서 BERT 전이학습









=====================================================================================================================
(완료)



### 201115 part4_일별인덱스생성_감성_갯수 - 060
# 엑셀로 인덱스들 그래프 그려보기


/*
*/
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T005930_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T006280_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T019170_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T058820_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T122630_index ORDER BY NEW_BAS_DT;
SELECT CNT,EMO_I,EMO_RT,R_CNT,R_EMO_I,R_EMO_RT FROM T252670_index ORDER BY NEW_BAS_DT;

INSERT INTO T006280_index
SELECT '20200104',0,0,0,0,0,0 ;

### 201115 part4_일별인덱스생성_감성_갯수 - 050 정리해서 6개 종목다 별도 테이블로 만들기 
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 일별 글 건수 인덱스 만들기 - DB작업   R_CNT  
# 일별 감정 지수 인덱스 만들기 - DB작업 R_EMO_I

# 일별 글 건수 인덱스  : CNT 
# 일별 감정 지수 인덱스  : EMO_I
# EMO_RT : 감정지수 비율 인덱스

# 일별 글 건수 인덱스   R_CNT  (추천 수 1 이상이고 추천이 더 많은 것 중)
# 일별 감정 지수 인덱스 R_EMO_I (추천 수 1 이상이고 추천이 더 많은 것 중)
# R_EMO_RT : 감정지수 비율 인덱스  (추천 수 1 이상이고 추천이 더 많은 것 중)


CREATE TABLE T006280_index AS 
WITH V1 AS 
( SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, CASE WHEN A.`LIKE` >= 1 AND `LIKE` > `DISLIKE` THEN 1 ELSE 0 END AS LIKE_I
, A.* 
FROM T006280_label A
) 
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
, CASE WHEN COUNT(*) = 0 THEN 0 ELSE NVL(SUM(label) / COUNT(*),0) END AS EMO_RT
, SUM(LIKE_I) AS  R_CNT
, SUM(LIKE_I*label) AS  R_EMO_I
, CASE WHEN SUM(LIKE_I) = 0 THEN 0 ELSE NVL(SUM(LIKE_I*label) / SUM(LIKE_I) ,0) END AS R_EMO_RT
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT 





### 201115 part4_일별인덱스생성_감성_갯수 - 060
# 엑셀로 인덱스들 그래프 그려보기

/*
*/
SELECT COUNT(*) FROM T005930_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T006280_index UNION ALL /* 281 */
SELECT COUNT(*) FROM T019170_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T058820_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T122630_index UNION ALL /* 283 */
SELECT COUNT(*) FROM T252670_index ;         /* 283 */

	/* 삼성전자 */ /* 2020 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
	SELECT COUNT(*) FROM T005930; /*415828 175MB*/

	/* 녹십자 */ /* 전기간 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
	SELECT COUNT(*) FROM T006280; /*59933 28MB*/

	/* CMG제약 */ /* 전기간 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
	SELECT COUNT(*) FROM T058820; /*81848 30MB*/

	/* KODEX 레버리지 */ /* 전기간 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
	SELECT COUNT(*) FROM T122630; /*171048 59MB*/

	/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
	SELECT COUNT(*) FROM T252670; /*339407 135MB*/

	/* 신풍제약 */ /* 2020 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
	SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201115 작업한 부분까지 설명 PPT 만들어서 공유
C:\Users\user\Desktop\# 졸프 PPT\# PPT
미래에셋_프로젝트설계및구현(5조)_중간진행공유_20201111.pptx

### 201010 part4_일별인덱스생성_감성_갯수 - 050 정리해서 6개 종목다 별도 테이블로 만들기  - 캡쳐 해서 PPT로 만들고 공유
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 일별 글 건수 인덱스 만들기 - DB작업   R_CNT  
# 일별 감정 지수 인덱스 만들기 - DB작업 R_EMO_I

# 일별 글 건수 인덱스  : CNT 
# 일별 감정 지수 인덱스  : EMO_I
# EMO_RT : 감정지수 비율 인덱스

# 일별 글 건수 인덱스   R_CNT  (추천 수 1 이상이고 추천이 더 많은 것 중)
# 일별 감정 지수 인덱스 R_EMO_I (추천 수 1 이상이고 추천이 더 많은 것 중)
# R_EMO_RT : 감정지수 비율 인덱스  (추천 수 1 이상이고 추천이 더 많은 것 중)


### 666 추천수 반영한 데이터셋 사용 - 상관관계 분석


### 201010 part4_일별인덱스생성_감성_갯수 - 040 : 추천 수 1 이상이고 추천이 더 많은 것
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 일별 글 건수 인덱스 만들기 - DB작업   R_CNT  
# 일별 감정 지수 인덱스 만들기 - DB작업 R_EMO_I

WITH V1 AS 
( SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, CASE WHEN A.`LIKE` >= 1 AND `LIKE` > `DISLIKE` THEN 1 ELSE 0 END AS LIKE_I
, A.* 
FROM T058820_label A
) 
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
, NVL(SUM(label) / COUNT(*),0) AS EMO_RT
, SUM(LIKE_I) AS  R_CNT
, SUM(LIKE_I*label) AS  R_EMO_I
, NVL(SUM(LIKE_I*label) / SUM(LIKE_I) ,0) AS EMO_RT
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT 


SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, CASE WHEN A.`LIKE` >= 1 AND `LIKE` > `DISLIKE` THEN 1 ELSE 0 END AS LIKE_I
, A.* 
FROM T058820_label A LIMIT 30;


WITH V1 AS 
( SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.* 
FROM T058820_label A
) 
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT 




### 201010 part4_일별인덱스생성_감성_갯수 - 030
# 일별 감정 지수 인덱스 만들기 : EMO_I
# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)
WITH V1 AS 
( SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.* 
FROM T058820_label A
) 
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
, SUM(label) AS  EMO_I
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT 

### 201010 part4_일별인덱스생성_감성_갯수 - 020
# 일별 글 건수 인덱스 만들기 - DB작업 : CNT
# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)
			
	SELECT 
	SUBSTR(DATE,1,10) AS BAS_DT
	, DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR AS BAS_DATE
	, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
	, A.* 
	FROM T058820_label A LIMIT 300;

WITH V1 AS 
( SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.* 
FROM T058820_label A
) 
SELECT NEW_BAS_DT
, COUNT(*) AS CNT
FROM V1
GROUP BY NEW_BAS_DT
HAVING NEW_BAS_DT BETWEEN '20200101' AND '20201009'
ORDER BY NEW_BAS_DT 


	6개 종목의 인덱스 대상기간을 맞추어
	대상 기간 : 6개 종목의 2020.1.1 ~ 2020.10.09 까지 인덱스 만듬
	(몇몇 종목은 더 길게 수집되었으나 향후 필요시에 활용)

	/* 녹십자 */ /* 전기간 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
	SELECT COUNT(*) FROM T006280; /*59933 28MB*/

	/* CMG제약 */ /* 전기간 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
	SELECT COUNT(*) FROM T058820; /*81848 30MB*/

	/* KODEX 레버리지 */ /* 전기간 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
	SELECT COUNT(*) FROM T122630; /*171048 59MB*/

	/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
	SELECT COUNT(*) FROM T252670; /*339407 135MB*/

	/* 삼성전자 */ /* 2020 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
	SELECT COUNT(*) FROM T005930; /*415828 175MB*/

	/* 신풍제약 */ /* 2020 수집완료 */
	SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
	SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201010 서론
주식에 맞는 성격 
넷째 손가락이 길면 - 남성호르몬 트레이딩 잘한다 
>>> 다 의미없음 : 인덱스들을 조합한 지표기준으로 투자 >>> 개인의 감정이 들어갈 일이 없다. 


### 201010 part3_모델불러와서_Classification수행.ipynb - 060
	# 전체 종목에 대해서 label 만들기
	
	- DB저장 시간 측정중 T005930_label_B
	- 신풍제약 019170 DB 저장중 
	


### 201010 part3_모델불러와서_Classification수행.ipynb - 060
	# 전체 종목에 대해서 label 만들기
	///* 녹십자                */ SELECT COUNT(*) FROM T006280; /*  59933  28MB */
	///* CMG제약               */ SELECT COUNT(*) FROM T058820; /*  81848  30MB */
	///* KODEX 레버리지        */ SELECT COUNT(*) FROM T122630; /* 171048  59MB */
	/* KODEX 200선물인버스2X */ SELECT COUNT(*) FROM T252670; /* 339407 135MB */
	/* 삼성전자              */ SELECT COUNT(*) FROM T005930; /* 415828 175MB */
	/* 신풍제약              */ SELECT COUNT(*) FROM T019170; /* 469022 186MB */

	>>> 다 되면 아카이빙 compress 처리하기 

6개 종목의 인덱스 대상기간을 맞추어
대상 기간 : 6개 종목의 2020.1.1 ~ 2020.10.09 까지 인덱스 만듬
(몇몇 종목은 더 길게 수집되었으나 향후 필요시에 활용)

https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=2138

/* 녹십자 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/

/* CMG제약 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
SELECT COUNT(*) FROM T058820; /*81848 30MB*/

/* KODEX 레버리지 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
SELECT COUNT(*) FROM T122630; /*171048 59MB*/

/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
SELECT COUNT(*) FROM T252670; /*339407 135MB*/

/* 삼성전자 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930; /*415828 175MB*/

/* 신풍제약 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201010 part4_일별인덱스생성_감성_갯수 - 020
# 일자 기준 잡기
# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)
# 전일의 데이터를 기초로 익일의 주가를 예측.
# 상관관계 분석은 당일것 하고 비교도 해보고 전일것 하고도 비교해보기

# 아침 08:30 이전까지 전일자에 포함 시키기  (동시호가 08:30~09:00)
SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE,'%Y.%m.%d %H:%i') AS BAS_DATE
, A.* 
FROM T058820_label A LIMIT 300;

select  *
from    `ordermaster` 
where   `Pick_date` = curdate() and
        `Pick_time` between (now() - interval 2 hour) and now() and
        `Status` = 2
		
SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR AS BAS_DATE
, A.* 
FROM T058820_label A LIMIT 300;

SELECT 
SUBSTR(DATE,1,10) AS BAS_DT
, DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR AS BAS_DATE
, DATE_FORMAT(DATE_FORMAT(DATE,'%Y.%m.%d %H:%i')- INTERVAL 30 MINUTE - INTERVAL 8 HOUR,'%Y%m%d') AS NEW_BAS_DT
, A.* 
FROM T058820_label A LIMIT 300;


### 201010 part3_모델불러와서_Classification수행.ipynb - 050
# 종목번호 변수처리


### 201010 part3_모델불러와서_Classification수행.ipynb - 040
# DB에 저장
SELECT * FROM T006280_label LIMIT 300;


### 201010 part3_모델불러와서_Classification수행.ipynb - 020
# submit set 만드는 부분 참조


### 201010 part3_모델불러와서_Classification수행.ipynb - 030
# (3) DB : 분석용 테이블 생성하기 : 태그컬럼 하나 추가 (날짜달린 원본데이터는 놔두고 여기다가 부어서 분석하기)
태그컬럼 : 작업전 2 > 작업 후 긍정1, 부정0




### 201010 part3_모델불러와서_Classification수행.ipynb - 010
# (2) 모델 불러오기 ( 불러온 걸로 시연 해보기 )
* 앞부분에 모델 갱신시점 확인도 하기
!ls -l --block-size=K /content/gdrive/'My Drive'/bert_model_save
!ls -l --block-size=M /content/gdrive/'My Drive'/bert_model_save/pytorch_model.bin


### 201010 part3_모델불러와서_Classification수행.ipynb - 001
## part3_모델불러와서_Classification수행

# (1) dataframe 불러오기
//* mariaDB 에 저장된 것 가져와서 감정분석
//: 저장된 데이터 dataframe 으로 불러와서 감정분석해서 건별로 태그 달기
//: dataframe에 컬럼1개 추가
//df['label'] = 0
//: 태그 단 것 DB에 저장하기 (컬럼 1개 추가)

/* 녹십자 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/




###
[매주 화 18:30 - 21:30] 나 수업
[매주 화 19:00~22:00] 지연 줌 수업


### 비고
좋아요는 의미있는 것 거르는 노이즈 제거용으로만.. 감성분석은 BERT 로만 했다고 하는게 더 좋아보임


### 프로토타입
(1) MariaDB : 데이터 수집한 것 하나로 합치기
(2) MariaDB : 라벨 컬럼 하나 추가해서 합친 테이블 만들기
(3) Colab 으로 모델 불러오기
     + DB 불러오기
     + 모델링 해서 라벨 달기
     + 다시 DB에 저장하기
(4) 주가, 거래량 데이터 어디서 구하기 : 거래소? 회사내?


### 도서관 외 미팅 공간
: 오픈된 공간이 더 좋음. 칸막이 있으면 별로.
- 도서관 : 08:00~22:00 (하나스퀘어, 4층 노트북)
# 과도 5층 도서관 앞 휴게룸
# 과도 4층 휴게실
# 과도1층 인피니트라운지
# 지하1층 델타라운지
# 하스 중앙 피아노실
# 과도1층 문쪽 공간(와이파이 잘 안됨)

### 일정
12 [D-4W] 11/17 2,3 이상근, 김현우
13 [D-3W] ★11/24 4,5 강재우, 백승준
14 [D-2W] 12/1 1,6 정순영, 임희석
15 [D-1W] 12/8 2,3 이상근, 김현우
16 [D-W]★ ★ 12/15 학교최종발표 : 1,2,3,4,5,6 정순영, 이상근, 김현우, 강재우, 백승준, 임희석

### 조편성
강재우 - 유근영
백승준 - 박준하
정순영 - 이인희
임희석 - 노시희, 오세현
이상근 - 백전호, 성다야
김현우 - 김유석

### 201104 회식
프로젝트 팀원과 지도교수님의 식사는
기존 이용하시던 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부) 외 더씨, 무르무르드구스토 에서도 가능합니다^^
(금액은 1회 30만원정도로 생각하고 있으나, 특별히 제한은 없는 것으로 하겠습니다)

다만 선결제가 필요한 식당이 있어,
식사하시기 전에 일정과 식사장소를 행정실로 전달 부탁드리겠습니다~

학생분들께 제공해드렸던 식대지원(1인 12,000원까지)은 기존에 안내드린 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부)에서만 이용가능하십니다!


### (D-5W) 졸업 프로젝트
nbooooo@korea.ac.kr (mailto:nbooooo@korea.ac.kr) (mailto:nbooooo@korea.ac.kr (mailto:nbooooo@korea.ac.kr))
http://korea-ac-kr.zoom.us/ (http://korea-ac-kr.zoom.us/) (http://korea-ac-kr.zoom.us/ (http://korea-ac-kr.zoom.us/))

[매주 화 18:30 - 21:30] 나 수업
[매주 화 19:00~22:00] 지연 줌 수업


### 좋아요는 의미있는 것 거르는 노이즈 제거용으로만.. 감성분석은 BERT 로만 했다고 하는게 더 좋아보임

### 프로토타입
(1) MariaDB : 데이터 수집한 것 하나로 합치기
(2) MariaDB : 라벨 컬럼 하나 추가해서 합친 테이블 만들기
(3) Colab 으로 모델 불러오기
     + DB 불러오기
     + 모델링 해서 라벨 달기
     + 다시 DB에 저장하기
(4) 주가, 거래량 데이터 어디서 구하기 : 거래소? 회사내?



### 공부장소
: 오픈된 공간이 더 좋음. 칸막이 있으면 별로.
- 도서관 : 08:00~22:00 (하나스퀘어, 4층 노트북)
- 과도 5층 방
- 과도 1층 오픈 라운지
- 델타스퀘어 : 과도 지하1층
- 과도 4층 정수기 방
- 하스 피아노방
- 과도 1층 엘리베이터옆 후문쪽 자리 : 와이파이 안 됨

### 빅스 미국공포지수

### 201104 회식
프로젝트 팀원과 지도교수님의 식사는
기존 이용하시던 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부) 외 더씨, 무르무르드구스토 에서도 가능합니다^^
(금액은 1회 30만원정도로 생각하고 있으나, 특별히 제한은 없는 것으로 하겠습니다)

다만 선결제가 필요한 식당이 있어,
식사하시기 전에 일정과 식사장소를 행정실로 전달 부탁드리겠습니다~

학생분들께 제공해드렸던 식대지원(1인 12,000원까지)은 기존에 안내드린 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부)에서만 이용가능하십니다!

더씨 : 네이버
출처 : 네이버 플레이스
 - http://naver.me/FSNFaVtG

무르무르드구스토 : 네이버
출처 : 네이버 플레이스
 - http://naver.me/FkbR0em0

다만 선결제가 필요한 식당이 있어,
식사하시기 전에 일정과 식사장소를 행정실로 전달 부탁드리겠습니다~

학생분들께 제공해드렸던 식대지원(1인 12,000원까지)은 기존에 안내드린 4개 식당(원진, 맘스터치, 전주완산골, 한사우순두부)에서만 이용가능하십니다!


### 조편성
강재우 - 유근영
백승준 - 박준하

정순영 - 이인희
임희석 - 노시희, 오세현

이상근 - 백전호, 성다야
김현우 - 김유석

### 1차 : 아이디어 발표하고 정하기

### 2차 : 정한 아이디어 구체화 시키고 데이터, 모델등 구체화
  2,3,4,5,최종 별 과업 마일스톤 만들고 역할분담
  일정작성

### 3차 : 프로토타입 개발 시연

# 주차 : 9/1~ 12/31

제2기 디지털융합금융학과
프로젝트설계및구현 1개 과목
강의시간은 18:30 - 21:30

실시간 온라인 강의로 진행

날짜 조 지도교수

9/1,9/22,10/13,11/3,11/24,12/15

1 [D-15W]  ★9/1 4,5 강재우, 백승준
2 [D-14W]  9/8 1,6 정순영, 임희석
3 [D-13W]  9/15 2,3 이상근, 김현우
4 [D-12W]  ★9/22 4,5 강재우, 백승준
5 [D-11W]  9/29 1,6 정순영, 임희석
6 [D-10W]  10/6 2,3 이상근, 김현우
7 [D-9W]  ★10/13 4,5 강재우, 백승준
8 [D-8W]  10/20 1,6 정순영, 임희석
9 [D-7W]  10/27 2,3 이상근, 김현우
-------------------------------------------------------
10 [D-6W]  ★11/3 4,5 강재우, 백승준
11 [D-5W]  11/10 1,6 정순영, 임희석
12 [D-4W]  11/17 2,3 이상근, 김현우
13 [D-3W]  ★11/24 4,5 강재우, 백승준
14 [D-2W]  12/1 1,6 정순영, 임희석
15 [D-1W]  12/8 2,3 이상근, 김현우
16 [D-W]★ ★  12/15 학교최종발표 : 1,2,3,4,5,6 정순영, 이상근, 김현우, 강재우, 백승준, 임희석


2. 진행방식
  1) 수업일 전날까지 지도교수에게 발표자료 전달
  2) 수업일에는 모든 원생들이 실시간으로 수업에 참여
  3) 발표 조는 수업시간에 발표를 진행한 후 지도교수의 피드백을 받음
  4) 마지막(12/15) 발표는 각 조당 20분씩 발표하며 지도교수의 피드백 없이 진행

위 사항을 참고하시어 다음학기 강의 및 졸업 프로젝트를 진행해주시기 바랍니다.
감사합니다.


### 도서관 외 미팅 공간
# 과도 5층 도서관 앞 휴게룸
# 과도 4층 휴게실
# 과도1층 인피니트라운지
# 지하1층 델타라운지
# 하스 중앙 피아노실
# 과도1층 문쪽 공간(와이파이 잘 안됨)


###
(애들이랑 눈오는날 놀러왔을 때는 내려주고 집에주차 > 따릉이로 이동 > 다시 픽업 식으로 해도 될 듯?)
고려대 주차 : 2시간 까지 5500원 (30분 1천원)
2시간 지나면 10분당 천원해서 시간당 6천원임
# 과도1층 문쪽 공간(와이파이 잘 안됨)
중앙광장 지하 군데군데 소파&책상 있고 (KU프라이드 라운지라는 작은 라운지 있는데 여는지 모르겠네요) 하나스퀘어에는 피아노방이라는 라운지 있습니다.


### 201110 압축
    (1) ROW_FORMAT=COMPRESSED
    (2) 엔진 = ARCHIVE

    ALTER TABLE 'T005930_B' ROW_FORMAT=COMPRESSED;
    ??? ALTER TABLE T006280_B PAGE_COMPRESSED=1;
    HeidiSQL 에서 기본으로 지원함


### 201110 MariaDB 압축 옵션 (PASS : 필요없음)
SHOW VARIABLES WHERE Variable_name LIKE "have_%" OR Variable_name LIKE "%_compression_%"

innodb_compression_algorithm zlib


### 20201110 테이블 백업 해놓기
CREATE TABLE T006280_B AS SELECT * FROM T006280;
CREATE TABLE T058820_B AS SELECT * FROM T058820;
CREATE TABLE T122630_B AS SELECT * FROM T122630;
CREATE TABLE T252670_B AS SELECT * FROM T252670;
CREATE TABLE T005930_B AS SELECT * FROM T005930;
CREATE TABLE T019170_B AS SELECT * FROM T019170;

ALTER TABLE T006280_B PAGE_COMPRESSED=1;
OPTIMIZE TABLE T006280_B;

/* 녹십자 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=2138
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/

/* CMG제약 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
SELECT COUNT(*) FROM T058820; /*81848 30MB*/

/* KODEX 레버리지 */ /* 전기간 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
SELECT COUNT(*) FROM T122630; /*171048 59MB*/

/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
SELECT COUNT(*) FROM T252670; /*339407 135MB*/

/* 삼성전자 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930; /*415828 175MB*/

/* 신풍제약 */ /* 2020 수집완료 */
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/


### 201110 데이터 수집 현황

/* 녹십자 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=2138
SELECT MIN(DATE), MAX(DATE) FROM T006280;  /*2017.06.08 14:16 2020.11.07 19:51*/
SELECT COUNT(*) FROM T006280; /*59933 28MB*/
SELECT * FROM T006280 LIMIT 10;

/* CMG제약 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=058820&page=1
https://finance.naver.com/item/board.nhn?code=058820&page=4086
SELECT MIN(DATE), MAX(DATE) FROM T058820;  /*2017.06.07 04:49 2020.11.08 16:54*/
SELECT COUNT(*) FROM T058820; /*81848 30MB*/
SELECT * FROM T058820 LIMIT 10;

/* KODEX 레버리지 */ /* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=122630&page=1
https://finance.naver.com/item/board.nhn?code=122630&page=4506
SELECT MIN(DATE), MAX(DATE) FROM T122630;  /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
SELECT COUNT(*) FROM T122630; /*171048 59MB*/
SELECT * FROM T122630 LIMIT 10;

/* KODEX 200선물인버스2X */ /* 2020 수집완료 */
https://finance.naver.com/item/board.nhn?code=252670&page=1
https://finance.naver.com/item/board.nhn?code=252670&page=19871
SELECT MIN(DATE), MAX(DATE) FROM T252670;  /* 2019.12.17 10:09 2020.10.10 12:23 */
SELECT COUNT(*) FROM T252670; /*339407 135MB*/
SELECT * FROM T252670 LIMIT 10;

/* 삼성전자 */ /* 2020 수집완료 */
https://finance.naver.com/item/board.nhn?code=005930&page=1
https://finance.naver.com/item/board.nhn?code=005930&page=21800
SELECT MIN(DATE), MAX(DATE) FROM T005930;  /* 2019.12.09 15:08 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930; /*415828 175MB*/
SELECT * FROM T005930 LIMIT 10;

/* 신풍제약 */ /* 2020 수집완료 */
https://finance.naver.com/item/board.nhn?code=019170&page=1
https://finance.naver.com/item/board.nhn?code=019170&page=25000
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/
SELECT * FROM T019170 LIMIT 10;


### 201110  KODEX 200선물인버스2X

/*SELECT MIN(DATE), MAX(DATE) FROM T252670;*/ /*2020.04.29 00:00 ~ 2020.10.10 12:23*/
/* 2020.04.28 10:10 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201009;*/  /*2020.09.18 09:14 2020.10.10 12:23*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201010;*/  /*2020.07.21 08:08 2020.09.23 17:36*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201011;*/  /*2020.04.28 10:10 2020.08.11 12:33*/
SELECT MIN(DATE), MAX(DATE) FROM T252670_20201107_B;   /*2019.12.17 10:09 2020.04.29 11:07*/

# 로그2

DELETE FROM T252670 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.09.19 00:00 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201010 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.21 08:08 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.22 00:02 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201011 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.28 10:10 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.04.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.29 00:00 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201107_B WHERE DATE < '2020.04.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.28 10:10 ~ 2020.10.10 12:23*/


### 201110

/* 신풍제약 */
https://finance.naver.com/item/board.nhn?code=019170&page=1
https://finance.naver.com/item/board.nhn?code=019170&page=25000
SELECT MIN(DATE), MAX(DATE) FROM T019170;  /* 2018.10.01 21:23 2020.10.10 16:18 */
SELECT COUNT(*) FROM T019170; /*469022 186MB*/
SELECT * FROM T019170 LIMIT 10;

/*SELECT MIN(DATE), MAX(DATE) FROM T019170*/ /*2020.07.28 00:00 ~ 2020.10.10 16:18*/
/* 2020.07.27 19:54 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201009;*/  /*2020.09.28 13:37 2020.10.10 16:18*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201010;*/  /*2020.09.09 09:27 2020.10.02 13:29*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201011;*/  /*2020.07.27 19:54 2020.09.09 10:37*/
SELECT MIN(DATE), MAX(DATE) FROM T019170_20201107_B;   /*2018.10.01 21:23 2020.10.20 22:32*/

# 로그
DELETE FROM T019170 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.29 00:01 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201010 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:27 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:30 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201011 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.07.27 19:54 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.07.28 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170 /*2020.07.28 00:00 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201107_B WHERE DATE < '2020.07.28 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.07.27 19:54 ~ 2020.10.10 16:18*/


### 201110

/* 삼성전자 */
https://finance.naver.com/item/board.nhn?code=005930&page=10800
https://finance.naver.com/item/board.nhn?code=005930&page=15800 $$$ (1)
https://finance.naver.com/item/board.nhn?code=005930&page=21800 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T005930;*/ /*2020.04.05 00:00 ~ 2020.10.10 16:07*/
/* 2020.04.04 09:29 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201009;*/  /*2020.09.14 14:16 2020.10.10 16:07*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201010;*/  /*2020.07.06 16:57 2020.09.18 03:08*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201011;*/  /*2020.04.04 09:29 2020.09.18 03:11*/
SELECT MIN(DATE), MAX(DATE) FROM T005930_20201107_B;   /*2019.12.09 15:08 2020.07.28 03:34*/


# 로그 1

SELECT STR_TO_DATE('2020090100','%Y%m%d%H');
SELECT COUNT(*) FROM T005930_20201009 WHERE DATE < '2020.09.15 00:00';

DELETE FROM T005930 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.09.15 00:06 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201010 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.06 16:57 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.07 00:03 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201011 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.04 09:29 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.04.05 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.05 00:00 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201107_B WHERE DATE < '2020.04.05 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2019.12.09 15:08 2020.10.10 16:07*/

### 201010 데이터 2차 수집 - 1000~4000 page
토요일 밤에 집에서 돌려놓기
일요일 저녁에 집에서 확인하기


### 201010 모델 제일 결과 좋았던 걸로 학습시켜서 저장하기
# part2_bert감정분석_모델학습및저장
일요일에 회사에서 돌려놓기
일요일에 집에서 저녁에 확인하기


### 201110 깃허브
## 깃허브
https://github.com/head4ths/CEI

# 깃허브 폴더
/data                    데이터 파일 넣는 곳
/model                   모델 넣는 곳
/etc_reference_html      참고용 html 넣는 곳
/etc_reference_source    참고용 소스 넣는 곳


### 201109 데이터 수집 현황


/* 녹십자 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=3000
/*SELECT MIN(DATE), MAX(DATE) FROM T006280_20201107;*/  /*2017.06.08 14:16 2020.11.07 19:51*/

/* KODEX 레버리지 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=122630&page=1
/*SELECT MIN(DATE), MAX(DATE) FROM T122630;*/ /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201009;*/  /*2020.05.24 04:13 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201010;*/  /*2018.12.02 19:11 2020.06.24 17:13*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201011;*/  /*2017.06.07 07:55 2018.12.03 20:39*/

/* CMG제약 */
https://finance.naver.com/item/board.nhn?code=058820&page=1
https://finance.naver.com/item/board.nhn?code=058820&page=4082  다다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T058820;*/ /* */



/* KODEX 200선물인버스2X */
https://finance.naver.com/item/board.nhn?code=252670&page=10900
https://finance.naver.com/item/board.nhn?code=252670&page=15900 $$$ (1)
https://finance.naver.com/item/board.nhn?code=252670&page=20900 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T252670;*/ /*2020.04.29 00:00 ~ 2020.10.10 12:23*/
/* 2020.04.28 10:10 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201009;*/  /*2020.09.18 09:14 2020.10.10 12:23*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201010;*/  /*2020.07.21 08:08 2020.09.23 17:36*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201011;*/  /*2020.04.28 10:10 2020.08.11 12:33*/

/* 삼성전자 */
https://finance.naver.com/item/board.nhn?code=005930&page=10800
https://finance.naver.com/item/board.nhn?code=005930&page=15800 $$$ (1)
https://finance.naver.com/item/board.nhn?code=005930&page=21800 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T005930;*/ /*2020.04.05 00:00 ~ 2020.10.10 16:07*/
/* 2020.04.04 09:29 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201009;*/  /*2020.09.14 14:16 2020.10.10 16:07*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201010;*/  /*2020.07.06 16:57 2020.09.18 03:08*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201011;*/  /*2020.04.04 09:29 2020.09.18 03:11*/

/* 신풍제약 */
https://finance.naver.com/item/board.nhn?code=019170&page=11300  다음번
https://finance.naver.com/item/board.nhn?code=019170&page=25000  다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T019170*/ /*2020.07.28 00:00 ~ 2020.10.10 16:18*/
/* 2020.07.27 19:54 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201009;*/  /*2020.09.28 13:37 2020.10.10 16:18*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201010;*/  /*2020.09.09 09:27 2020.10.02 13:29*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201011;*/  /*2020.07.27 19:54 2020.09.09 10:37*/




### 201107  4차 데이터 수집
/*
(1) 적재기간 확인하고 추가로 적재

(2) 테이블간 겹치는 부분 확인해서 중복없이 한 테이블로 합치기
CREATE TABLE T005930 AS SELECT * FROM T005930_20201009;
CREATE TABLE T252670 AS SELECT * FROM T252670_20201009;
CREATE TABLE T019170 AS SELECT * FROM T019170_20201009;
CREATE TABLE T122630 AS SELECT * FROM T122630_20201009;


*/



/* KODEX 200선물인버스2X */
https://finance.naver.com/item/board.nhn?code=252670&page=10900
https://finance.naver.com/item/board.nhn?code=252670&page=15900 $$$ (1)
https://finance.naver.com/item/board.nhn?code=252670&page=20900 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T252670;*/ /*2020.04.29 00:00 ~ 2020.10.10 12:23*/
/* 2020.04.28 10:10 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201009;*/  /*2020.09.18 09:14 2020.10.10 12:23*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201010;*/  /*2020.07.21 08:08 2020.09.23 17:36*/
/*SELECT MIN(DATE), MAX(DATE) FROM T252670_20201011;*/  /*2020.04.28 10:10 2020.08.11 12:33*/

/* 삼성전자 */
https://finance.naver.com/item/board.nhn?code=005930&page=10800
https://finance.naver.com/item/board.nhn?code=005930&page=15800 $$$ (1)
https://finance.naver.com/item/board.nhn?code=005930&page=21800 $$$ (2)
/*SELECT MIN(DATE), MAX(DATE) FROM T005930;*/ /*2020.04.05 00:00 ~ 2020.10.10 16:07*/
/* 2020.04.04 09:29 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201009;*/  /*2020.09.14 14:16 2020.10.10 16:07*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201010;*/  /*2020.07.06 16:57 2020.09.18 03:08*/
/*SELECT MIN(DATE), MAX(DATE) FROM T005930_20201011;*/  /*2020.04.04 09:29 2020.09.18 03:11*/

/* 신풍제약 */
https://finance.naver.com/item/board.nhn?code=019170&page=11300  다음번
https://finance.naver.com/item/board.nhn?code=019170&page=25000  다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T019170*/ /*2020.07.28 00:00 ~ 2020.10.10 16:18*/
/* 2020.07.27 19:54 이전 수집 필요 */
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201009;*/  /*2020.09.28 13:37 2020.10.10 16:18*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201010;*/  /*2020.09.09 09:27 2020.10.02 13:29*/
/*SELECT MIN(DATE), MAX(DATE) FROM T019170_20201011;*/  /*2020.07.27 19:54 2020.09.09 10:37*/



/* CMG제약 */
https://finance.naver.com/item/board.nhn?code=058820&page=1
https://finance.naver.com/item/board.nhn?code=058820&page=4082  다다음번
/*SELECT MIN(DATE), MAX(DATE) FROM T058820;*/ /* */



/* 녹십자 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=006280&page=1
https://finance.naver.com/item/board.nhn?code=006280&page=3000
/*SELECT MIN(DATE), MAX(DATE) FROM T006280_20201107;*/  /*2017.06.08 14:16 2020.11.07 19:51*/

/* KODEX 레버리지 */
/* 전기간 수집완료 */
https://finance.naver.com/item/board.nhn?code=122630&page=1
/*SELECT MIN(DATE), MAX(DATE) FROM T122630;*/ /*2017.06.07 07:55 ~ 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201009;*/  /*2020.05.24 04:13 2020.10.10 07:40*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201010;*/  /*2018.12.02 19:11 2020.06.24 17:13*/
/*SELECT MIN(DATE), MAX(DATE) FROM T122630_20201011;*/  /*2017.06.07 07:55 2018.12.03 20:39*/


# 로그 1

SELECT STR_TO_DATE('2020090100','%Y%m%d%H');
SELECT COUNT(*) FROM T005930_20201009 WHERE DATE < '2020.09.15 00:00';

DELETE FROM T005930 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.09.15 00:06 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201010 WHERE DATE < '2020.09.15 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.06 16:57 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.07.07 00:03 ~ 2020.10.10 16:07*/

INSERT INTO T005930 SELECT * FROM T005930_20201011 WHERE DATE < '2020.07.07 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.04 09:29 ~ 2020.10.10 16:07*/

DELETE FROM T005930 WHERE DATE < '2020.04.05 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T005930; /*2020.04.05 00:00 ~ 2020.10.10 16:07*/


# 로그2

DELETE FROM T252670 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.09.19 00:00 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201010 WHERE DATE < '2020.09.19 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.21 08:08 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.07.22 00:02 ~ 2020.10.10 12:23*/

INSERT INTO T252670 SELECT * FROM T252670_20201011 WHERE DATE < '2020.07.22 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.28 10:10 ~ 2020.10.10 12:23*/

DELETE FROM T252670 WHERE DATE < '2020.04.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T252670; /*2020.04.29 00:00 ~ 2020.10.10 12:23*/


# 로그3

DELETE FROM T019170 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.29 00:01 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201010 WHERE DATE < '2020.09.29 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:27 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.09.09 09:30 ~ 2020.10.10 16:18*/

INSERT INTO T019170 SELECT * FROM T019170_20201011 WHERE DATE < '2020.09.09 09:30';
SELECT MIN(DATE), MAX(DATE) FROM T019170; /*2020.07.27 19:54 ~ 2020.10.10 16:18*/

DELETE FROM T019170 WHERE DATE < '2020.07.28 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T019170 /*2020.07.28 00:00 ~ 2020.10.10 16:18*/


# 로그4


DELETE FROM T122630 WHERE DATE < '2020.05.25 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2020.05.25 00:12 ~ 2020.10.10 07:40*/

INSERT INTO T122630 SELECT * FROM T122630_20201010 WHERE DATE < '2020.05.25 00:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2018.12.02 19:11 ~ 2020.10.10 07:40*/

DELETE FROM T122630 WHERE DATE < '2018.12.02 20:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2018.12.02 21:20 ~ 2020.10.10 07:40*/

INSERT INTO T122630 SELECT * FROM T122630_20201011 WHERE DATE < '2018.12.02 20:00';
SELECT MIN(DATE), MAX(DATE) FROM T122630; /*2017.06.07 07:55 ~ 2020.10.10 07:40*/


### 201010 데이터 수집 현황
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

##
계속 뒤로 밀리니까
(1차) 1~1000 page 스크랩했으면 (5H)
(2차) 1000~4000 page 스크랩 하면 됨. (15H 예상) (테이블 이름 20201010으로 바꾸기)
    >>> 계속 밀리니까 겹치는 부분이 있음 (1000~1100 페이지 정도까지의 앞부분)
    >>> 합칠때는 시간순으로 보고 중복된 것 지우고 합치기

##
: 1000번 돌림 > 20000개 > 15000초 (4H)  18MB
: 총 40MB
SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

SELECT COUNT(*) FROM T122630_20201009; /* 20000건 완료 */ /* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT COUNT(*) FROM T252670_20201009; /* 20000건 완료 */ /* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT COUNT(*) FROM T019170_20201009; /* 20000건 완료 */ /* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930_20201009; /* 20000건 완료 */ /* 2020.09.28 13:37 ~ 2020.10.10 16:18 */

/* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT * FROM T122630_20201009 ORDER BY DATE asc;
SELECT * FROM T122630_20201009 ORDER BY DATE desc;

/* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT * FROM T252670_20201009 ORDER BY DATE asc;
SELECT * FROM T252670_20201009 ORDER BY DATE desc;

/* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT * FROM T005930_20201009 ORDER BY DATE asc;
SELECT * FROM T005930_20201009 ORDER BY DATE desc;

/* 2020.09.28 13:37 ~ 2020.10.10 16:18 */
SELECT * FROM T019170_20201009 ORDER BY DATE asc;
SELECT * FROM T019170_20201009 ORDER BY DATE desc;


### 201010
    ## 발표 컨셉
    소문에 사서 뉴스에 판다.
    뉴스를 분석하면 이미 늦는다
    소문을 분석해라!!!

    소문에 해당하는
    소문지수

    이오 플로우

    >>>>> 여러 종목 돌리다 보면... 분명히 소문지수가 주가보다 먼저 튀는 종목이 있을 것임
    >>>>> 발표 용으로는 제격

    # 발표 컨셉
    알라딘 - 진흙속의 진주
    뻘(진흙)글 속에서 통계적으로 숨겨진 진짜배기 소문을 찾다

    # 발표 컨셉
    BERT 정확도 학습시 test set에 대하여 90몇 프로 나온걸로 정확도 높아 정확한 척
    현혹 가능.


### 200910

    결정한 부분
    - 개발 환경 : 구글 Colab 및 Github (https://github.com/head4ths/CEI/)) 사용
    - 데이터 저장 : 개인 NAS 서버에 Maria DB (스크래핑 데이터, 개인매수동향, 주가데이터 보관) (http://121.128.223.185:3307)  cei / Mrs너구리1!  : 개인 PC 라 IP 바뀔수도 ??? 프로젝트 기간이 짧아서 별 상관없을 듯 )
    - 모델 중간 저장 : ??? 구글 드라이브 ???

    Todo 크게 네 부분
    ( 1,2,3,4 가 순차적이라 프로토타입 만들어보는 단계에서는 전원 참여 식으로 운영 )
    ( 프로토타입으로 검증되면 개선은 각자 담당 영역 맡아서 진행 )
    - (1) 스크래핑 및 전처리 후 DB화 저장
    - (2) BERT 감정분석 ( 네이버 무비 영화 감성 분석 - 전이학습 모델 사용 ) 후 태그 달기
    - (3) 개인매수동향 ( 코스콤 등에서 다운로드 ) 이나 주가데이터와 상관관계 분석 - 사회공학적인 인사이트 필요
    - (4) 발표


### 201003 040 일자별 인덱스 만들기


### 201009 050 일자별 주가, 일자별 거래량 가져오기


### 201003 060 상관관계 분석
    : 글의 갯수, 감정분석 비율
    : 주가, 거래량, 투자자별 매매동향 ( 거래소? )

### 201003 020 Bert 소스 정리
## part3_모델불러와서_Classification수행
# 불러오기 ( 불러온 걸로 시연 해보기 )

# 앞부분에 모델 갱신시점 확인도 하기
!ls -l --block-size=K /content/gdrive/'My Drive'/bert_model_save
!ls -l --block-size=M /content/gdrive/'My Drive'/bert_model_save/pytorch_model.bin



### 201010  참고
Github의 경우 개당 파일은 100MB 제한이고 있지만 전체 용량 제한은 없다. Bitbucket은 개당 파일의 제한은 없지만 전체용량이 2GB이상이 안되는 제한사항이 존재 한다. Github의 경우 50Mb 는 Warning을 표시하며, 100Mb 부터는 Error를 나타낸다.


### 201010 데이터 수집 현황
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

: 1000번 돌림 > 20000개 > 15000초 (4H)  18MB
: 총 40MB
SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

SELECT COUNT(*) FROM T122630_20201009; /* 20000건 완료 */ /* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT COUNT(*) FROM T252670_20201009; /* 20000건 완료 */ /* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT COUNT(*) FROM T019170_20201009; /* 20000건 완료 */ /* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930_20201009; /* 20000건 완료 */ /* 2020.09.28 13:37 ~ 2020.10.10 16:18 */

/* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT * FROM T122630_20201009 ORDER BY DATE asc;
SELECT * FROM T122630_20201009 ORDER BY DATE desc;

/* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT * FROM T252670_20201009 ORDER BY DATE asc;
SELECT * FROM T252670_20201009 ORDER BY DATE desc;

/* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT * FROM T005930_20201009 ORDER BY DATE asc;
SELECT * FROM T005930_20201009 ORDER BY DATE desc;

/* 2020.09.28 13:37 ~ 2020.10.10 16:18 */
SELECT * FROM T019170_20201009 ORDER BY DATE asc;
SELECT * FROM T019170_20201009 ORDER BY DATE desc;


### 201003 020 Bert 소스 정리
## part2_bert감정분석_모델학습및저장
    : 검증 완료


### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 >>> 다시 불러오기 해보기
: 700MB 정도 되는 듯 ???
: 구글 드라이브 쓰는게 구글 코랩에서 저장하고 불러오는데 매우 빠름 !!! 좋은 선택 !!!
: 뭐가 저장되는지 다운 받아지면 한번 열어보기


## 참조2 (Good!)
https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP

## 핵심 키워드
# from_pretrained
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# save_pretrained
bcm.save_pretrained('the output directory path')

## 원래 소스
model = BertForSequenceClassification.from_pretrained(gv_model_nm, num_labels=2)
tokenizer = BertTokenizer.from_pretrained(gv_model_nm, do_lower_case=gv_do_lower_case)

## 불러오기 ( 불러온 걸로 시연 해보기 )



### 201010 bert 소스 수정
## 소스
(Colab) part2_bert감정분석

## 학습시 과정에서 시간 출력하게 수정하기
#615K_20200615_v19


### 201010
OSError: [Errno 95] Operation not supported: '/content/drive/Mask_RCNN' on Google Colab-자주끊기는-런타임-방지하기
https://stackoverflow.com/questions/60132033/oserror-errno-95-operation-not-supported-content-drive-mask-rcnn-on-googl




### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조4
# BertForSequenceClassification
# How to save a model as a BertModel #2094
https://github.com/huggingface/transformers/issues/2094

save_pretrained

from transformers import BertForSequenceClassification, BertConfig

config = BertConfig.from_pretrained("bert-base-cased", num_labels=3)
model = BertForSequenceClassification.from_pretrained("bert-base-cased", config=config)
model.load_state_dict(torch.load("SAVED_SST_MODEL_DIR/pytorch_model.bin"))


# How to load BertforSequenceClassification models weights into BertforTokenClassification model? https://stackoverflow.com/questions/60897514/how-to-load-bertforsequenceclassification-models-weights-into-bertfortokenclassi


new_model = BertForTokenClassification(config=config)
new_model.bert.load_state_dict(model.bert.state_dict())

!mkdir -p saved_model
model.save('saved_model/my_model')

# BertForTokenClassification model.save

model = BertForSequenceClassification.from_pretrained(gv_model_nm, num_labels=2)

## 참조
https://github.com/huggingface/transformers/issues/2517
# load config
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# load a sequence model
bsm = BertForTokenClassification.from_pretrained('bert-base-uncased', config=conf)
# get bert core model
bcm = bsm.bert
# save the core model
bcm.save_pretrained('the output directory path')
# you also need to save your tokenizer in the same directory

## 참조2 - 검색
https://www.google.com/search?sxsrf=ALeKk013i9Ir4dm_XebJUjc_kCRRRuGjew%3A1602323594165&ei=ioSBX8m8CaTKmAWB46SQBw&q=BertForTokenClassification+model+save_pretrained+colab&oq=BertForTokenClassification+model+save_pretrained+colab&gs_lcp=CgZwc3ktYWIQAzoECCMQJzoHCCMQrgIQJzoHCCEQChCgAVCXLFi4PmDhQmgBcAB4AIAB1QGIAZELkgEFMC43LjGYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwjJqZTF4KnsAhUkJaYKHYExCXIQ4dUDCA0&uact=5

## 참조2 (Good!)
https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP

## 참조3
https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb#scrollTo

## 핵심 키워드
# from_pretrained
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# save_pretrained
bcm.save_pretrained('the output directory path')




### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조2
# 참조2
http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221564411127&categoryNo=49&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

from os import path
from google.colab import drive

model_dir_name = 'bert_model_save'
drive.mount('/content/gdrive')
model_base_dir = path.join('./gdrive/My Drive/', model_dir_name)
if not path.exists(model_base_dir):
  print('Check your google drive directory. See you file explorer')


with open(path.join(model_base_dir, "myfile.txt"), "w") as f:
    f.write("Google Colab is good!!!")



### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조1
## 참조 1,2,3은 개념만 대강 보고 참조4에서 bert 모델용으로 나온 것 쓰기

## 모델 중간 저장 - 텐서 플로우
# 참조1
https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko#scrollTo=mQF_dlgIVOvq

tf.keras.callbacks.ModelCheckpoint
checkpoint_path = "training_1/cp.ckpt"

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest_checkpoint

# 이전에 저장한 가중치를 로드합니다
model.load_weights(latest)


수동으로 가중치 저장하기
앞에서 가중치를 모델에 로드하는 방법을 보았습니다. 수동으로 가중치를 저장하는 것도 쉽습니다. Model.save_weights 메서드를 사용합니다. tf.keras는, 특히 save_weights는 .ckpt 확장자를 가진 텐서플로 체크포인트 포맷을 사용합니다(.h5 확장자의 HDF5으로 저장하는 것은 Save and serialize models 가이드에서 다룹니다):


전체 모델 저장하기
model.save 메서드를 호출하여 모델의 구조, 가중치, 훈련 설정을 하나의 파일/폴더에 저장합니다. 모델을 저장하기 때문에 원본 파이썬 코드*가 없어도 사용할 수 있습니다. 옵티마이저 상태가 복원되므로 정확히 중지한 시점에서 다시 훈련을 시작할 수 있습니다.

두 개의 포맷(SavedModel과 HDF5)으로 모델을 저장할 수 있습니다. 텐서플로의 SavedModel 포맷은 TF2.x에서기본 파일 포맷입니다. 하지만 HDF5 포맷으로 저장할 수도 있습니다. 두 파일 포맷으로 전체 모델을 저장하는 방법은 아래에서 자세히 설명합니다.

전체 모델을 저장하는 기능은 매우 유용합니다. TensorFlow.js로 모델을 로드한 다음 웹 브라우저에서 모델을 훈련하고 실행할 수 있습니다(Saved Model, HDF5). 또는 모바일 장치에 맞도록 변환한 다음 TensorFlow Lite를 사용하여 실행할 수 있습니다(Saved Model, HDF5).

* 사용자 정의 객체(예를 들면 상속으로 만든 클래스나 층)는 저장하고 로드하는데 특별한 주의가 필요합니다. 아래 사용자 정의 객체 저장하기 섹션을 참고하세요.


# SavedModel로 전체 모델을 저장합니다
!mkdir -p saved_model
model.save('saved_model/my_model')

new_model = tf.keras.models.load_model('saved_model/my_model')

# 모델 구조를 확인합니다
new_model.summary()

model.load_weights(latest)
model.save
tf.keras.models.load_model


### 201010 모델 저장용 local 설치
pip install --upgrade pip
pip install -q pyyaml h5py
pip install pyyaml h5py
ERROR: Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'c:\\programdata\\anaconda3\\lib\\site-packages\\pip\\_internal\\build_env.py'
Consider using the `--user` option or check the permissions.
해당 폴더에 적절한 권한이 없어서 발생하는 문제로, 관리자 권한으로 CMD를 실행하면 해결됩니다.
C:\\Users\\user\\AppData\\Local\\Temp\\pip-uninstall-8wl6ryze\\pip.exe
C:\Users\user\AppData\Local\Temp\pip-uninstall-8wl6ryze\



### 201010 GPU
안녕하세요. 서버에 대해 1도 모르는 대학원생이 작은 서버구축 관련하여 질문 드립니다. 최근 같이 일하게 된 Psychology/Biomedical Engineering 교수님이 (작은) 연구실용 서버 구축하는 것을 도와드리게 되었습니다.
첫 번째 질문입니다. 딥러닝 연구 목적으로 생각했을 때 Tesla P100 한 대 vs. TITAN RTX 두 대 vs. RTX 2080 Ti 네 대 비교한다면 어떤 게 좋을까요? 24/7 계속해서 GPU 돌릴게 아니고 간단한 벤치마크로 봐도 RTX 2080 Ti가 나아보이는데요... 경험 있으신 분들 의견을 구합니다. 저는 Titan Xp와 1080ti 밖에 사용을 안 해봤습니다 ㅜㅜ
두 번째 질문입니다. 현재 이 연구실은 PowerEdge T440 Tower Server를 가지고 있습니다 ㅜㅜ 지금까지 GPU로 연산을 돌리지 않았아서 T440을 가지고 있는 것 같아요. Intel Xeon Gold 5118 (2.3G, 12C/24T, 10.4GT/s, 16M
Cache) x 2개, 32GB RDIMM 2666MT/s Dual Rank x 6개, 4TB 7.2K RPM SATA 6Gbps 512n 3.5in Hot-plug Hard Drive x 8개가 장착되어 있습니다. 중요한건 꽂을 수 있는 GPU가 Quadro NVS 310나 P4000 밖에 없습니다. OMG... 이런 경우에 어떤 해결책이 있을까요? Tower를 T640 (Nvidia P100 장착가능)이나 R740 (Nvidia P100 or V100 장착가능)로 바꾸는 것은 현명한 방법일까요? 혹은 RTX 2080 Ti 설치가능한 Dell Server용 Tower가 있나요?
세 번째 질문입니다. 데이터는 주로 2D 이미지 일 것 같은데요, HDD로 충분할까요 아니면 적어도 SATA SSD를 구입해야 할까요? (이 질문에 대한 답변은 충분히 되었습니다.)
어떤 코멘트라도 좋으니 생각 공유해주시면 감사하겠습니다.

https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/




### 201010 데이터 수집은
어차피 GPU 안 쓰니까
집 데스크탑에서 돌리는게 나음

주피터노트북으로도 여러건 동시에 돌릴 수 있고 selenium 도 좀더 빠른것 같고
특히 DB에 건건이 CONNECT 하고 INSERT 하는게 같은 망이라서 그런지 비교도 안되게 빠름

구글 코랩은 계속 세션 끊김.

BERT는 GPU 써야 되고 자원써서 발열 심하니까 코랩 어쩔 수 없이 쓰지만
데이터 수집은 로컬이 우월.



### 555 모델 중간 저장 - 텐서 플로우
- 참조1
https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko#scrollTo=mQF_dlgIVOvq

- 참조2
http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221564411127&categoryNo=49&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

- 참조3
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko





### 201010 데스크탑 로컬
    # KODEX레버리지
    SELECT COUNT(*) FROM T122630_20201009
    2353 / 20000


### 201010 데스크탑 로컬
C:\Users\head4\myjupyter\cei



### 201009 소스 정리하고 종목별로 나누기



### 201009 오늘 밤에 자기 전에 데스크탑으로 돌려 놓고 데이터 대량으로 모으기
: 꼭 한번에 다 받을 필요없음. 끊어서 받아와서 합쳐도 됨
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB




### 201010 데스크탑 로컬
C:\Users\head4\myjupyter\cei

### 201010 데스크탑

https://stackoverflow.com/questions/30337394/pandas-to-sql-fails-on-duplicate-primary-key

 SELECT COUNT(*) FROM T122630_20201009

df1
SELECT * FROM T122630_20201009
ORDER BY DATE ASC;



### 201010 Colab-자주끊기는-런타임-방지하기 데스크탑
Colab - 세션유지

방법은

구글 코랩에서 F12로 개발자 도구창을 열고

Console 선택 후

아래의 코드를 입력한 뒤 엔터를 누르면됩니다.

function ClickConnect() {var buttons = document.querySelectorAll("colab-dialog.yes-no-dialog paper-button#cancel"); buttons.forEach(function(btn) { btn.click(); }); console.log("1분마다 자동 재연결"); document.querySelector("colab-toolbar-button#connect").click(); } setInterval(ClickConnect,1000*60);

출처: https://somjang.tistory.com/entry/Google-Colab-자주끊기는-런타임-방지하기 [솜씨좋은장씨]


1. 구글 colab에서 90 time out 세션 유지 javascript 코드

function ClickConnect() {
    var buttons = document.querySelectorAll("colab-dialog.yes-no-dialog paper-button#cancel");
    buttons.forEach(function(btn) {
        btn.click();
    });
    console.log("1분마다 자동 재연결");
    document.querySelector("colab-toolbar-button#connect").click();
}
setInterval(ClickConnect,1000*60);

2. 구글 colab에서 buffered data was truncated after reaching the output size limit 에러 방지를 위한 현재 출력창 자동 지우기

function CleanCurrentOutput(){
    var btn = document.querySelector(".output-icon.clear_outputs_enabled.output-icon-selected[title$='현재 실행 중...'] iron-icon[command=clear-focused-or-selected-outputs]");
    if(btn) { console.log("30분마다 출력 지우기");
     btn.click();
    }
}
setInterval(CleanCurrentOutput,1000*60*30);



### 201009 로컬로 데스크탑에서 1번 소스 파이썬으로 돌려보고 시간재기 (로컬 수행)
: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB



### 201009 로컬로 노트북에서 1번 소스 파이썬으로 돌려보고 시간재기 (로컬 수행)
: 주피터 노트북

: 속도는 거의 똑같음.

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB
https://stackoverflow.com/questions/29858752/error-message-chromedriver-executable-needs-to-be-available-in-the-path
driver = webdriver.Chrome('/path/to/chromedriver')
driver = webdriver.Chrome("C:/Users/head4/myjupyter/cei/chromedriver.exe")

# How To Make Selenium WebDriver Scripts Faster
https://seleniumjava.com/2015/12/12/how-to-make-selenium-webdriver-scripts-faster/



### 201009 010 mariaDB 에 저장하기 # 여러건 수집해보기 (시간, 용량 측정)

# 사이즈 : MySQL 테이블 및 데이타베이스 크기 알아내기
https://www.lesstif.com/dbms/mysql-17105786.html

SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

0.38 MB : 420개
SELECT COUNT(*) FROM 신풍제약_20201009;

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB


### 20201009
SELECT * FROM 신풍제약_20201009
WHERE DATE LIKE '2020.10.09 19%'

SELECT * FROM TABLES;
SELECT * FROM TABLES WHERE TABLE_SCHEMA = 'cei';

### 201009 010 mariaDB 에 저장하기 # 저장된 것 다시 불러와서 dataframe 만들기
https://greendreamtrre.tistory.com/196

indata = pd.read_sql_query("select * from kopo_product_volume", engine)
indata.head()



### 201009 010 mariaDB 에 저장하기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율




### 201009 010 mariaDB 에 저장하기 # v7 mariaDB 테이블 만들기
### 201009 010 mariaDB 에 저장하기 # v8 mariaDB 랑 연동해서 담아보기
### 201009 010 mariaDB 에 저장하기 # v9 중복된것 있으면 PK기준 덮어쓰기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율


### 201009 010 mariaDB 에 저장하기 # 소스 찾아보기
    : Google colab mariadb conn

    # 파이썬에서 마리아 DB 연결하기
    https://m.blog.naver.com/smonone/221491863406

    # sqlalchemy와 pandas로 MariaDB에 데이터 insert하기
    https://yeomkyeorae.github.io/pandas/sqlalchemy/

SQLALCHEMY_DATABASE_URI = 'mysqI+mysqIconnector://yeom：yeom@IocaIhost:3306/price'
engine = sqlalchemy.create_engine(SQLALCHEMY_DATABASE_URI, echo=FaIse)

from sqlalchemy import create_engine engine = create_engine('mssql+pymssql://username:passwd@host/database', echo=True)

출처: https://excelsior-cjh.tistory.com/77 [EXCELSIOR]
GRANT ALL PRIVILEGES ON alchemy.* to 'cei'@'%';

SQLALCHEMY_DATABASE_URI =


engine = sqlalchemy.create_engine('mysqI+mysqIconnector://cei:Mrssjrnfl1!@121.128.223.185:3307/cei', echo=True)

https://stackoverflow.com/questions/51783313/how-do-i-get-sqlalchemy-create-engine-with-mysqlconnector-to-connect-using-mysql

SELECT * FROM df1_20201009;
selloutData.to_sql(name=resultname, con=engine, index = False, if_exists='replace')
use database_name;

use cei;

MariaDB [(none)]> use cei;
Database changed
MariaDB [cei]> desc df1_20201009;
+---------+------+------+-----+---------+-------+
| Field   | Type | Null | Key | Default | Extra |
+---------+------+------+-----+---------+-------+
| DATE    | text | YES  |     | NULL    |       |
| ITEM    | text | YES  |     | NULL    |       |
| TITLE   | text | YES  |     | NULL    |       |
| CONTENT | text | YES  |     | NULL    |       |
| READ    | text | YES  |     | NULL    |       |
| LIKE    | text | YES  |     | NULL    |       |
| DISLIKE | text | YES  |     | NULL    |       |
| HREF    | text | YES  |     | NULL    |       |
+---------+------+------+-----+---------+-------+
8 rows in set (0.009 sec)


MariaDB [cei]> select * from df1_20201009;




### 201003 010 mariaDB 에 저장하기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율
    # v7 mariaDB 테이블 만들기
    # v8 mariaDB 랑 연동해서 담아보기
    # v9 중복된것 있으면 PK기준 덮어쓰기


### 200920 github 에 csv write 하는 방법
: 알아보기 >>> 일단 보류 mariaDB 가 되면 그게 더 나음

https://gist.github.com/ifightcrime/f9cae5a568656897041e

# csv save on github upload
https://stackoverflow.com/questions/46866077/how-to-upload-csv-files-to-github-repo-and-use-them-as-data-for-my-r-scripts

# Dataverse csv upload
https://projects.iq.harvard.edu/odap/frequently-asked-questions





### 200920 데이터 파일 중간 저장 깃허브 data 폴더에 해보기

### 200920 스크랩 해서 깃허브 data 폴더에 넣는 것 (폴더 따로 해서) 해보기

### 200920 크롤링 참조
https://jeongwookie.github.io/2019/03/18/190318-naver-finance-data-crawling-using-python/


### 201003 002 스크랩 해서 dataframe 만들어 보기  v3
    /*
    : 필요한 부분만 list 로 뽑기
        시각   : 2020.10.03 11:26
        제목   : 부광에서 왔습니다
        내용   : fill content at the next step
        링크   : /item/board_read.nhn?code=019170&amp;nid=145679220&amp;st=&amp;sw=&amp;page=1
        조회수 : 90
        공감   : 4
        비공감 : 0
    */
    //### 참조 국민청원
    https://colab.research.google.com/github/huisung/president_go_kr_petitions_scraping/blob/master/get_petition.ipynb#scrollTo=T7v_K1bAgNtJ

    //: 함수형으로 만들기
    //: dataframe 형식으로 담기
    //: dataframe 제목 달기
    //: 클릭해서 들어가는 페이지 내용도 가져와보기 (해당 URL 주고 loop 돌려 찾기?)
    //: v4 컨텐츠 가져오기도 함수화 시키기
    //: v5 selenium 등 부분 안쓰면 지우기
    //: v6 여러페이지 loop 로 받아오기


### 201003 002 스크랩 해서 dataframe 만들어 보기  v2
    //: 게시판 글 부분만 뽑아내기 : 앞부분 자르기
    //: 게시판 글 부분만 뽑아내기 : 뒷부분 자르기
    //  :  href="/item/board_read.nhn 아닌것 골라내기

### 201003 002 스크랩 해서 dataframe 만들어 보기  v1
    # 신풍제약  019170  : 제2의 신풍을 찾아보자
        https://finance.naver.com/item/board.nhn?code=019170&page=1
        https://finance.naver.com/item/board.nhn?code=019170&page=2
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675817&st=&sw=&page=8
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675611&st=&sw=&page=9
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675609&st=&sw=&page=9
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675261&st=&sw=&page=1

        https://m.blog.naver.com/PostView.nhn?blogId=timtaeil&logNo=221420471952&categoryNo=30&proxyReferer=https:%2F%2Fwww.google.com%2F

        ============================================================
        <tr align="center" onmouseout="mouseOut(this)" onmouseover="mouseOver(this)">
        <td><span class="tah p10 gray03">2020.10.03 11:40</span></td>
        <td class="title">
        <a href="/item/board_read.nhn?code=019170&amp;nid=145679533&amp;st=&amp;sw=&amp;page=1" onclick="return singleSubmitCheck();" title="셀트랑 신풍 둘다 승인되지 않을까?">셀트랑 신풍 둘다 승인되지 않을까?
                        <span class="tah p9" style="color:#639933">[<b>4</b>]</span>
        <img height="10" src="https://ssl.pstatic.net/imgstock/images5/new.gif" width="10"/></a></td>
        <td class="p11">
                        seoh****
                    </td>
        <td><span class="tah p10 gray03">99</span></td>
        <td><strong class="tah p10 red01">9</strong></td>
        <td><strong class="tah p10 blue01">1</strong></td>
        </tr>

        http://hleecaster.com/python-web-crawling-with-beautifulsoup/
        http://hleecaster.com/narajangteo-crawling/

        https://stackoverflow.com/questions/40760441/exclude-unwanted-tag-on-beautifulsoup-python

        https://jungwoon.github.io/python/crawling/2018/04/12/Crawling-2/




    # KODEX 200선물인버스2X  252670 : 특정종목이 아닌 전체 흐름



### 201003 001 어디 스크랩 할지 정하기
    DC? >>> 너무 뻘글 - 노이즈가 많음
    팍스넷?
    네이버 인버스?

    네이버 - 신풍제약
    개인매수 상위종목 - 인기검색종목

    # NAVER 인기검색종목 (글이 많은 것)
        신풍제약    019170
        KODEX 200선물인버스2X    252670


### 200920 NAS MariaDB 접속
    ## http://himchan1185.synology.me:5000/

    ## MariaDB 유저
    cei / Mrs너구리1!

    ## Command
    # 집PC, 외부 (외부망)
    mysql -u cei -p -h 121.128.223.185 --port 3307 //OK
    # 노트북 (내부망)
    mysql -u cei -p -h 172.30.1.35 --port 3307    //OK : 노트북



### 200920 NAS 포트포워딩 설정 (외부IP로 특정 포트 접속시 특정 내부IP 장비로 포워딩)


선택  소스IP 주소 소스포트    외부포트    내부 IP 주소    내부 포트   프로토콜    설명  플래그
        -   5000-5000   172.30.1.35 5000-5000   TCP
        -   5001-5001   172.30.1.35 5001-5001   TCP
        -   21-21   172.30.1.35 21-21   TCP
        -   3307-3307   172.30.1.35 3307-3307   TCP
        -   5005-5005   172.30.1.35 5005-5005   TCP
        -   5006-5006   172.30.1.35 5006-5006   TCP
        -   80-80   172.30.1.35 80-80   TCP
        -   60003-60003 172.30.1.35 60003-60003 TCP

        ## 외부접속
        121.128.223.185:5000
        121.128.223.185:3307

        172.30.1.35:5000
        172.30.1.35:3307


        ## NAS 장비
        http://172.30.1.35:5000/

        172.30.1.35:5000
        # 외부망 접속
        https://swimdrg.tistory.com/8

        ## 와이파이
        172.30.1.254


        ## 노트북
        시스템 정보
        장비명 홈허브
        모델명/제조사 TI04-708H / allRadio
        버전  1.2.7
        날짜/시간   September 20 21:58:41 2020
        시스템업타임  2 Hour(s) 12 Minute(s) 55 Second(s) Elapsed
        메모리사용량  60.35%
        CPU사용량  1분: 3.50% / 5분: 3.00% / 15분: 2.72%
        대표 MAC 주소   00:07:89:C2:8C:96
        인터넷 연결정보
        인터페이스   WAN
        IP 할당 방식    DHCP
        IP주소    121.128.223.185
        서브넷마스크  255.255.255.0
        게이트웨이   121.128.223.254
        기본 DNS  168.126.63.1
        보조 DNS  168.126.63.2
        LAN 연결 정보
        IP 할당 정책    kt모드
        DHCP 서버 활성
        IP 주소   172.30.1.254
        서브넷마스크  255.255.255.0
        코넷 DHCP IP 범위   172.30.1.1 - 172.30.1.60
        프리미엄 DHCP IP 범위 172.30.1.128 - 172.30.1.148
        DHCP 임대시간 (sec) 3600

        ## 200920 Mysql 접속
        https://hannom.tistory.com/113
        IPv4 주소 . . . . . . . . . : 172.30.1.54 (노트북)
        IPv4 게이트웨이 . . . . . . . . . : 172.30.1.254 (노트북)

        IPv4 주소 . . . . . . . . . : 121.128.227.139 (집PC)
        http://homehub.olleh.com:8899/nat/portfwd

           연결별 DNS 접미사. . . . : kornet
           링크-로컬 IPv6 주소 . . . . : fe80::21f6:b6a6:1043:db30%6
           IPv4 주소 . . . . . . . . . : 121.128.227.139
           서브넷 마스크 . . . . . . . : 255.255.255.0
           기본 게이트웨이 . . . . . . : 121.128.227.254

        C:\WINDOWS\System32>mysql -u cei -p -h 172.30.1.54 --port 3307
        Enter password: ***********
        ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)
        시놀로지 mariaDB ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)

        ## http://172.30.1.35/phpMyAdmin/


        ## https://devks.tistory.com/2


        ## 노트북에서 연결 성공함
        172.30.1.35 //OK
        172.30.1.54
        3307

        mysql -u cei -p -h 121.128.227.185 --port 3307

        mysql -u cei -p -h 172.30.1.35 --port 3307 //OK : 노트북

        ## 외부에서 해보기


        ## 200920 mariaDB pandas 연동해보기

        ## 집 PC 에서 해보기
        # KT 와이파이 구성
        https://quasarzone.com/bbs/qb_tip/views/21528
        192.168.0.1

        # KT PC 공유기 DDNS


        # tcping
        C:\>tcping -d -s 121.128.227.139 3307

        2020:09:20 21:41:55 Probing 172.30.1.35:3307/tcp - No response - time=2007.774ms
        2020:09:20 21:41:57 Probing 172.30.1.35:3307/tcp - No response - time=2007.982ms
        2020:09:20 21:41:59 Probing 172.30.1.35:3307/tcp - No response - time=2014.208ms
        2020:09:20 21:42:01 Probing 172.30.1.35:3307/tcp - No response - time=2002.515ms

        Ping statistics for 172.30.1.35:3307
             4 probes sent.
             0 successful, 4 failed.  (100.00% fail)
        Was unable to connect, cannot provide trip statistics.

        C:\>

### 200920 MariaDB 10
## 참고
# Synology Nas에서 MariaDB 서버 처음부터 끝까지 설치하기
https://redapply.tistory.com/entry/Synology-%EC%97%90%EC%84%9C-MariaDB-%EC%84%9C%EB%B2%84-%EC%84%A4%EC%B9%98-%ED%95%9C%EB%B2%88%EC%97%90-%EB%81%9D%EA%B9%8C%EC%A7%80-%ED%95%98%EA%B8%B0

# KT 공유기 포트 포워딩
https://m.blog.naver.com/PostView.nhn?blogId=zetezz&logNo=221224911338&proxyReferer=https%3A%2F%2Fwww.google.com%2F

# 시놀로지 마리아DB 외부접속
https://lotus77.tistory.com/40

# CentOS 7 에 MariaDB 10 설치하기
https://blog.miyam.net/86

## MariaDB 10
root / Mrs너구리1!
3307

# KT 포트포워딩
https://m.blog.naver.com/PostView.nhn?blogId=zetezz&logNo=221224911338&proxyReferer=https%3A%2F%2Fwww.google.com%2F
IPv4 주소 . . . . . . . . . : 172.30.1.54 (노트북)
IPv4 주소 . . . . . . . . . : 121.128.227.139 (집PC)
http://homehub.olleh.com:8899/nat/portfwd

## phpMyAdmin
https://www.clien.net/service/board/cm_nas/12411854
 공인ip/phpMyAdmin/
포트포워딩 해야 접속 가능할 듯
https://himchan1185.synology.me:60004/phpMyAdmin/
https://himchan1185.synology.me:60003/phpMyAdmin/
121.128.227.139
포트포워딩 해야 접속 가능할 듯
http://172.30.1.35/phpMyAdmin/
노트북으로는 접속 성공함

http://172.30.1.35/phpMyAdmin/db_structure.php?server=1&db=cei&table=test


## phpMyAdmin
유저생성
cei / Mrs너구리1!

mysql -u cei -p -h 172.30.1.54 --port 3307
mysql -u cei -p -h 172.30.1.54 --port 3307

C:\WINDOWS\System32>mysql -u cei -p -h 172.30.1.54 --port 3307
Enter password: ***********
ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)


### 200920 스크랩 소스 깃허브에 넣기


### 깃허브 만들기
커뮤니티 감정 인덱스
PJ_COMM_EMO_INDEX
CEI
PJ_COMM_EMO_INDEX (Project Community Emotional Index)


https://github.com/head4ths/CEI

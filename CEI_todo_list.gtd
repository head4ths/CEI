/*
## 깃허브
https://github.com/head4ths/CEI

# 깃허브 폴더
/data                    데이터 파일 넣는 곳
/model                   모델 넣는 곳
/etc_reference_html      참고용 html 넣는 곳
/etc_reference_source    참고용 소스 넣는 곳
*/




$$$ Do - 토요일 밤에 집에서 돌려놓기 
$$$ Do - 일요일 저녁에 집에서 확인하기  
### 201010 데이터 2차 수집 - 1000~4000 page 


$$$ Do - 일요일에 회사에서 돌려놓기 
$$$ Do - 일요일에 집에서 저녁에 확인하기 
### 201010 모델 제일 결과 좋았던 걸로 학습시켜서 저장하기 
# part2_bert감정분석_모델학습및저장




$$$ 
### 201010 part3_모델불러와서_Classification수행.ipynb 
## part3_모델불러와서_Classification수행 

# (1) dataframe 불러오기 
* mariaDB 에 저장된 것 가져와서 감정분석
: 저장된 데이터 dataframe 으로 불러와서 감정분석해서 건별로 태그 달기 
: 태그 단 것 DB에 저장하기 


# (2) 모델 불러오기 ( 불러온 걸로 시연 해보기 )
* 앞부분에 모델 갱신시점 확인도 하기 
!ls -l --block-size=K /content/gdrive/'My Drive'/bert_model_save
!ls -l --block-size=M /content/gdrive/'My Drive'/bert_model_save/pytorch_model.bin


# (3) DB : 분석용 테이블 생성하기 : 태그컬럼 하나 추가 (날짜달린 원본데이터는 놔두고 여기다가 부어서 분석하기)
태그컬럼 : 작업전 2 > 작업 후 긍정1, 부정2 




--------------------


$$$ 
### 201010 part4_일별인덱스생성_감성_갯수
# 일자별 인덱스 만들기 

	
$$$ 
### 201010 part5_상관관계 분석.ipynb	
# 상관관계 분석
: 글의 갯수, 감정분석 비율 
: 주가, 거래량, 투자자별 매매동향 ( 거래소? ) 


$$$ 
### 201010 일자별 주가, 일자별 거래량 가져오기 
# 거래소 ??? 
# 회사데이터 ??? 
>>> 이건 분담 가능 



--------------------



### 888 발표
- BERT 가 긍정적인 댓글, 부정적인 댓글 잘 분류해주는 것 샘플로 보여줌 (편집의 힘!)

- BERT로 만들어낸 소문지수가 주가에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)

- 통계적인 글의 양이 거래량에 선행하는 것을 보여줌  (편집과 좋은 샘플 종목 선택의 힘!)


### 999 아이디어
- 글의 갯수, 길이 등등도 피쳐로 딥러닝

- 네이버무비 + 수작업 분류한 것 합쳐서 BERT 전이학습

- 아니면 주가가 내린 날에 올라온 글, 주가가 오른 날 올라온 글로 학습 ? >> 하지만 후행적이지 않을까? 목표는 소문을 먼저 알아내는 것



### 201010 데이터 수집 현황
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

## 
계속 뒤로 밀리니까 
(1차) 1~1000 page 스크랩했으면 (5H) 
(2차) 1000~4000 page 스크랩 하면 됨. (15H 예상) (테이블 이름 20201010으로 바꾸기) 
	>>> 계속 밀리니까 겹치는 부분이 있음 (1000~1100 페이지 정도까지의 앞부분)
	>>> 합칠때는 시간순으로 보고 중복된 것 지우고 합치기 

## 
: 1000번 돌림 > 20000개 > 15000초 (4H)  18MB
: 총 40MB 
SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

SELECT COUNT(*) FROM T122630_20201009; /* 20000건 완료 */ /* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT COUNT(*) FROM T252670_20201009; /* 20000건 완료 */ /* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT COUNT(*) FROM T019170_20201009; /* 20000건 완료 */ /* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930_20201009; /* 20000건 완료 */ /* 2020.09.28 13:37 ~ 2020.10.10 16:18 */

/* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT * FROM T122630_20201009 ORDER BY DATE asc; 
SELECT * FROM T122630_20201009 ORDER BY DATE desc;

/* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT * FROM T252670_20201009 ORDER BY DATE asc; 
SELECT * FROM T252670_20201009 ORDER BY DATE desc;

/* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT * FROM T005930_20201009 ORDER BY DATE asc; 
SELECT * FROM T005930_20201009 ORDER BY DATE desc;

/* 2020.09.28 13:37 ~ 2020.10.10 16:18 */
SELECT * FROM T019170_20201009 ORDER BY DATE asc; 
SELECT * FROM T019170_20201009 ORDER BY DATE desc;



=====================================================================================================================
(완료)



### 201010
	## 발표 컨셉 
	소문에 사서 뉴스에 판다. 
	뉴스를 분석하면 이미 늦는다 
	소문을 분석해라!!! 

	소문에 해당하는
	소문지수 

	이오 플로우 

	>>>>> 여러 종목 돌리다 보면... 분명히 소문지수가 주가보다 먼저 튀는 종목이 있을 것임
	>>>>> 발표 용으로는 제격

	# 발표 컨셉  
	알라딘 - 진흙속의 진주 
	뻘(진흙)글 속에서 통계적으로 숨겨진 진짜배기 소문을 찾다

	# 발표 컨셉   
	BERT 정확도 학습시 test set에 대하여 90몇 프로 나온걸로 정확도 높아 정확한 척
	현혹 가능. 


### 200910

	결정한 부분 
	- 개발 환경 : 구글 Colab 및 Github (https://github.com/head4ths/CEI/)) 사용 
	- 데이터 저장 : 개인 NAS 서버에 Maria DB (스크래핑 데이터, 개인매수동향, 주가데이터 보관) (http://121.128.223.185:3307)  cei / Mrs너구리1!  : 개인 PC 라 IP 바뀔수도 ??? 프로젝트 기간이 짧아서 별 상관없을 듯 )
	- 모델 중간 저장 : ??? 구글 드라이브 ???

	Todo 크게 네 부분 
	( 1,2,3,4 가 순차적이라 프로토타입 만들어보는 단계에서는 전원 참여 식으로 운영 ) 
	( 프로토타입으로 검증되면 개선은 각자 담당 영역 맡아서 진행 )
	- (1) 스크래핑 및 전처리 후 DB화 저장 
	- (2) BERT 감정분석 ( 네이버 무비 영화 감성 분석 - 전이학습 모델 사용 ) 후 태그 달기
	- (3) 개인매수동향 ( 코스콤 등에서 다운로드 ) 이나 주가데이터와 상관관계 분석 - 사회공학적인 인사이트 필요
	- (4) 발표
		
	
### 201003 040 일자별 인덱스 만들기 


### 201009 050 일자별 주가, 일자별 거래량 가져오기 


### 201003 060 상관관계 분석
    : 글의 갯수, 감정분석 비율 
    : 주가, 거래량, 투자자별 매매동향 ( 거래소? ) 

### 201003 020 Bert 소스 정리 
## part3_모델불러와서_Classification수행 
# 불러오기 ( 불러온 걸로 시연 해보기 )

# 앞부분에 모델 갱신시점 확인도 하기 
!ls -l --block-size=K /content/gdrive/'My Drive'/bert_model_save
!ls -l --block-size=M /content/gdrive/'My Drive'/bert_model_save/pytorch_model.bin



### 201010  참고 
Github의 경우 개당 파일은 100MB 제한이고 있지만 전체 용량 제한은 없다. Bitbucket은 개당 파일의 제한은 없지만 전체용량이 2GB이상이 안되는 제한사항이 존재 한다. Github의 경우 50Mb 는 Warning을 표시하며, 100Mb 부터는 Error를 나타낸다.


### 201010 데이터 수집 현황
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

: 1000번 돌림 > 20000개 > 15000초 (4H)  18MB
: 총 40MB 
SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

SELECT COUNT(*) FROM T122630_20201009; /* 20000건 완료 */ /* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT COUNT(*) FROM T252670_20201009; /* 20000건 완료 */ /* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT COUNT(*) FROM T019170_20201009; /* 20000건 완료 */ /* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT COUNT(*) FROM T005930_20201009; /* 20000건 완료 */ /* 2020.09.28 13:37 ~ 2020.10.10 16:18 */

/* 2020.05.24 04:13 ~ 2020.10.10 07:40 */
SELECT * FROM T122630_20201009 ORDER BY DATE asc; 
SELECT * FROM T122630_20201009 ORDER BY DATE desc;

/* 2020.09.18 09:14 ~ 2020.10.10 12:23 */
SELECT * FROM T252670_20201009 ORDER BY DATE asc; 
SELECT * FROM T252670_20201009 ORDER BY DATE desc;

/* 2020.09.14 14:16 ~ 2020.10.10 16:07 */
SELECT * FROM T005930_20201009 ORDER BY DATE asc; 
SELECT * FROM T005930_20201009 ORDER BY DATE desc;

/* 2020.09.28 13:37 ~ 2020.10.10 16:18 */
SELECT * FROM T019170_20201009 ORDER BY DATE asc; 
SELECT * FROM T019170_20201009 ORDER BY DATE desc;


### 201003 020 Bert 소스 정리 
## part2_bert감정분석_모델학습및저장 
	: 검증 완료 


### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 >>> 다시 불러오기 해보기 
: 700MB 정도 되는 듯 ??? 
: 구글 드라이브 쓰는게 구글 코랩에서 저장하고 불러오는데 매우 빠름 !!! 좋은 선택 !!!
: 뭐가 저장되는지 다운 받아지면 한번 열어보기 


## 참조2 (Good!)
https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP

## 핵심 키워드 
# from_pretrained
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# save_pretrained
bcm.save_pretrained('the output directory path')

## 원래 소스 
model = BertForSequenceClassification.from_pretrained(gv_model_nm, num_labels=2)
tokenizer = BertTokenizer.from_pretrained(gv_model_nm, do_lower_case=gv_do_lower_case)

## 불러오기 ( 불러온 걸로 시연 해보기 )



### 201010 bert 소스 수정 
## 소스 
(Colab) part2_bert감정분석
	
## 학습시 과정에서 시간 출력하게 수정하기 
#615K_20200615_v19


### 201010 
OSError: [Errno 95] Operation not supported: '/content/drive/Mask_RCNN' on Google Colab-자주끊기는-런타임-방지하기
https://stackoverflow.com/questions/60132033/oserror-errno-95-operation-not-supported-content-drive-mask-rcnn-on-googl




### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조4
# BertForSequenceClassification
# How to save a model as a BertModel #2094
https://github.com/huggingface/transformers/issues/2094

save_pretrained 

from transformers import BertForSequenceClassification, BertConfig

config = BertConfig.from_pretrained("bert-base-cased", num_labels=3)
model = BertForSequenceClassification.from_pretrained("bert-base-cased", config=config)
model.load_state_dict(torch.load("SAVED_SST_MODEL_DIR/pytorch_model.bin"))


# How to load BertforSequenceClassification models weights into BertforTokenClassification model? https://stackoverflow.com/questions/60897514/how-to-load-bertforsequenceclassification-models-weights-into-bertfortokenclassi


new_model = BertForTokenClassification(config=config)
new_model.bert.load_state_dict(model.bert.state_dict())

!mkdir -p saved_model
model.save('saved_model/my_model') 

# BertForTokenClassification model.save

model = BertForSequenceClassification.from_pretrained(gv_model_nm, num_labels=2)

## 참조 
https://github.com/huggingface/transformers/issues/2517
# load config
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# load a sequence model
bsm = BertForTokenClassification.from_pretrained('bert-base-uncased', config=conf)
# get bert core model
bcm = bsm.bert
# save the core model
bcm.save_pretrained('the output directory path')
# you also need to save your tokenizer in the same directory

## 참조2 - 검색 
https://www.google.com/search?sxsrf=ALeKk013i9Ir4dm_XebJUjc_kCRRRuGjew%3A1602323594165&ei=ioSBX8m8CaTKmAWB46SQBw&q=BertForTokenClassification+model+save_pretrained+colab&oq=BertForTokenClassification+model+save_pretrained+colab&gs_lcp=CgZwc3ktYWIQAzoECCMQJzoHCCMQrgIQJzoHCCEQChCgAVCXLFi4PmDhQmgBcAB4AIAB1QGIAZELkgEFMC43LjGYAQCgAQGqAQdnd3Mtd2l6wAEB&sclient=psy-ab&ved=0ahUKEwjJqZTF4KnsAhUkJaYKHYExCXIQ4dUDCA0&uact=5

## 참조2 (Good!)
https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP

## 참조3 
https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb#scrollTo

## 핵심 키워드 
# from_pretrained
conf = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)
# save_pretrained
bcm.save_pretrained('the output directory path')




### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조2
# 참조2
http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221564411127&categoryNo=49&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

from os import path
from google.colab import drive

model_dir_name = 'bert_model_save'
drive.mount('/content/gdrive')
model_base_dir = path.join('./gdrive/My Drive/', model_dir_name)
if not path.exists(model_base_dir):
  print('Check your google drive directory. See you file explorer')


with open(path.join(model_base_dir, "myfile.txt"), "w") as f:
    f.write("Google Colab is good!!!")
	

	
### 201003 020 Bert 모델 구글드라이브등 어딘가에 중간저장 해보기 - 참조1
## 참조 1,2,3은 개념만 대강 보고 참조4에서 bert 모델용으로 나온 것 쓰기 

## 모델 중간 저장 - 텐서 플로우
# 참조1
https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko#scrollTo=mQF_dlgIVOvq

tf.keras.callbacks.ModelCheckpoint
checkpoint_path = "training_1/cp.ckpt"

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest_checkpoint

# 이전에 저장한 가중치를 로드합니다
model.load_weights(latest)


수동으로 가중치 저장하기
앞에서 가중치를 모델에 로드하는 방법을 보았습니다. 수동으로 가중치를 저장하는 것도 쉽습니다. Model.save_weights 메서드를 사용합니다. tf.keras는, 특히 save_weights는 .ckpt 확장자를 가진 텐서플로 체크포인트 포맷을 사용합니다(.h5 확장자의 HDF5으로 저장하는 것은 Save and serialize models 가이드에서 다룹니다):


전체 모델 저장하기
model.save 메서드를 호출하여 모델의 구조, 가중치, 훈련 설정을 하나의 파일/폴더에 저장합니다. 모델을 저장하기 때문에 원본 파이썬 코드*가 없어도 사용할 수 있습니다. 옵티마이저 상태가 복원되므로 정확히 중지한 시점에서 다시 훈련을 시작할 수 있습니다.

두 개의 포맷(SavedModel과 HDF5)으로 모델을 저장할 수 있습니다. 텐서플로의 SavedModel 포맷은 TF2.x에서기본 파일 포맷입니다. 하지만 HDF5 포맷으로 저장할 수도 있습니다. 두 파일 포맷으로 전체 모델을 저장하는 방법은 아래에서 자세히 설명합니다.

전체 모델을 저장하는 기능은 매우 유용합니다. TensorFlow.js로 모델을 로드한 다음 웹 브라우저에서 모델을 훈련하고 실행할 수 있습니다(Saved Model, HDF5). 또는 모바일 장치에 맞도록 변환한 다음 TensorFlow Lite를 사용하여 실행할 수 있습니다(Saved Model, HDF5).

* 사용자 정의 객체(예를 들면 상속으로 만든 클래스나 층)는 저장하고 로드하는데 특별한 주의가 필요합니다. 아래 사용자 정의 객체 저장하기 섹션을 참고하세요.


# SavedModel로 전체 모델을 저장합니다
!mkdir -p saved_model
model.save('saved_model/my_model') 

new_model = tf.keras.models.load_model('saved_model/my_model')

# 모델 구조를 확인합니다
new_model.summary()

model.load_weights(latest)
model.save
tf.keras.models.load_model


### 201010 모델 저장용 local 설치 
pip install --upgrade pip
pip install -q pyyaml h5py
pip install pyyaml h5py 
ERROR: Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'c:\\programdata\\anaconda3\\lib\\site-packages\\pip\\_internal\\build_env.py'
Consider using the `--user` option or check the permissions.
해당 폴더에 적절한 권한이 없어서 발생하는 문제로, 관리자 권한으로 CMD를 실행하면 해결됩니다.
C:\\Users\\user\\AppData\\Local\\Temp\\pip-uninstall-8wl6ryze\\pip.exe
C:\Users\user\AppData\Local\Temp\pip-uninstall-8wl6ryze\



### 201010 GPU
안녕하세요. 서버에 대해 1도 모르는 대학원생이 작은 서버구축 관련하여 질문 드립니다. 최근 같이 일하게 된 Psychology/Biomedical Engineering 교수님이 (작은) 연구실용 서버 구축하는 것을 도와드리게 되었습니다.
첫 번째 질문입니다. 딥러닝 연구 목적으로 생각했을 때 Tesla P100 한 대 vs. TITAN RTX 두 대 vs. RTX 2080 Ti 네 대 비교한다면 어떤 게 좋을까요? 24/7 계속해서 GPU 돌릴게 아니고 간단한 벤치마크로 봐도 RTX 2080 Ti가 나아보이는데요... 경험 있으신 분들 의견을 구합니다. 저는 Titan Xp와 1080ti 밖에 사용을 안 해봤습니다 ㅜㅜ
두 번째 질문입니다. 현재 이 연구실은 PowerEdge T440 Tower Server를 가지고 있습니다 ㅜㅜ 지금까지 GPU로 연산을 돌리지 않았아서 T440을 가지고 있는 것 같아요. Intel Xeon Gold 5118 (2.3G, 12C/24T, 10.4GT/s, 16M
Cache) x 2개, 32GB RDIMM 2666MT/s Dual Rank x 6개, 4TB 7.2K RPM SATA 6Gbps 512n 3.5in Hot-plug Hard Drive x 8개가 장착되어 있습니다. 중요한건 꽂을 수 있는 GPU가 Quadro NVS 310나 P4000 밖에 없습니다. OMG... 이런 경우에 어떤 해결책이 있을까요? Tower를 T640 (Nvidia P100 장착가능)이나 R740 (Nvidia P100 or V100 장착가능)로 바꾸는 것은 현명한 방법일까요? 혹은 RTX 2080 Ti 설치가능한 Dell Server용 Tower가 있나요?
세 번째 질문입니다. 데이터는 주로 2D 이미지 일 것 같은데요, HDD로 충분할까요 아니면 적어도 SATA SSD를 구입해야 할까요? (이 질문에 대한 답변은 충분히 되었습니다.)
어떤 코멘트라도 좋으니 생각 공유해주시면 감사하겠습니다.

https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/




### 201010 데이터 수집은
어차피 GPU 안 쓰니까  
집 데스크탑에서 돌리는게 나음 

주피터노트북으로도 여러건 동시에 돌릴 수 있고 selenium 도 좀더 빠른것 같고 
특히 DB에 건건이 CONNECT 하고 INSERT 하는게 같은 망이라서 그런지 비교도 안되게 빠름 

구글 코랩은 계속 세션 끊김. 

BERT는 GPU 써야 되고 자원써서 발열 심하니까 코랩 어쩔 수 없이 쓰지만 
데이터 수집은 로컬이 우월. 



### 555 모델 중간 저장 - 텐서 플로우
- 참조1
https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko#scrollTo=mQF_dlgIVOvq

- 참조2
http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221564411127&categoryNo=49&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

- 참조3
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb?hl=ko





### 201010 데스크탑 로컬 
    # KODEX레버리지
    SELECT COUNT(*) FROM T122630_20201009
    2353 / 20000
 
 
### 201010 데스크탑 로컬
C:\Users\head4\myjupyter\cei



### 201009 소스 정리하고 종목별로 나누기 



### 201009 오늘 밤에 자기 전에 데스크탑으로 돌려 놓고 데이터 대량으로 모으기
: 꼭 한번에 다 받을 필요없음. 끊어서 받아와서 합쳐도 됨 
# 122630    KODEX레버리지
# 252670    KODEX200선물인버스2X
# 019170    신풍제약
# 005930    삼성전자

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB




### 201010 데스크탑 로컬
C:\Users\head4\myjupyter\cei

### 201010 데스크탑

https://stackoverflow.com/questions/30337394/pandas-to-sql-fails-on-duplicate-primary-key

 SELECT COUNT(*) FROM T122630_20201009

df1 
SELECT * FROM T122630_20201009
ORDER BY DATE ASC;



### 201010 Colab-자주끊기는-런타임-방지하기 데스크탑
Colab - 세션유지

방법은

구글 코랩에서 F12로 개발자 도구창을 열고

Console 선택 후

아래의 코드를 입력한 뒤 엔터를 누르면됩니다.

function ClickConnect() {var buttons = document.querySelectorAll("colab-dialog.yes-no-dialog paper-button#cancel"); buttons.forEach(function(btn) { btn.click(); }); console.log("1분마다 자동 재연결"); document.querySelector("colab-toolbar-button#connect").click(); } setInterval(ClickConnect,1000*60);

출처: https://somjang.tistory.com/entry/Google-Colab-자주끊기는-런타임-방지하기 [솜씨좋은장씨]


1. 구글 colab에서 90 time out 세션 유지 javascript 코드

function ClickConnect() {
    var buttons = document.querySelectorAll("colab-dialog.yes-no-dialog paper-button#cancel"); 
    buttons.forEach(function(btn) { 
        btn.click(); 
    }); 
    console.log("1분마다 자동 재연결"); 
    document.querySelector("colab-toolbar-button#connect").click(); 
} 
setInterval(ClickConnect,1000*60);
 
2. 구글 colab에서 buffered data was truncated after reaching the output size limit 에러 방지를 위한 현재 출력창 자동 지우기

function CleanCurrentOutput(){ 
    var btn = document.querySelector(".output-icon.clear_outputs_enabled.output-icon-selected[title$='현재 실행 중...'] iron-icon[command=clear-focused-or-selected-outputs]"); 
    if(btn) { console.log("30분마다 출력 지우기");
     btn.click(); 
    } 
} 
setInterval(CleanCurrentOutput,1000*60*30);
 


### 201009 로컬로 데스크탑에서 1번 소스 파이썬으로 돌려보고 시간재기 (로컬 수행)
: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB



### 201009 로컬로 노트북에서 1번 소스 파이썬으로 돌려보고 시간재기 (로컬 수행)
: 주피터 노트북 

: 속도는 거의 똑같음. 

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB
https://stackoverflow.com/questions/29858752/error-message-chromedriver-executable-needs-to-be-available-in-the-path
driver = webdriver.Chrome('/path/to/chromedriver') 
driver = webdriver.Chrome("C:/Users/head4/myjupyter/cei/chromedriver.exe")

# How To Make Selenium WebDriver Scripts Faster
https://seleniumjava.com/2015/12/12/how-to-make-selenium-webdriver-scripts-faster/



### 201009 010 mariaDB 에 저장하기 # 여러건 수집해보기 (시간, 용량 측정)

# 사이즈 : MySQL 테이블 및 데이타베이스 크기 알아내기
https://www.lesstif.com/dbms/mysql-17105786.html

SELECT TABLE_NAME AS "Tables",
round(((data_length + index_length) / 1024 / 1024), 2) "Size in MB"
FROM information_schema.TABLES
WHERE table_schema = 'cei'
ORDER BY (data_length + index_length) DESC;

0.38 MB : 420개
SELECT COUNT(*) FROM 신풍제약_20201009;

: 20번 돌림 > 400개 > 300초 > 0.36MB : 신풍제약 기준 약 하루
: 2000번 돌림 > 40000개 > 30000초 (8H)  > 36MB


### 20201009
SELECT * FROM 신풍제약_20201009
WHERE DATE LIKE '2020.10.09 19%'

SELECT * FROM TABLES;
SELECT * FROM TABLES WHERE TABLE_SCHEMA = 'cei';

### 201009 010 mariaDB 에 저장하기 # 저장된 것 다시 불러와서 dataframe 만들기
https://greendreamtrre.tistory.com/196

indata = pd.read_sql_query("select * from kopo_product_volume", engine)
indata.head()



### 201009 010 mariaDB 에 저장하기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율




### 201009 010 mariaDB 에 저장하기 # v7 mariaDB 테이블 만들기
### 201009 010 mariaDB 에 저장하기 # v8 mariaDB 랑 연동해서 담아보기
### 201009 010 mariaDB 에 저장하기 # v9 중복된것 있으면 PK기준 덮어쓰기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율


### 201009 010 mariaDB 에 저장하기 # 소스 찾아보기
    : Google colab mariadb conn

    # 파이썬에서 마리아 DB 연결하기
    https://m.blog.naver.com/smonone/221491863406

    # sqlalchemy와 pandas로 MariaDB에 데이터 insert하기
    https://yeomkyeorae.github.io/pandas/sqlalchemy/

SQLALCHEMY_DATABASE_URI = 'mysqI+mysqIconnector://yeom：yeom@IocaIhost:3306/price'
engine = sqlalchemy.create_engine(SQLALCHEMY_DATABASE_URI, echo=FaIse)

from sqlalchemy import create_engine engine = create_engine('mssql+pymssql://username:passwd@host/database', echo=True)

출처: https://excelsior-cjh.tistory.com/77 [EXCELSIOR]
GRANT ALL PRIVILEGES ON alchemy.* to 'cei'@'%';

SQLALCHEMY_DATABASE_URI =


engine = sqlalchemy.create_engine('mysqI+mysqIconnector://cei:Mrssjrnfl1!@121.128.223.185:3307/cei', echo=True)

https://stackoverflow.com/questions/51783313/how-do-i-get-sqlalchemy-create-engine-with-mysqlconnector-to-connect-using-mysql

SELECT * FROM df1_20201009;
selloutData.to_sql(name=resultname, con=engine, index = False, if_exists='replace')
use database_name;

use cei;

MariaDB [(none)]> use cei;
Database changed
MariaDB [cei]> desc df1_20201009;
+---------+------+------+-----+---------+-------+
| Field   | Type | Null | Key | Default | Extra |
+---------+------+------+-----+---------+-------+
| DATE    | text | YES  |     | NULL    |       |
| ITEM    | text | YES  |     | NULL    |       |
| TITLE   | text | YES  |     | NULL    |       |
| CONTENT | text | YES  |     | NULL    |       |
| READ    | text | YES  |     | NULL    |       |
| LIKE    | text | YES  |     | NULL    |       |
| DISLIKE | text | YES  |     | NULL    |       |
| HREF    | text | YES  |     | NULL    |       |
+---------+------+------+-----+---------+-------+
8 rows in set (0.009 sec)


MariaDB [cei]> select * from df1_20201009;




### 201003 010 mariaDB 에 저장하기
    # 원본 : (날짜,종목,seq) Text
    # 통계 : (날짜,종목) 글 수, 긍정비율
    # v7 mariaDB 테이블 만들기
    # v8 mariaDB 랑 연동해서 담아보기
    # v9 중복된것 있으면 PK기준 덮어쓰기


### 200920 github 에 csv write 하는 방법
: 알아보기 >>> 일단 보류 mariaDB 가 되면 그게 더 나음

https://gist.github.com/ifightcrime/f9cae5a568656897041e

# csv save on github upload
https://stackoverflow.com/questions/46866077/how-to-upload-csv-files-to-github-repo-and-use-them-as-data-for-my-r-scripts

# Dataverse csv upload
https://projects.iq.harvard.edu/odap/frequently-asked-questions





### 200920 데이터 파일 중간 저장 깃허브 data 폴더에 해보기

### 200920 스크랩 해서 깃허브 data 폴더에 넣는 것 (폴더 따로 해서) 해보기

### 200920 크롤링 참조
https://jeongwookie.github.io/2019/03/18/190318-naver-finance-data-crawling-using-python/


### 201003 002 스크랩 해서 dataframe 만들어 보기  v3
    /*
    : 필요한 부분만 list 로 뽑기
        시각   : 2020.10.03 11:26
        제목   : 부광에서 왔습니다
        내용   : fill content at the next step
        링크   : /item/board_read.nhn?code=019170&amp;nid=145679220&amp;st=&amp;sw=&amp;page=1
        조회수 : 90
        공감   : 4
        비공감 : 0
    */
    //### 참조 국민청원
    https://colab.research.google.com/github/huisung/president_go_kr_petitions_scraping/blob/master/get_petition.ipynb#scrollTo=T7v_K1bAgNtJ

    //: 함수형으로 만들기
    //: dataframe 형식으로 담기
    //: dataframe 제목 달기
    //: 클릭해서 들어가는 페이지 내용도 가져와보기 (해당 URL 주고 loop 돌려 찾기?)
    //: v4 컨텐츠 가져오기도 함수화 시키기
    //: v5 selenium 등 부분 안쓰면 지우기
    //: v6 여러페이지 loop 로 받아오기


### 201003 002 스크랩 해서 dataframe 만들어 보기  v2
    //: 게시판 글 부분만 뽑아내기 : 앞부분 자르기
    //: 게시판 글 부분만 뽑아내기 : 뒷부분 자르기
    //  :  href="/item/board_read.nhn 아닌것 골라내기

### 201003 002 스크랩 해서 dataframe 만들어 보기  v1
    # 신풍제약  019170  : 제2의 신풍을 찾아보자
        https://finance.naver.com/item/board.nhn?code=019170&page=1
        https://finance.naver.com/item/board.nhn?code=019170&page=2
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675817&st=&sw=&page=8
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675611&st=&sw=&page=9
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675609&st=&sw=&page=9
        https://finance.naver.com/item/board_read.nhn?code=019170&nid=145675261&st=&sw=&page=1

        https://m.blog.naver.com/PostView.nhn?blogId=timtaeil&logNo=221420471952&categoryNo=30&proxyReferer=https:%2F%2Fwww.google.com%2F

        ============================================================
        <tr align="center" onmouseout="mouseOut(this)" onmouseover="mouseOver(this)">
        <td><span class="tah p10 gray03">2020.10.03 11:40</span></td>
        <td class="title">
        <a href="/item/board_read.nhn?code=019170&amp;nid=145679533&amp;st=&amp;sw=&amp;page=1" onclick="return singleSubmitCheck();" title="셀트랑 신풍 둘다 승인되지 않을까?">셀트랑 신풍 둘다 승인되지 않을까?
                        <span class="tah p9" style="color:#639933">[<b>4</b>]</span>
        <img height="10" src="https://ssl.pstatic.net/imgstock/images5/new.gif" width="10"/></a></td>
        <td class="p11">
                        seoh****
                    </td>
        <td><span class="tah p10 gray03">99</span></td>
        <td><strong class="tah p10 red01">9</strong></td>
        <td><strong class="tah p10 blue01">1</strong></td>
        </tr>

        http://hleecaster.com/python-web-crawling-with-beautifulsoup/
        http://hleecaster.com/narajangteo-crawling/

        https://stackoverflow.com/questions/40760441/exclude-unwanted-tag-on-beautifulsoup-python

        https://jungwoon.github.io/python/crawling/2018/04/12/Crawling-2/




    # KODEX 200선물인버스2X  252670 : 특정종목이 아닌 전체 흐름



### 201003 001 어디 스크랩 할지 정하기
    DC? >>> 너무 뻘글 - 노이즈가 많음
    팍스넷?
    네이버 인버스?

    네이버 - 신풍제약
    개인매수 상위종목 - 인기검색종목

    # NAVER 인기검색종목 (글이 많은 것)
        신풍제약    019170
        KODEX 200선물인버스2X    252670


### 200920 NAS MariaDB 접속
    ## http://himchan1185.synology.me:5000/

    ## MariaDB 유저
    cei / Mrs너구리1!

    ## Command
    # 집PC, 외부 (외부망)
    mysql -u cei -p -h 121.128.223.185 --port 3307 //OK
    # 노트북 (내부망)
    mysql -u cei -p -h 172.30.1.35 --port 3307    //OK : 노트북



### 200920 NAS 포트포워딩 설정 (외부IP로 특정 포트 접속시 특정 내부IP 장비로 포워딩)


선택  소스IP 주소 소스포트    외부포트    내부 IP 주소    내부 포트   프로토콜    설명  플래그
        -   5000-5000   172.30.1.35 5000-5000   TCP
        -   5001-5001   172.30.1.35 5001-5001   TCP
        -   21-21   172.30.1.35 21-21   TCP
        -   3307-3307   172.30.1.35 3307-3307   TCP
        -   5005-5005   172.30.1.35 5005-5005   TCP
        -   5006-5006   172.30.1.35 5006-5006   TCP
        -   80-80   172.30.1.35 80-80   TCP
        -   60003-60003 172.30.1.35 60003-60003 TCP

        ## 외부접속
        121.128.223.185:5000
        121.128.223.185:3307

        172.30.1.35:5000
        172.30.1.35:3307


        ## NAS 장비
        http://172.30.1.35:5000/

        172.30.1.35:5000
        # 외부망 접속
        https://swimdrg.tistory.com/8

        ## 와이파이
        172.30.1.254


        ## 노트북
        시스템 정보
        장비명 홈허브
        모델명/제조사 TI04-708H / allRadio
        버전  1.2.7
        날짜/시간   September 20 21:58:41 2020
        시스템업타임  2 Hour(s) 12 Minute(s) 55 Second(s) Elapsed
        메모리사용량  60.35%
        CPU사용량  1분: 3.50% / 5분: 3.00% / 15분: 2.72%
        대표 MAC 주소   00:07:89:C2:8C:96
        인터넷 연결정보
        인터페이스   WAN
        IP 할당 방식    DHCP
        IP주소    121.128.223.185
        서브넷마스크  255.255.255.0
        게이트웨이   121.128.223.254
        기본 DNS  168.126.63.1
        보조 DNS  168.126.63.2
        LAN 연결 정보
        IP 할당 정책    kt모드
        DHCP 서버 활성
        IP 주소   172.30.1.254
        서브넷마스크  255.255.255.0
        코넷 DHCP IP 범위   172.30.1.1 - 172.30.1.60
        프리미엄 DHCP IP 범위 172.30.1.128 - 172.30.1.148
        DHCP 임대시간 (sec) 3600

        ## 200920 Mysql 접속
        https://hannom.tistory.com/113
        IPv4 주소 . . . . . . . . . : 172.30.1.54 (노트북)
        IPv4 게이트웨이 . . . . . . . . . : 172.30.1.254 (노트북)

        IPv4 주소 . . . . . . . . . : 121.128.227.139 (집PC)
        http://homehub.olleh.com:8899/nat/portfwd

           연결별 DNS 접미사. . . . : kornet
           링크-로컬 IPv6 주소 . . . . : fe80::21f6:b6a6:1043:db30%6
           IPv4 주소 . . . . . . . . . : 121.128.227.139
           서브넷 마스크 . . . . . . . : 255.255.255.0
           기본 게이트웨이 . . . . . . : 121.128.227.254

        C:\WINDOWS\System32>mysql -u cei -p -h 172.30.1.54 --port 3307
        Enter password: ***********
        ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)
        시놀로지 mariaDB ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)

        ## http://172.30.1.35/phpMyAdmin/


        ## https://devks.tistory.com/2


        ## 노트북에서 연결 성공함
        172.30.1.35 //OK
        172.30.1.54
        3307

        mysql -u cei -p -h 121.128.227.185 --port 3307

        mysql -u cei -p -h 172.30.1.35 --port 3307 //OK : 노트북

        ## 외부에서 해보기


        ## 200920 mariaDB pandas 연동해보기

        ## 집 PC 에서 해보기
        # KT 와이파이 구성
        https://quasarzone.com/bbs/qb_tip/views/21528
        192.168.0.1

        # KT PC 공유기 DDNS


        # tcping
        C:\>tcping -d -s 121.128.227.139 3307

        2020:09:20 21:41:55 Probing 172.30.1.35:3307/tcp - No response - time=2007.774ms
        2020:09:20 21:41:57 Probing 172.30.1.35:3307/tcp - No response - time=2007.982ms
        2020:09:20 21:41:59 Probing 172.30.1.35:3307/tcp - No response - time=2014.208ms
        2020:09:20 21:42:01 Probing 172.30.1.35:3307/tcp - No response - time=2002.515ms

        Ping statistics for 172.30.1.35:3307
             4 probes sent.
             0 successful, 4 failed.  (100.00% fail)
        Was unable to connect, cannot provide trip statistics.

        C:\>

### 200920 MariaDB 10
## 참고
# Synology Nas에서 MariaDB 서버 처음부터 끝까지 설치하기
https://redapply.tistory.com/entry/Synology-%EC%97%90%EC%84%9C-MariaDB-%EC%84%9C%EB%B2%84-%EC%84%A4%EC%B9%98-%ED%95%9C%EB%B2%88%EC%97%90-%EB%81%9D%EA%B9%8C%EC%A7%80-%ED%95%98%EA%B8%B0

# KT 공유기 포트 포워딩
https://m.blog.naver.com/PostView.nhn?blogId=zetezz&logNo=221224911338&proxyReferer=https%3A%2F%2Fwww.google.com%2F

# 시놀로지 마리아DB 외부접속
https://lotus77.tistory.com/40

# CentOS 7 에 MariaDB 10 설치하기
https://blog.miyam.net/86

## MariaDB 10
root / Mrs너구리1!
3307

# KT 포트포워딩
https://m.blog.naver.com/PostView.nhn?blogId=zetezz&logNo=221224911338&proxyReferer=https%3A%2F%2Fwww.google.com%2F
IPv4 주소 . . . . . . . . . : 172.30.1.54 (노트북)
IPv4 주소 . . . . . . . . . : 121.128.227.139 (집PC)
http://homehub.olleh.com:8899/nat/portfwd

## phpMyAdmin
https://www.clien.net/service/board/cm_nas/12411854
 공인ip/phpMyAdmin/
포트포워딩 해야 접속 가능할 듯
https://himchan1185.synology.me:60004/phpMyAdmin/
https://himchan1185.synology.me:60003/phpMyAdmin/
121.128.227.139
포트포워딩 해야 접속 가능할 듯
http://172.30.1.35/phpMyAdmin/
노트북으로는 접속 성공함

http://172.30.1.35/phpMyAdmin/db_structure.php?server=1&db=cei&table=test


## phpMyAdmin
유저생성
cei / Mrs너구리1!

mysql -u cei -p -h 172.30.1.54 --port 3307
mysql -u cei -p -h 172.30.1.54 --port 3307

C:\WINDOWS\System32>mysql -u cei -p -h 172.30.1.54 --port 3307
Enter password: ***********
ERROR 2002 (HY000): Can't connect to MySQL server on '172.30.1.54' (10061)


### 200920 스크랩 소스 깃허브에 넣기


### 깃허브 만들기
커뮤니티 감정 인덱스
PJ_COMM_EMO_INDEX
CEI
PJ_COMM_EMO_INDEX (Project Community Emotional Index)


https://github.com/head4ths/CEI
